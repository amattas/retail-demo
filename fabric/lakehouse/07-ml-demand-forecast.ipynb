{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Demand Forecasting with Prophet\n",
        "\n",
        "Generates 14-day demand forecasts by store and product using Facebook Prophet.\n",
        "\n",
        "## Data Flow\n",
        "```\n",
        "fact_receipt_lines (Silver) --> gold_demand_forecast (Gold)\n",
        "```\n",
        "\n",
        "## Model Details\n",
        "- **Algorithm:** Facebook Prophet (additive time series)\n",
        "- **Granularity:** Store × Product × Day\n",
        "- **Forecast horizon:** 14 days\n",
        "- **Features:** Historical sales with daily/weekly seasonality\n",
        "- **Target metric:** MAPE < 25% for top 80% products by volume\n",
        "\n",
        "## Usage\n",
        "Schedule this notebook to run **daily at 6 AM** via Fabric pipeline to ensure fresh forecasts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Prophet import\n",
        "try:\n",
        "    from prophet import Prophet\n",
        "except ImportError:\n",
        "    print(\"Installing prophet...\")\n",
        "    !pip install prophet\n",
        "    from prophet import Prophet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "def get_env(var_name, default=None):\n",
        "    return os.environ.get(var_name, default)\n",
        "\n",
        "SILVER_DB = get_env(\"SILVER_DB\", default=\"ag\")\n",
        "GOLD_DB = get_env(\"GOLD_DB\", default=\"au\")\n",
        "\n",
        "# Forecasting parameters\n",
        "FORECAST_HORIZON_DAYS = 14\n",
        "MIN_HISTORY_DAYS = 30  # Minimum historical data required\n",
        "MIN_DAILY_SALES = 0.5  # Minimum average daily sales to forecast\n",
        "TARGET_MAPE = 25.0  # Target MAPE threshold\n",
        "\n",
        "print(f\"Configuration: SILVER_DB={SILVER_DB}, GOLD_DB={GOLD_DB}\")\n",
        "print(f\"Forecast horizon: {FORECAST_HORIZON_DAYS} days\")\n",
        "print(f\"Minimum history: {MIN_HISTORY_DAYS} days\")\n",
        "print(f\"Target MAPE: {TARGET_MAPE}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HELPER FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def ensure_database(name):\n",
        "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {name}\")\n",
        "\n",
        "def read_silver(table_name):\n",
        "    return spark.table(f\"{SILVER_DB}.{table_name}\")\n",
        "\n",
        "def silver_exists(table_name):\n",
        "    try:\n",
        "        spark.table(f\"{SILVER_DB}.{table_name}\")\n",
        "        return True\n",
        "    except AnalysisException:\n",
        "        return False\n",
        "\n",
        "ensure_database(GOLD_DB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"DATA PREPARATION\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate historical sales by store, product, and day\n",
        "if not silver_exists(\"fact_receipt_lines\"):\n",
        "    raise RuntimeError(\"fact_receipt_lines table not found in Silver layer\")\n",
        "\n",
        "print(\"Reading fact_receipt_lines and joining with receipts...\")\n",
        "\n",
        "# Join receipt lines with receipts to get store_id\n",
        "df_lines = read_silver(\"fact_receipt_lines\")\n",
        "df_receipts = read_silver(\"fact_receipts\")\n",
        "\n",
        "# Aggregate to daily sales by store and product\n",
        "df_daily_sales = (\n",
        "    df_lines\n",
        "    .join(df_receipts.select(\"receipt_id_ext\", \"store_id\"), on=\"receipt_id_ext\")\n",
        "    .withColumn(\"sale_date\", F.to_date(\"event_ts\"))\n",
        "    .groupBy(\"store_id\", \"product_id\", \"sale_date\")\n",
        "    .agg(\n",
        "        F.sum(\"quantity\").alias(\"units_sold\"),\n",
        "        F.sum(\"ext_price\").alias(\"revenue\")\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"Daily sales aggregated: {df_daily_sales.count()} rows\")\n",
        "\n",
        "# Filter to store-product combinations with sufficient history\n",
        "df_history_check = (\n",
        "    df_daily_sales\n",
        "    .groupBy(\"store_id\", \"product_id\")\n",
        "    .agg(\n",
        "        F.min(\"sale_date\").alias(\"first_sale\"),\n",
        "        F.max(\"sale_date\").alias(\"last_sale\"),\n",
        "        F.count(\"sale_date\").alias(\"days_with_sales\"),\n",
        "        F.avg(\"units_sold\").alias(\"avg_daily_sales\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"history_days\",\n",
        "        F.datediff(F.col(\"last_sale\"), F.col(\"first_sale\"))\n",
        "    )\n",
        "    .filter(\n",
        "        (F.col(\"history_days\") >= MIN_HISTORY_DAYS) &\n",
        "        (F.col(\"avg_daily_sales\") >= MIN_DAILY_SALES)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"Store-product combinations with sufficient history: {df_history_check.count()}\")\n",
        "\n",
        "# Join back to keep only qualifying combinations\n",
        "df_training_data = (\n",
        "    df_daily_sales\n",
        "    .join(\n",
        "        df_history_check.select(\"store_id\", \"product_id\"),\n",
        "        on=[\"store_id\", \"product_id\"],\n",
        "        how=\"inner\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"Training data prepared: {df_training_data.count()} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL TRAINING & FORECASTING\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_forecast(store_id, product_id, sales_data):\n",
        "    \"\"\"\n",
        "    Train Prophet model and generate forecasts for a store-product combination.\n",
        "    \n",
        "    Args:\n",
        "        store_id: Store identifier\n",
        "        product_id: Product identifier\n",
        "        sales_data: List of tuples (sale_date, units_sold)\n",
        "    \n",
        "    Returns:\n",
        "        List of forecast records with confidence intervals\n",
        "    \"\"\"\n",
        "    # Prepare data in Prophet format\n",
        "    df_prophet = pd.DataFrame(sales_data, columns=[\"ds\", \"y\"])\n",
        "    \n",
        "    # Handle edge cases\n",
        "    if len(df_prophet) < 2:\n",
        "        return []\n",
        "    \n",
        "    try:\n",
        "        # Initialize and train model\n",
        "        model = Prophet(\n",
        "            daily_seasonality=True,\n",
        "            weekly_seasonality=True,\n",
        "            yearly_seasonality=False,  # Not enough history\n",
        "            changepoint_prior_scale=0.05,  # Less sensitive to changes\n",
        "            seasonality_prior_scale=10.0,\n",
        "            interval_width=0.95  # 95% confidence intervals\n",
        "        )\n",
        "        \n",
        "        # Suppress Prophet logs\n",
        "        import logging\n",
        "        logging.getLogger('prophet').setLevel(logging.ERROR)\n",
        "        \n",
        "        model.fit(df_prophet)\n",
        "        \n",
        "        # Generate future dates\n",
        "        future = model.make_future_dataframe(periods=FORECAST_HORIZON_DAYS)\n",
        "        forecast = model.predict(future)\n",
        "        \n",
        "        # Calculate MAPE on historical data\n",
        "        historical_forecast = forecast[forecast['ds'].isin(df_prophet['ds'])]\n",
        "        merged = df_prophet.merge(historical_forecast[['ds', 'yhat']], on='ds')\n",
        "        \n",
        "        # Avoid division by zero\n",
        "        merged = merged[merged['y'] > 0]\n",
        "        if len(merged) > 0:\n",
        "            mape = (abs(merged['y'] - merged['yhat']) / merged['y']).mean() * 100\n",
        "        else:\n",
        "            mape = None\n",
        "        \n",
        "        # Extract future forecasts only\n",
        "        future_forecast = forecast[forecast['ds'] > df_prophet['ds'].max()]\n",
        "        \n",
        "        # Build result records\n",
        "        results = []\n",
        "        for _, row in future_forecast.iterrows():\n",
        "            results.append((\n",
        "                store_id,\n",
        "                product_id,\n",
        "                row['ds'].date(),\n",
        "                max(0.0, float(row['yhat'])),  # Predicted units (non-negative)\n",
        "                max(0.0, float(row['yhat_lower'])),  # Lower bound\n",
        "                max(0.0, float(row['yhat_upper'])),  # Upper bound\n",
        "                float(mape) if mape is not None else None,\n",
        "                pd.Timestamp.utcnow()\n",
        "            ))\n",
        "        \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Log error and return empty list\n",
        "        print(f\"Error forecasting store_id={store_id}, product_id={product_id}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "print(\"Prophet forecasting function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect training data for each store-product combination\n",
        "# Note: For large datasets, consider sampling or processing in batches\n",
        "print(\"Collecting training data by store and product...\")\n",
        "\n",
        "df_grouped = (\n",
        "    df_training_data\n",
        "    .groupBy(\"store_id\", \"product_id\")\n",
        "    .agg(\n",
        "        F.collect_list(\n",
        "            F.struct(\"sale_date\", \"units_sold\")\n",
        "        ).alias(\"sales_history\")\n",
        "    )\n",
        ")\n",
        "\n",
        "store_product_data = df_grouped.collect()\n",
        "print(f\"Processing {len(store_product_data)} store-product combinations...\")\n",
        "\n",
        "# Train models and generate forecasts\n",
        "all_forecasts = []\n",
        "success_count = 0\n",
        "error_count = 0\n",
        "\n",
        "for i, row in enumerate(store_product_data):\n",
        "    if i % 50 == 0:\n",
        "        print(f\"  Progress: {i}/{len(store_product_data)} combinations processed\")\n",
        "    \n",
        "    store_id = row['store_id']\n",
        "    product_id = row['product_id']\n",
        "    sales_history = [(s['sale_date'], float(s['units_sold'])) for s in row['sales_history']]\n",
        "    \n",
        "    forecasts = train_and_forecast(store_id, product_id, sales_history)\n",
        "    \n",
        "    if forecasts:\n",
        "        all_forecasts.extend(forecasts)\n",
        "        success_count += 1\n",
        "    else:\n",
        "        error_count += 1\n",
        "\n",
        "print(f\"\\nForecasting complete:\")\n",
        "print(f\"  Successful: {success_count}\")\n",
        "print(f\"  Failed: {error_count}\")\n",
        "print(f\"  Total forecast records: {len(all_forecasts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVE FORECASTS TO GOLD LAYER\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert forecasts to DataFrame\n",
        "if all_forecasts:\n",
        "    schema = [\n",
        "        \"store_id\",\n",
        "        \"product_id\",\n",
        "        \"forecast_date\",\n",
        "        \"predicted_units\",\n",
        "        \"lower_bound\",\n",
        "        \"upper_bound\",\n",
        "        \"mape\",\n",
        "        \"generated_at\"\n",
        "    ]\n",
        "    \n",
        "    df_forecasts = spark.createDataFrame(all_forecasts, schema=schema)\n",
        "    \n",
        "    # Cast types explicitly\n",
        "    df_forecasts = (\n",
        "        df_forecasts\n",
        "        .withColumn(\"store_id\", F.col(\"store_id\").cast(\"long\"))\n",
        "        .withColumn(\"product_id\", F.col(\"product_id\").cast(\"long\"))\n",
        "        .withColumn(\"forecast_date\", F.col(\"forecast_date\").cast(\"date\"))\n",
        "        .withColumn(\"predicted_units\", F.col(\"predicted_units\").cast(\"double\"))\n",
        "        .withColumn(\"lower_bound\", F.col(\"lower_bound\").cast(\"double\"))\n",
        "        .withColumn(\"upper_bound\", F.col(\"upper_bound\").cast(\"double\"))\n",
        "        .withColumn(\"mape\", F.col(\"mape\").cast(\"double\"))\n",
        "        .withColumn(\"generated_at\", F.col(\"generated_at\").cast(\"timestamp\"))\n",
        "    )\n",
        "    \n",
        "    # Save to Gold layer\n",
        "    table_name = f\"{GOLD_DB}.gold_demand_forecast\"\n",
        "    df_forecasts.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
        "    \n",
        "    print(f\"Saved {df_forecasts.count()} forecast records to {table_name}\")\n",
        "    \n",
        "    # Display sample forecasts\n",
        "    print(\"\\nSample forecasts:\")\n",
        "    df_forecasts.orderBy(\"store_id\", \"product_id\", \"forecast_date\").show(10, truncate=False)\n",
        "    \n",
        "else:\n",
        "    print(\"No forecasts generated. Check data quality and parameters.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FORECAST ACCURACY METRICS\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate summary statistics\n",
        "if all_forecasts:\n",
        "    df_metrics = spark.table(f\"{GOLD_DB}.gold_demand_forecast\")\n",
        "    \n",
        "    # Overall MAPE distribution\n",
        "    print(\"MAPE Statistics:\")\n",
        "    df_metrics.select(\"mape\").filter(F.col(\"mape\").isNotNull()).describe().show()\n",
        "    \n",
        "    # Count products meeting target MAPE\n",
        "    total_combinations = df_metrics.select(\"store_id\", \"product_id\").distinct().count()\n",
        "    meeting_target = (\n",
        "        df_metrics\n",
        "        .select(\"store_id\", \"product_id\", \"mape\")\n",
        "        .filter(F.col(\"mape\").isNotNull())\n",
        "        .groupBy(\"store_id\", \"product_id\")\n",
        "        .agg(F.avg(\"mape\").alias(\"avg_mape\"))\n",
        "        .filter(F.col(\"avg_mape\") < TARGET_MAPE)\n",
        "        .count()\n",
        "    )\n",
        "    \n",
        "    pct_meeting_target = (meeting_target / total_combinations * 100) if total_combinations > 0 else 0\n",
        "    \n",
        "    print(f\"\\nAccuracy Summary:\")\n",
        "    print(f\"  Total store-product combinations: {total_combinations}\")\n",
        "    print(f\"  Combinations meeting MAPE < {TARGET_MAPE}%: {meeting_target} ({pct_meeting_target:.1f}%)\")\n",
        "    \n",
        "    if pct_meeting_target >= 80:\n",
        "        print(f\"\\n  SUCCESS: {pct_meeting_target:.1f}% of products meet the accuracy target!\")\n",
        "    else:\n",
        "        print(f\"\\n  WARNING: Only {pct_meeting_target:.1f}% meet target. Consider:\")\n",
        "        print(\"  - Increasing MIN_HISTORY_DAYS\")\n",
        "        print(\"  - Adjusting Prophet hyperparameters\")\n",
        "        print(\"  - Adding external regressors (promotions, holidays)\")\n",
        "    \n",
        "    # Top 10 best and worst performers by MAPE\n",
        "    print(\"\\nTop 10 Most Accurate Forecasts (by avg MAPE):\")\n",
        "    (\n",
        "        df_metrics\n",
        "        .groupBy(\"store_id\", \"product_id\")\n",
        "        .agg(F.avg(\"mape\").alias(\"avg_mape\"))\n",
        "        .filter(F.col(\"avg_mape\").isNotNull())\n",
        "        .orderBy(\"avg_mape\")\n",
        "        .show(10, truncate=False)\n",
        "    )\n",
        "    \n",
        "    print(\"\\nTop 10 Least Accurate Forecasts (by avg MAPE):\")\n",
        "    (\n",
        "        df_metrics\n",
        "        .groupBy(\"store_id\", \"product_id\")\n",
        "        .agg(F.avg(\"mape\").alias(\"avg_mape\"))\n",
        "        .filter(F.col(\"avg_mape\").isNotNull())\n",
        "        .orderBy(F.desc(\"avg_mape\"))\n",
        "        .show(10, truncate=False)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DEMAND FORECASTING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "gold_tables = spark.sql(f\"SHOW TABLES IN {GOLD_DB}\").collect()\n",
        "print(f\"\\nGold ({GOLD_DB}): {len(gold_tables)} tables\")\n",
        "print(\"\\nForecast table: gold_demand_forecast\")\n",
        "print(f\"Forecast horizon: {FORECAST_HORIZON_DAYS} days\")\n",
        "print(\"\\nSchedule this notebook to run daily at 6 AM for fresh forecasts.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
