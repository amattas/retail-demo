{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML: Delivery Time Prediction\n",
    "\n",
    "Predicts truck dwell times at stores and distribution centers using LightGBM regression.\n",
    "\n",
    "## Business Value\n",
    "- Optimize receiving dock scheduling\n",
    "- Reduce driver wait times and costs\n",
    "- Improve labor planning for receiving staff\n",
    "- Better ETA estimates for inventory availability\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "Silver (fact_truck_moves, dimensions) --> ML Model --> Gold (gold_dwell_predictions)\n",
    "```\n",
    "\n",
    "## Model Details\n",
    "- **Algorithm**: LightGBM Regressor with quantile regression\n",
    "- **Target**: Dwell time (minutes) = departed_ts - arrived_ts\n",
    "- **Features**: Location type, arrival hour, day of week, truck type, historical averages\n",
    "- **Metrics**: MAE < 15 minutes, MAPE < 25%\n",
    "- **Output**: Point predictions + 80% confidence intervals\n",
    "\n",
    "## Usage\n",
    "Schedule this notebook to run **daily** via Fabric pipeline to retrain model and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ML imports\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "except ImportError:\n",
    "    print(\"Installing required packages...\")\n",
    "    !pip install lightgbm scikit-learn\n",
    "    import lightgbm as lgb\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "def get_env(var_name, default=None):\n",
    "    return os.environ.get(var_name, default)\n",
    "\n",
    "SILVER_DB = get_env(\"SILVER_DB\", default=\"ag\")\n",
    "GOLD_DB = get_env(\"GOLD_DB\", default=\"au\")\n",
    "\n",
    "# Model parameters\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "TARGET_MAE = 15.0  # Target: MAE < 15 minutes\n",
    "TARGET_MAPE = 0.25  # Target: MAPE < 25%\n",
    "\n",
    "print(f\"Configuration: SILVER_DB={SILVER_DB}, GOLD_DB={GOLD_DB}\")\n",
    "print(f\"Model Targets: MAE < {TARGET_MAE} min, MAPE < {TARGET_MAPE*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def ensure_database(name):\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {name}\")\n",
    "    print(f\"Database '{name}' ready.\")\n",
    "\n",
    "def read_silver(table_name):\n",
    "    return spark.table(f\"{SILVER_DB}.{table_name}\")\n",
    "\n",
    "def save_gold(df, table_name):\n",
    "    full_name = f\"{GOLD_DB}.{table_name}\"\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_name)\n",
    "    print(f\"  {full_name}: {df.count()} rows\")\n",
    "\n",
    "ensure_database(GOLD_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Data Preparation\n",
    "\n",
    "Calculate dwell times from truck arrivals and departures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PREPARING DWELL TIME DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Read fact_truck_moves - contains ARRIVED and DEPARTED events\n",
    "# ETA is populated on ARRIVED events, ETD is populated on DEPARTED events\n",
    "df_truck_moves = read_silver(\"fact_truck_moves\")\n",
    "\n",
    "print(f\"Total truck move records: {df_truck_moves.count()}\")\n",
    "df_truck_moves.groupBy(\"status\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join ARRIVED and DEPARTED events by shipment_id to calculate dwell time\n",
    "# ARRIVED events have eta populated, DEPARTED events have etd populated\n",
    "df_arrived = (\n",
    "    df_truck_moves\n",
    "    .filter(F.col(\"status\") == \"ARRIVED\")\n",
    "    .select(\n",
    "        F.col(\"shipment_id\"),\n",
    "        F.col(\"truck_id\"),\n",
    "        F.col(\"store_id\"),\n",
    "        F.col(\"dc_id\"),\n",
    "        F.col(\"eta\").alias(\"arrived_ts\"),\n",
    "        F.col(\"event_ts\").alias(\"arrived_event_ts\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_departed = (\n",
    "    df_truck_moves\n",
    "    .filter(F.col(\"status\") == \"DEPARTED\")\n",
    "    .select(\n",
    "        F.col(\"shipment_id\"),\n",
    "        F.col(\"etd\").alias(\"departed_ts\"),\n",
    "        F.col(\"event_ts\").alias(\"departed_event_ts\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join to get complete shipments with both arrival and departure\n",
    "df_dwell = (\n",
    "    df_arrived\n",
    "    .join(df_departed, on=\"shipment_id\", how=\"inner\")\n",
    "    .withColumn(\n",
    "        \"dwell_minutes\",\n",
    "        (F.unix_timestamp(\"departed_ts\") - F.unix_timestamp(\"arrived_ts\")) / 60\n",
    "    )\n",
    "    # Filter out invalid records (negative or zero dwell time)\n",
    "    .filter((F.col(\"dwell_minutes\") > 0) & (F.col(\"dwell_minutes\") < 480))  # Max 8 hours\n",
    ")\n",
    "\n",
    "print(f\"\\nShipments with complete dwell times: {df_dwell.count()}\")\n",
    "\n",
    "# Show statistics\n",
    "df_dwell.select(\n",
    "    F.min(\"dwell_minutes\").alias(\"min_dwell\"),\n",
    "    F.avg(\"dwell_minutes\").alias(\"avg_dwell\"),\n",
    "    F.max(\"dwell_minutes\").alias(\"max_dwell\"),\n",
    "    F.stddev(\"dwell_minutes\").alias(\"stddev_dwell\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Feature Engineering\n",
    "\n",
    "Create temporal, location, and historical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Temporal features from arrival time\n",
    "df_features = (\n",
    "    df_dwell\n",
    "    .withColumn(\"arrival_hour\", F.hour(\"arrived_ts\"))\n",
    "    .withColumn(\"arrival_day_of_week\", F.dayofweek(\"arrived_ts\"))  # 1=Sunday, 7=Saturday\n",
    "    .withColumn(\"is_weekend\", F.when(F.col(\"arrival_day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "    .withColumn(\"arrival_date\", F.to_date(\"arrived_ts\"))\n",
    ")\n",
    "\n",
    "# Location type feature\n",
    "df_features = df_features.withColumn(\n",
    "    \"location_type\",\n",
    "    F.when(F.col(\"store_id\").isNotNull(), \"STORE\").otherwise(\"DC\")\n",
    ")\n",
    "\n",
    "# Get location ID (store_id or dc_id)\n",
    "df_features = df_features.withColumn(\n",
    "    \"location_id\",\n",
    "    F.when(F.col(\"store_id\").isNotNull(), F.col(\"store_id\")).otherwise(F.col(\"dc_id\"))\n",
    ")\n",
    "\n",
    "print(f\"Features created: {df_features.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with dimension tables for additional context\n",
    "print(\"\\nEnriching with dimension data...\")\n",
    "\n",
    "# Get truck information\n",
    "df_trucks = read_silver(\"dim_trucks\").select(\n",
    "    F.col(\"truck_id\"),\n",
    "    F.col(\"capacity_cubic_feet\").alias(\"truck_capacity\"),\n",
    "    F.col(\"fuel_type\").alias(\"truck_fuel_type\")\n",
    ")\n",
    "\n",
    "df_features = df_features.join(df_trucks, on=\"truck_id\", how=\"left\")\n",
    "\n",
    "# Get store/DC information\n",
    "df_stores = read_silver(\"dim_stores\").select(\n",
    "    F.col(\"store_id\"),\n",
    "    F.col(\"square_feet\").alias(\"location_size\")\n",
    ")\n",
    "\n",
    "df_dcs = read_silver(\"dim_distribution_centers\").select(\n",
    "    F.col(\"dc_id\"),\n",
    "    F.col(\"square_feet\").alias(\"location_size\")\n",
    ")\n",
    "\n",
    "# Join stores for STORE type\n",
    "df_features = df_features.join(\n",
    "    df_stores, \n",
    "    on=\"store_id\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Join DCs for DC type (coalesce location_size)\n",
    "df_features_dc = df_features.join(\n",
    "    df_dcs,\n",
    "    on=\"dc_id\",\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"location_size\",\n",
    "    F.coalesce(F.col(\"location_size\"), df_dcs[\"location_size\"])\n",
    ")\n",
    "\n",
    "df_features = df_features_dc\n",
    "\n",
    "print(\"Dimension enrichment complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical average dwell time by location\n",
    "print(\"\\nCalculating historical averages...\")\n",
    "\n",
    "# Calculate average dwell time per location\n",
    "df_location_avg = (\n",
    "    df_features\n",
    "    .groupBy(\"location_id\", \"location_type\")\n",
    "    .agg(\n",
    "        F.avg(\"dwell_minutes\").alias(\"location_avg_dwell\"),\n",
    "        F.count(\"*\").alias(\"location_shipment_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_features = df_features.join(\n",
    "    df_location_avg,\n",
    "    on=[\"location_id\", \"location_type\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Calculate average dwell time by hour of day\n",
    "df_hour_avg = (\n",
    "    df_features\n",
    "    .groupBy(\"arrival_hour\")\n",
    "    .agg(F.avg(\"dwell_minutes\").alias(\"hour_avg_dwell\"))\n",
    ")\n",
    "\n",
    "df_features = df_features.join(\n",
    "    df_hour_avg,\n",
    "    on=\"arrival_hour\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Historical features added.\")\n",
    "print(f\"\\nFinal feature set: {df_features.count()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Model Training\n",
    "\n",
    "Train LightGBM models for point prediction and quantile regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert to pandas for training\n",
    "feature_cols = [\n",
    "    \"arrival_hour\",\n",
    "    \"arrival_day_of_week\",\n",
    "    \"is_weekend\",\n",
    "    \"location_type\",\n",
    "    \"truck_capacity\",\n",
    "    \"location_size\",\n",
    "    \"location_avg_dwell\",\n",
    "    \"location_shipment_count\",\n",
    "    \"hour_avg_dwell\"\n",
    "]\n",
    "\n",
    "target_col = \"dwell_minutes\"\n",
    "\n",
    "# Select features and target\n",
    "df_model = df_features.select(\n",
    "    [\"shipment_id\", \"arrived_ts\", \"departed_ts\"] + feature_cols + [target_col]\n",
    ").na.drop()  # Drop rows with missing values\n",
    "\n",
    "print(f\"Records for modeling: {df_model.count()}\")\n",
    "\n",
    "# Convert to pandas\n",
    "pdf = df_model.toPandas()\n",
    "\n",
    "# Encode categorical features\n",
    "pdf['location_type'] = pdf['location_type'].map({'STORE': 0, 'DC': 1})\n",
    "\n",
    "# Prepare training data\n",
    "X = pdf[feature_cols]\n",
    "y = pdf[target_col]\n",
    "\n",
    "print(f\"\\nFeature shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train main model (point prediction)\n",
    "print(\"\\nTraining point prediction model...\")\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=100,\n",
    "    valid_sets=[test_data],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
    ")\n",
    "\n",
    "print(\"Point prediction model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train quantile models for confidence intervals (10th and 90th percentiles)\n",
    "print(\"\\nTraining quantile models for confidence intervals...\")\n",
    "\n",
    "# Lower bound (10th percentile)\n",
    "params_lower = params.copy()\n",
    "params_lower['objective'] = 'quantile'\n",
    "params_lower['alpha'] = 0.10\n",
    "\n",
    "model_lower = lgb.train(\n",
    "    params_lower,\n",
    "    train_data,\n",
    "    num_boost_round=100,\n",
    "    valid_sets=[test_data],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
    ")\n",
    "\n",
    "# Upper bound (90th percentile)\n",
    "params_upper = params.copy()\n",
    "params_upper['objective'] = 'quantile'\n",
    "params_upper['alpha'] = 0.90\n",
    "\n",
    "model_upper = lgb.train(\n",
    "    params_upper,\n",
    "    train_data,\n",
    "    num_boost_round=100,\n",
    "    valid_sets=[test_data],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
    ")\n",
    "\n",
    "print(\"Quantile models trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  MAE:  {mae:.2f} minutes (Target: < {TARGET_MAE} min)\")\n",
    "print(f\"  MAPE: {mape*100:.2f}% (Target: < {TARGET_MAPE*100}%)\")\n",
    "print(f\"  RMSE: {rmse:.2f} minutes\")\n",
    "\n",
    "# Check if targets are met\n",
    "mae_pass = mae < TARGET_MAE\n",
    "mape_pass = mape < TARGET_MAPE\n",
    "\n",
    "print(f\"\\nTarget Achievement:\")\n",
    "print(f\"  MAE Target:  {'✓ PASS' if mae_pass else '✗ FAIL'}\")\n",
    "print(f\"  MAPE Target: {'✓ PASS' if mape_pass else '✗ FAIL'}\")\n",
    "\n",
    "if mae_pass and mape_pass:\n",
    "    print(\"\\n✓ Model meets all performance targets!\")\n",
    "else:\n",
    "    print(\"\\n⚠ Model does not meet all targets. Consider:\")\n",
    "    print(\"  - More training data\")\n",
    "    print(\"  - Additional features\")\n",
    "    print(\"  - Hyperparameter tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "print(\"\\nFeature Importance:\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Generate Predictions\n",
    "\n",
    "Create predictions for all completed shipments and save to Gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GENERATING PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate predictions for all data\n",
    "X_all = pdf[feature_cols]\n",
    "\n",
    "predictions = model.predict(X_all, num_iteration=model.best_iteration)\n",
    "lower_bound = model_lower.predict(X_all, num_iteration=model_lower.best_iteration)\n",
    "upper_bound = model_upper.predict(X_all, num_iteration=model_upper.best_iteration)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "pdf['predicted_dwell_minutes'] = predictions\n",
    "pdf['lower_bound_minutes'] = lower_bound\n",
    "pdf['upper_bound_minutes'] = upper_bound\n",
    "pdf['prediction_error'] = pdf['dwell_minutes'] - pdf['predicted_dwell_minutes']\n",
    "pdf['prediction_timestamp'] = datetime.now(timezone.utc)\n",
    "\n",
    "# Decode location_type back to string\n",
    "pdf['location_type'] = pdf['location_type'].map({0: 'STORE', 1: 'DC'})\n",
    "\n",
    "print(f\"Generated {len(pdf)} predictions.\")\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(pdf[[\n",
    "    'shipment_id', 'arrived_ts', 'dwell_minutes', \n",
    "    'predicted_dwell_minutes', 'lower_bound_minutes', 'upper_bound_minutes'\n",
    "]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to Spark DataFrame and save to Gold\n",
    "print(\"\\nSaving predictions to Gold...\")\n",
    "\n",
    "output_cols = [\n",
    "    'shipment_id',\n",
    "    'arrived_ts',\n",
    "    'departed_ts',\n",
    "    'location_type',\n",
    "    'arrival_hour',\n",
    "    'arrival_day_of_week',\n",
    "    'dwell_minutes',\n",
    "    'predicted_dwell_minutes',\n",
    "    'lower_bound_minutes',\n",
    "    'upper_bound_minutes',\n",
    "    'prediction_error',\n",
    "    'prediction_timestamp'\n",
    "]\n",
    "\n",
    "df_predictions = spark.createDataFrame(pdf[output_cols])\n",
    "\n",
    "save_gold(df_predictions, \"gold_dwell_predictions\")\n",
    "\n",
    "print(\"\\n✓ Predictions saved to gold_dwell_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DELIVERY TIME PREDICTION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  MAE:  {mae:.2f} minutes\")\n",
    "print(f\"  MAPE: {mape*100:.2f}%\")\n",
    "print(f\"  RMSE: {rmse:.2f} minutes\")\n",
    "\n",
    "print(f\"\\nOutput Table:\")\n",
    "print(f\"  {GOLD_DB}.gold_dwell_predictions\")\n",
    "print(f\"  Rows: {df_predictions.count()}\")\n",
    "\n",
    "print(f\"\\nTop Features:\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.0f}\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Create logistics planning dashboard\")\n",
    "print(\"  2. Integrate predictions with dock scheduling system\")\n",
    "print(\"  3. Monitor model performance over time\")\n",
    "print(\"  4. Retrain model periodically with new data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
