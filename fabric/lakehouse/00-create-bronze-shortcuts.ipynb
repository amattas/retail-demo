{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bronze Layer Shortcuts (cusn schema)\n",
    "\n",
    "This notebook creates the Bronze layer (cusn schema) with shortcuts to both:\n",
    "- **Batch Historical Data**: 24 parquet tables in ADLSv2 (6 dimensions + 18 facts)\n",
    "- **Streaming Real-Time Data**: 18 event tables in Eventhouse\n",
    "\n",
    "The Bronze layer serves as the unified source for the Silver layer transformation.\n",
    "\n",
    "## Prerequisites\n",
    "- ADLSv2 storage account with parquet files (exported from datagen)\n",
    "- Eventhouse with streaming event tables\n",
    "- Lakehouse workspace permissions\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "ADLSv2 Parquet (24 tables) ──shortcut──> cusn.dim_*, cusn.fact_*\n",
    "Eventhouse Events (18 tables) ──shortcut──> cusn.receipt_created, etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "# Replace these with your actual Azure resource values\n",
    "\n",
    "# ADLSv2 Configuration\n",
    "ADLS_ACCOUNT = \"stdretail\"  # Storage account name\n",
    "ADLS_CONTAINER = \"supermarket\"  # Container name\n",
    "ADLS_BASE_PATH = \"\"  # Base path within container (empty for root)\n",
    "\n",
    "# Eventhouse Configuration  \n",
    "EVENTHOUSE_URI = \"<replace-with-eventhouse-uri>\"  # e.g., https://xyz.kusto.windows.net\n",
    "EVENTHOUSE_DATABASE = \"kql_retail_db\"  # KQL database name\n",
    "\n",
    "# Schema Names\n",
    "BRONZE_SCHEMA = \"cusn\"\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  ADLSv2: abfss://{ADLS_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/{ADLS_BASE_PATH}\")\n",
    "print(f\"  Eventhouse: {EVENTHOUSE_URI}/{EVENTHOUSE_DATABASE}\")\n",
    "print(f\"  Bronze Schema: {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Bronze Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cusn schema\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {BRONZE_SCHEMA}\")\n",
    "print(f\"✓ Schema '{BRONZE_SCHEMA}' created or already exists\")\n",
    "\n",
    "# Verify schema creation\n",
    "schemas = spark.sql(\"SHOW SCHEMAS\").collect()\n",
    "if any(row.namespace == BRONZE_SCHEMA for row in schemas):\n",
    "    print(f\"✓ Verified: Schema '{BRONZE_SCHEMA}' exists\")\n",
    "else:\n",
    "    raise Exception(f\"Failed to create schema '{BRONZE_SCHEMA}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Shortcuts to ADLSv2 Parquet Tables (Batch Historical Data)\n",
    "\n",
    "Creates 24 shortcuts to parquet files in ADLSv2:\n",
    "- 6 Dimension Tables\n",
    "- 18 Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimension tables\n",
    "dimension_tables = [\n",
    "    \"dim_geographies\",\n",
    "    \"dim_stores\",\n",
    "    \"dim_distribution_centers\",\n",
    "    \"dim_trucks\",\n",
    "    \"dim_customers\",\n",
    "    \"dim_products\"\n",
    "]\n",
    "\n",
    "# Define fact tables (all 18)\n",
    "fact_tables = [\n",
    "    \"fact_receipts\",\n",
    "    \"fact_receipt_lines\",\n",
    "    \"fact_store_inventory_txn\",\n",
    "    \"fact_dc_inventory_txn\",\n",
    "    \"fact_truck_moves\",\n",
    "    \"fact_truck_inventory\",\n",
    "    \"fact_foot_traffic\",\n",
    "    \"fact_ble_pings\",\n",
    "    \"fact_customer_zone_changes\",\n",
    "    \"fact_marketing\",\n",
    "    \"fact_online_order_headers\",\n",
    "    \"fact_online_order_lines\",\n",
    "    \"fact_payments\",\n",
    "    \"fact_store_ops\",\n",
    "    \"fact_stockouts\",\n",
    "    \"fact_promotions\",\n",
    "    \"fact_promo_lines\",\n",
    "    \"fact_reorders\"\n",
    "]\n",
    "\n",
    "all_parquet_tables = dimension_tables + fact_tables\n",
    "print(f\"Total parquet tables to create shortcuts for: {len(all_parquet_tables)}\")\n",
    "print(f\"  Dimensions: {len(dimension_tables)}\")\n",
    "print(f\"  Facts: {len(fact_tables)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "\n",
    "# Helper function to create ADLSv2 shortcut\n",
    "def create_adls_shortcut(table_name: str, schema: str = BRONZE_SCHEMA) -> bool:\n",
    "    \"\"\"\n",
    "    Create a shortcut to an ADLSv2 parquet table.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table (e.g., 'dim_stores')\n",
    "        schema: Target schema name (default: cusn)\n",
    "        \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct ADLS path\n",
    "        adls_path = f\"abfss://{ADLS_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/{ADLS_BASE_PATH}{table_name}/\"\n",
    "        \n",
    "        # Create shortcut using mssparkutils\n",
    "        # Note: This API may vary based on Fabric version\n",
    "        # Alternative: Use Fabric REST API or Portal for manual creation\n",
    "        \n",
    "        # For now, we'll create external tables pointing to ADLS\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {schema}.{table_name}\n",
    "            USING PARQUET\n",
    "            LOCATION '{adls_path}'\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"  ✓ {schema}.{table_name} -> {adls_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed to create {schema}.{table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create shortcuts for all parquet tables\n",
    "print(f\"\\nCreating shortcuts to ADLSv2 parquet tables...\\n\")\n",
    "success_count = 0\n",
    "failed_tables = []\n",
    "\n",
    "for table in all_parquet_tables:\n",
    "    if create_adls_shortcut(table):\n",
    "        success_count += 1\n",
    "    else:\n",
    "        failed_tables.append(table)\n",
    "\n",
    "print(f\"\\nADLSv2 Shortcut Creation Summary:\")\n",
    "print(f\"  Success: {success_count}/{len(all_parquet_tables)}\")\n",
    "if failed_tables:\n",
    "    print(f\"  Failed tables: {', '.join(failed_tables)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Shortcuts to Eventhouse Event Tables (Streaming Real-Time Data)\n",
    "\n",
    "Creates 18 shortcuts to streaming event tables in Eventhouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define streaming event tables (matches EventType enum)\n",
    "event_tables = [\n",
    "    # Transaction Events\n",
    "    \"receipt_created\",\n",
    "    \"receipt_line_added\",\n",
    "    \"payment_processed\",\n",
    "    \n",
    "    # Inventory Events\n",
    "    \"inventory_updated\",\n",
    "    \"stockout_detected\",\n",
    "    \"reorder_triggered\",\n",
    "    \n",
    "    # Customer Events\n",
    "    \"customer_entered\",\n",
    "    \"customer_zone_changed\",\n",
    "    \"ble_ping_detected\",\n",
    "    \n",
    "    # Operational Events\n",
    "    \"truck_arrived\",\n",
    "    \"truck_departed\",\n",
    "    \"store_opened\",\n",
    "    \"store_closed\",\n",
    "    \n",
    "    # Marketing Events\n",
    "    \"ad_impression\",\n",
    "    \"promotion_applied\",\n",
    "    \n",
    "    # Omnichannel Events\n",
    "    \"online_order_created\",\n",
    "    \"online_order_picked\",\n",
    "    \"online_order_shipped\"\n",
    "]\n",
    "\n",
    "print(f\"Total event tables to create shortcuts for: {len(event_tables)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eventhouse_shortcut(table_name: str, schema: str = BRONZE_SCHEMA) -> bool:\n",
    "    \"\"\"\n",
    "    Create a shortcut to an Eventhouse KQL table.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the event table (e.g., 'receipt_created')\n",
    "        schema: Target schema name (default: cusn)\n",
    "        \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct Eventhouse URI\n",
    "        kql_uri = f\"{EVENTHOUSE_URI}/{EVENTHOUSE_DATABASE}\"\n",
    "        \n",
    "        # Create shortcut to Eventhouse table\n",
    "        # Note: Syntax may vary - this is for Fabric Lakehouse shortcuts to KQL\n",
    "        # Alternative: Use Fabric UI or REST API for shortcut creation\n",
    "        \n",
    "        # For external KQL tables, use USING delta with options\n",
    "        # This is a placeholder - actual syntax depends on Fabric's shortcut API\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {schema}.{table_name}\n",
    "            USING org.apache.spark.sql.eventhouse\n",
    "            OPTIONS (\n",
    "                'eventhouse.uri' = '{kql_uri}',\n",
    "                'eventhouse.table' = '{table_name}'\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"  ✓ {schema}.{table_name} -> {kql_uri}/{table_name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Eventhouse shortcuts may need to be created via UI or REST API\n",
    "        print(f\"  ⚠ {schema}.{table_name}: Shortcut may need manual creation in Fabric UI\")\n",
    "        print(f\"    Source: {EVENTHOUSE_URI}/{EVENTHOUSE_DATABASE}.{table_name}\")\n",
    "        return False\n",
    "\n",
    "# Create shortcuts for all event tables\n",
    "print(f\"\\nCreating shortcuts to Eventhouse event tables...\\n\")\n",
    "print(f\"Note: Eventhouse shortcuts may require manual creation via Fabric UI\")\n",
    "print(f\"      if the programmatic API is not available.\\n\")\n",
    "\n",
    "event_success_count = 0\n",
    "event_failed_tables = []\n",
    "\n",
    "for table in event_tables:\n",
    "    if create_eventhouse_shortcut(table):\n",
    "        event_success_count += 1\n",
    "    else:\n",
    "        event_failed_tables.append(table)\n",
    "\n",
    "print(f\"\\nEventhouse Shortcut Creation Summary:\")\n",
    "print(f\"  Attempted: {event_success_count}/{len(event_tables)}\")\n",
    "if event_failed_tables:\n",
    "    print(f\"  Manual creation needed for: {', '.join(event_failed_tables)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify Bronze Layer Shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in Bronze schema\n",
    "bronze_tables = spark.sql(f\"SHOW TABLES IN {BRONZE_SCHEMA}\").collect()\n",
    "\n",
    "print(f\"\\nBronze Schema ({BRONZE_SCHEMA}) Tables:\")\n",
    "print(f\"  Total: {len(bronze_tables)} tables\\n\")\n",
    "\n",
    "# Categorize tables\n",
    "dim_count = sum(1 for t in bronze_tables if t.tableName.startswith('dim_'))\n",
    "fact_count = sum(1 for t in bronze_tables if t.tableName.startswith('fact_'))\n",
    "event_count = len(bronze_tables) - dim_count - fact_count\n",
    "\n",
    "print(f\"  Dimensions: {dim_count} (expected: 6)\")\n",
    "print(f\"  Facts: {fact_count} (expected: 18)\")\n",
    "print(f\"  Events: {event_count} (expected: 18)\")\n",
    "print(f\"\\n  Target: 42 tables (6 dims + 18 facts + 18 events)\")\n",
    "\n",
    "if len(bronze_tables) == 42:\n",
    "    print(f\"\\n✓ Bronze layer complete with all 42 shortcuts!\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Bronze layer has {len(bronze_tables)}/42 tables\")\n",
    "    print(f\"  Check for missing tables and create shortcuts manually if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Bronze Layer Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reading from Bronze shortcuts\n",
    "print(\"\\nTesting Bronze layer shortcuts...\\n\")\n",
    "\n",
    "# Test dimension shortcut\n",
    "try:\n",
    "    df_dim = spark.table(f\"{BRONZE_SCHEMA}.dim_stores\")\n",
    "    print(f\"✓ dim_stores: {df_dim.count()} rows\")\n",
    "    df_dim.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"✗ dim_stores failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Test fact shortcut\n",
    "try:\n",
    "    df_fact = spark.table(f\"{BRONZE_SCHEMA}.fact_receipts\")\n",
    "    print(f\"✓ fact_receipts: {df_fact.count()} rows\")\n",
    "    df_fact.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"✗ fact_receipts failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Test event shortcut (if available)\n",
    "try:\n",
    "    df_event = spark.table(f\"{BRONZE_SCHEMA}.receipt_created\")\n",
    "    print(f\"✓ receipt_created: {df_event.count()} rows\")\n",
    "    df_event.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"⚠ receipt_created: {e}\")\n",
    "    print(f\"  (Event shortcuts may need manual creation)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nBronze layer testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has created the Bronze layer (cusn schema) with shortcuts to:\n",
    "\n",
    "### ADLSv2 Parquet Tables (24 tables)\n",
    "- 6 Dimension tables: dim_*\n",
    "- 18 Fact tables: fact_*\n",
    "\n",
    "### Eventhouse Event Tables (18 tables)\n",
    "- Transaction events: receipt_created, receipt_line_added, payment_processed\n",
    "- Inventory events: inventory_updated, stockout_detected, reorder_triggered\n",
    "- Customer events: customer_entered, customer_zone_changed, ble_ping_detected\n",
    "- Operational events: truck_arrived, truck_departed, store_opened, store_closed\n",
    "- Marketing events: ad_impression, promotion_applied\n",
    "- Omnichannel events: online_order_created, online_order_picked, online_order_shipped\n",
    "\n",
    "### Next Steps\n",
    "1. If any shortcuts failed, create them manually via Fabric UI\n",
    "2. Verify all 42 shortcuts are accessible\n",
    "3. Run the Silver transformation notebook (02-onelake-to-silver.ipynb)\n",
    "4. Silver layer will combine batch + streaming data into unified ag.* tables\n",
    "\n",
    "### Manual Shortcut Creation (if needed)\n",
    "For Eventhouse shortcuts that couldn't be created programmatically:\n",
    "1. Navigate to Lakehouse in Fabric workspace\n",
    "2. Right-click on Tables\n",
    "3. Select \"New shortcut\" → \"Eventhouse\"\n",
    "4. Enter Eventhouse URI and table name\n",
    "5. Target schema: cusn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
