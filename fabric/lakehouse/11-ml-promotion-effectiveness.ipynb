{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Enhancement: Promotion Effectiveness Analysis\n",
        "\n",
        "Analyzes price elasticity and promotional lift to optimize marketing spend.\n",
        "\n",
        "## Business Objective\n",
        "- Identify which promotions drive incremental sales\n",
        "- Understand price sensitivity by product category\n",
        "- Measure promotional ROI and cannibalization effects\n",
        "- Optimize discount levels and promotional calendar\n",
        "\n",
        "## Method\n",
        "- **Price Elasticity**: Log-log regression of quantity vs price\n",
        "- **Incremental Lift**: Compare promoted vs. baseline sales periods\n",
        "- **Cannibalization**: Measure sales decline in adjacent periods\n",
        "\n",
        "## Data Flow\n",
        "```\n",
        "fact_receipt_lines + fact_promotions + dim_products\n",
        "  --> gold_price_elasticity (product-level)\n",
        "  --> gold_promotion_lift (promotion-level)\n",
        "```\n",
        "\n",
        "## Usage\n",
        "Run this notebook after historical data load to establish baseline metrics.\n",
        "Re-run periodically (weekly/monthly) to track promotional effectiveness trends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "def get_env(var_name: str, default: str | None = None) -> str | None:\n",
        "    return os.environ.get(var_name, default)\n",
        "\n",
        "SILVER_DB = get_env(\"SILVER_DB\", default=\"ag\")\n",
        "GOLD_DB = get_env(\"GOLD_DB\", default=\"au\")\n",
        "\n",
        "# Analysis parameters\n",
        "MIN_OBSERVATIONS = 30  # Minimum data points for elasticity calculation\n",
        "BASELINE_WINDOW_DAYS = 30  # Days before/after promo for baseline\n",
        "TOP_N_PRODUCTS = 100  # Top products by revenue to analyze\n",
        "\n",
        "print(f\"Configuration: SILVER_DB={SILVER_DB}, GOLD_DB={GOLD_DB}\")\n",
        "print(f\"Analysis Parameters:\")\n",
        "print(f\"  MIN_OBSERVATIONS: {MIN_OBSERVATIONS}\")\n",
        "print(f\"  BASELINE_WINDOW_DAYS: {BASELINE_WINDOW_DAYS}\")\n",
        "print(f\"  TOP_N_PRODUCTS: {TOP_N_PRODUCTS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HELPER FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def ensure_database(name: str) -> None:\n",
        "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {name}\")\n",
        "\n",
        "def read_silver(table_name: str):\n",
        "    return spark.table(f\"{SILVER_DB}.{table_name}\")\n",
        "\n",
        "def save_gold(df, table_name: str) -> None:\n",
        "    full_name = f\"{GOLD_DB}.{table_name}\"\n",
        "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_name)\n",
        "    print(f\"  {full_name}: {df.count()} rows\")\n",
        "\n",
        "def silver_exists(table_name: str) -> bool:\n",
        "    try:\n",
        "        spark.table(f\"{SILVER_DB}.{table_name}\")\n",
        "        return True\n",
        "    except AnalysisException:\n",
        "        return False\n",
        "\n",
        "ensure_database(GOLD_DB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Data Preparation\n",
        "\n",
        "Join receipt lines with product dimensions and promotion data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"DATA PREPARATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check required tables exist\n",
        "required_tables = [\"fact_receipt_lines\", \"dim_products\", \"fact_promotions\"]\n",
        "missing_tables = [t for t in required_tables if not silver_exists(t)]\n",
        "\n",
        "if missing_tables:\n",
        "    print(f\"ERROR: Missing required tables: {missing_tables}\")\n",
        "    print(\"Cannot proceed with analysis.\")\n",
        "    raise Exception(f\"Missing required tables: {missing_tables}\")\n",
        "\n",
        "print(\"All required tables present.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dimension and fact tables\n",
        "df_receipt_lines = read_silver(\"fact_receipt_lines\")\n",
        "df_products = read_silver(\"dim_products\")\n",
        "df_promotions = read_silver(\"fact_promotions\")\n",
        "\n",
        "print(f\"Receipt lines: {df_receipt_lines.count():,} rows\")\n",
        "print(f\"Products: {df_products.count():,} rows\")\n",
        "print(f\"Promotions: {df_promotions.count():,} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enrich receipt lines with product attributes\n",
        "df_sales = (\n",
        "    df_receipt_lines\n",
        "    .join(df_products, on=\"product_id\", how=\"inner\")\n",
        "    .select(\n",
        "        \"event_ts\",\n",
        "        \"receipt_id_ext\",\n",
        "        \"product_id\",\n",
        "        F.col(\"name\").alias(\"product_name\"),\n",
        "        F.col(\"category\").alias(\"product_category\"),\n",
        "        F.col(\"subcategory\").alias(\"product_subcategory\"),\n",
        "        \"quantity\",\n",
        "        \"unit_price\",\n",
        "        \"ext_price\",\n",
        "        \"promo_code\",\n",
        "        F.col(\"base_price\").alias(\"regular_price\"),\n",
        "        F.col(\"base_cost\").alias(\"product_cost\")\n",
        "    )\n",
        "    .withColumn(\"date\", F.to_date(\"event_ts\"))\n",
        "    .withColumn(\n",
        "        \"is_promoted\",\n",
        "        F.when(F.col(\"promo_code\").isNotNull(), 1).otherwise(0)\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"discount_pct\",\n",
        "        F.when(\n",
        "            (F.col(\"regular_price\") > 0) & (F.col(\"unit_price\") < F.col(\"regular_price\")),\n",
        "            ((F.col(\"regular_price\") - F.col(\"unit_price\")) / F.col(\"regular_price\")) * 100\n",
        "        ).otherwise(0.0)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"\\nEnriched sales data: {df_sales.count():,} rows\")\n",
        "df_sales.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Price Elasticity Estimation\n",
        "\n",
        "Calculate price elasticity using log-log regression:\n",
        "```\n",
        "log(quantity) = β₀ + β₁ * log(price) + ε\n",
        "```\n",
        "Where β₁ is the price elasticity coefficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"PRICE ELASTICITY ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Identify top products by revenue for analysis\n",
        "df_top_products = (\n",
        "    df_sales\n",
        "    .groupBy(\"product_id\", \"product_name\", \"product_category\")\n",
        "    .agg(\n",
        "        F.sum(\"ext_price\").alias(\"total_revenue\"),\n",
        "        F.sum(\"quantity\").alias(\"total_units\"),\n",
        "        F.countDistinct(\"date\").alias(\"days_sold\")\n",
        "    )\n",
        "    .filter(F.col(\"days_sold\") >= MIN_OBSERVATIONS)\n",
        "    .orderBy(F.desc(\"total_revenue\"))\n",
        "    .limit(TOP_N_PRODUCTS)\n",
        ")\n",
        "\n",
        "print(f\"\\nAnalyzing top {TOP_N_PRODUCTS} products with >= {MIN_OBSERVATIONS} days of sales data\")\n",
        "df_top_products.show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate sales by product and date for elasticity calculation\n",
        "df_daily_sales = (\n",
        "    df_sales\n",
        "    .join(\n",
        "        df_top_products.select(\"product_id\"),\n",
        "        on=\"product_id\",\n",
        "        how=\"inner\"\n",
        "    )\n",
        "    .groupBy(\"product_id\", \"product_name\", \"product_category\", \"date\")\n",
        "    .agg(\n",
        "        F.sum(\"quantity\").alias(\"daily_quantity\"),\n",
        "        F.avg(\"unit_price\").alias(\"avg_price\"),\n",
        "        F.max(\"regular_price\").alias(\"regular_price\"),\n",
        "        F.max(\"is_promoted\").alias(\"is_promoted\")\n",
        "    )\n",
        "    .filter(\n",
        "        (F.col(\"daily_quantity\") > 0) &\n",
        "        (F.col(\"avg_price\") > 0)\n",
        "    )\n",
        "    .withColumn(\"log_quantity\", F.log(F.col(\"daily_quantity\")))\n",
        "    .withColumn(\"log_price\", F.log(F.col(\"avg_price\")))\n",
        ")\n",
        "\n",
        "print(f\"\\nDaily sales aggregation: {df_daily_sales.count():,} product-day observations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate elasticity statistics per product using aggregation\n",
        "# Elasticity = covariance(log_price, log_quantity) / variance(log_price)\n",
        "\n",
        "df_elasticity_stats = (\n",
        "    df_daily_sales\n",
        "    .groupBy(\"product_id\", \"product_name\", \"product_category\")\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"n_observations\"),\n",
        "        F.avg(\"log_price\").alias(\"mean_log_price\"),\n",
        "        F.avg(\"log_quantity\").alias(\"mean_log_quantity\"),\n",
        "        F.stddev(\"log_price\").alias(\"stddev_log_price\"),\n",
        "        F.stddev(\"log_quantity\").alias(\"stddev_log_quantity\"),\n",
        "        F.avg(\"avg_price\").alias(\"avg_price\"),\n",
        "        F.avg(\"regular_price\").alias(\"regular_price\"),\n",
        "        F.avg(\"daily_quantity\").alias(\"avg_daily_quantity\")\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"\\nElasticity statistics calculated for {df_elasticity_stats.count():,} products\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Join back to calculate covariance and elasticity\n",
        "df_elasticity_calc = (\n",
        "    df_daily_sales\n",
        "    .join(df_elasticity_stats, on=[\"product_id\", \"product_name\", \"product_category\"], how=\"inner\")\n",
        "    .withColumn(\n",
        "        \"price_deviation\",\n",
        "        F.col(\"log_price\") - F.col(\"mean_log_price\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"quantity_deviation\",\n",
        "        F.col(\"log_quantity\") - F.col(\"mean_log_quantity\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"cross_product\",\n",
        "        F.col(\"price_deviation\") * F.col(\"quantity_deviation\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"price_sq_deviation\",\n",
        "        F.col(\"price_deviation\") * F.col(\"price_deviation\")\n",
        "    )\n",
        ")\n",
        "\n",
        "df_price_elasticity = (\n",
        "    df_elasticity_calc\n",
        "    .groupBy(\"product_id\", \"product_name\", \"product_category\")\n",
        "    .agg(\n",
        "        F.first(\"n_observations\").alias(\"n_observations\"),\n",
        "        F.first(\"avg_price\").alias(\"avg_price\"),\n",
        "        F.first(\"regular_price\").alias(\"regular_price\"),\n",
        "        F.first(\"avg_daily_quantity\").alias(\"avg_daily_quantity\"),\n",
        "        (F.sum(\"cross_product\") / F.sum(\"price_sq_deviation\")).alias(\"elasticity_coefficient\"),\n",
        "        F.first(\"stddev_log_price\").alias(\"stddev_log_price\"),\n",
        "        F.first(\"stddev_log_quantity\").alias(\"stddev_log_quantity\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"elasticity_category\",\n",
        "        F.when(F.abs(F.col(\"elasticity_coefficient\")) > 1.5, \"Highly Elastic\")\n",
        "         .when(F.abs(F.col(\"elasticity_coefficient\")) > 1.0, \"Elastic\")\n",
        "         .when(F.abs(F.col(\"elasticity_coefficient\")) > 0.5, \"Unit Elastic\")\n",
        "         .otherwise(\"Inelastic\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"standard_error\",\n",
        "        F.col(\"stddev_log_quantity\") / F.sqrt(F.col(\"n_observations\"))\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"confidence_interval_lower\",\n",
        "        F.col(\"elasticity_coefficient\") - (1.96 * F.col(\"standard_error\"))\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"confidence_interval_upper\",\n",
        "        F.col(\"elasticity_coefficient\") + (1.96 * F.col(\"standard_error\"))\n",
        "    )\n",
        "    .withColumn(\"computed_at\", F.current_timestamp())\n",
        "    .select(\n",
        "        \"product_id\",\n",
        "        \"product_name\",\n",
        "        \"product_category\",\n",
        "        \"elasticity_coefficient\",\n",
        "        \"elasticity_category\",\n",
        "        \"confidence_interval_lower\",\n",
        "        \"confidence_interval_upper\",\n",
        "        \"n_observations\",\n",
        "        \"avg_price\",\n",
        "        \"regular_price\",\n",
        "        \"avg_daily_quantity\",\n",
        "        \"computed_at\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"\\nPrice elasticity calculated for {df_price_elasticity.count():,} products\")\n",
        "print(\"\\nSample results:\")\n",
        "df_price_elasticity.orderBy(F.abs(F.col(\"elasticity_coefficient\")), ascending=False).show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save price elasticity to Gold layer\n",
        "print(\"\\nSaving gold_price_elasticity...\")\n",
        "save_gold(df_price_elasticity, \"gold_price_elasticity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: Promotion Lift Analysis\n",
        "\n",
        "Measure incremental sales lift from promotions by comparing:\n",
        "- Baseline sales (non-promoted periods)\n",
        "- Promoted sales (during promotion)\n",
        "- Post-promotion sales (cannibalization effect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"PROMOTION LIFT ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Aggregate promotion events by promo_code and date range\n",
        "df_promo_periods = (\n",
        "    df_promotions\n",
        "    .withColumn(\"promo_date\", F.to_date(\"event_ts\"))\n",
        "    .groupBy(\"promo_code\", \"discount_type\")\n",
        "    .agg(\n",
        "        F.min(\"promo_date\").alias(\"promo_start_date\"),\n",
        "        F.max(\"promo_date\").alias(\"promo_end_date\"),\n",
        "        F.countDistinct(\"receipt_id\").alias(\"promoted_receipts\"),\n",
        "        F.avg(\"discount_amount\").alias(\"avg_discount\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"baseline_start_date\",\n",
        "        F.date_sub(F.col(\"promo_start_date\"), BASELINE_WINDOW_DAYS)\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"baseline_end_date\",\n",
        "        F.date_sub(F.col(\"promo_start_date\"), 1)\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"post_promo_start_date\",\n",
        "        F.date_add(F.col(\"promo_end_date\"), 1)\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"post_promo_end_date\",\n",
        "        F.date_add(F.col(\"promo_end_date\"), BASELINE_WINDOW_DAYS)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"\\nPromotion periods identified: {df_promo_periods.count():,}\")\n",
        "df_promo_periods.show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate baseline sales (pre-promotion)\n",
        "df_sales_with_promo = df_sales.alias(\"sales\").join(\n",
        "    df_promo_periods.alias(\"promo\"),\n",
        "    F.col(\"sales.promo_code\") == F.col(\"promo.promo_code\"),\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "df_baseline_sales = (\n",
        "    df_sales_with_promo\n",
        "    .filter(\n",
        "        (F.col(\"sales.date\") >= F.col(\"promo.baseline_start_date\")) &\n",
        "        (F.col(\"sales.date\") <= F.col(\"promo.baseline_end_date\"))\n",
        "    )\n",
        "    .groupBy(\"promo.promo_code\", \"sales.product_id\")\n",
        "    .agg(\n",
        "        F.sum(\"sales.quantity\").alias(\"baseline_quantity\"),\n",
        "        F.sum(\"sales.ext_price\").alias(\"baseline_revenue\"),\n",
        "        F.countDistinct(\"sales.date\").alias(\"baseline_days\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"baseline_daily_quantity\",\n",
        "        F.col(\"baseline_quantity\") / F.col(\"baseline_days\")\n",
        "    )\n",
        "    .withColumnRenamed(\"promo_code\", \"baseline_promo_code\")\n",
        "    .withColumnRenamed(\"product_id\", \"baseline_product_id\")\n",
        ")\n",
        "\n",
        "print(f\"\\nBaseline sales calculated: {df_baseline_sales.count():,} promo-product combinations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate promoted sales\n",
        "df_promoted_sales = (\n",
        "    df_sales_with_promo\n",
        "    .filter(\n",
        "        (F.col(\"sales.date\") >= F.col(\"promo.promo_start_date\")) &\n",
        "        (F.col(\"sales.date\") <= F.col(\"promo.promo_end_date\"))\n",
        "    )\n",
        "    .groupBy(\"promo.promo_code\", \"sales.product_id\")\n",
        "    .agg(\n",
        "        F.sum(\"sales.quantity\").alias(\"promo_quantity\"),\n",
        "        F.sum(\"sales.ext_price\").alias(\"promo_revenue\"),\n",
        "        F.countDistinct(\"sales.date\").alias(\"promo_days\"),\n",
        "        F.avg(\"sales.discount_pct\").alias(\"avg_discount_pct\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"promo_daily_quantity\",\n",
        "        F.col(\"promo_quantity\") / F.col(\"promo_days\")\n",
        "    )\n",
        "    .withColumnRenamed(\"promo_code\", \"promo_promo_code\")\n",
        "    .withColumnRenamed(\"product_id\", \"promo_product_id\")\n",
        ")\n",
        "\n",
        "print(f\"\\nPromoted sales calculated: {df_promoted_sales.count():,} promo-product combinations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate post-promotion sales (cannibalization)\n",
        "df_post_promo_sales = (\n",
        "    df_sales_with_promo\n",
        "    .filter(\n",
        "        (F.col(\"sales.date\") >= F.col(\"promo.post_promo_start_date\")) &\n",
        "        (F.col(\"sales.date\") <= F.col(\"promo.post_promo_end_date\"))\n",
        "    )\n",
        "    .groupBy(\"promo.promo_code\", \"sales.product_id\")\n",
        "    .agg(\n",
        "        F.sum(\"sales.quantity\").alias(\"post_quantity\"),\n",
        "        F.sum(\"sales.ext_price\").alias(\"post_revenue\"),\n",
        "        F.countDistinct(\"sales.date\").alias(\"post_days\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"post_daily_quantity\",\n",
        "        F.col(\"post_quantity\") / F.col(\"post_days\")\n",
        "    )\n",
        "    .withColumnRenamed(\"promo_code\", \"post_promo_code\")\n",
        "    .withColumnRenamed(\"product_id\", \"post_product_id\")\n",
        ")\n",
        "\n",
        "print(f\"\\nPost-promotion sales calculated: {df_post_promo_sales.count():,} promo-product combinations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine and calculate lift metrics\n",
        "df_promotion_lift = (\n",
        "    df_promoted_sales\n",
        "    .join(\n",
        "        df_baseline_sales,\n",
        "        (F.col(\"promo_promo_code\") == F.col(\"baseline_promo_code\")) &\n",
        "        (F.col(\"promo_product_id\") == F.col(\"baseline_product_id\")),\n",
        "        how=\"left\"\n",
        "    )\n",
        "    .join(\n",
        "        df_post_promo_sales,\n",
        "        (F.col(\"promo_promo_code\") == F.col(\"post_promo_code\")) &\n",
        "        (F.col(\"promo_product_id\") == F.col(\"post_product_id\")),\n",
        "        how=\"left\"\n",
        "    )\n",
        "    .join(\n",
        "        df_products.select(\n",
        "            F.col(\"product_id\"),\n",
        "            F.col(\"name\").alias(\"product_name\"),\n",
        "            F.col(\"category\").alias(\"product_category\")\n",
        "        ),\n",
        "        F.col(\"promo_product_id\") == F.col(\"product_id\"),\n",
        "        how=\"inner\"\n",
        "    )\n",
        "    .join(\n",
        "        df_promo_periods.select(\n",
        "            F.col(\"promo_code\"),\n",
        "            F.col(\"discount_type\"),\n",
        "            F.col(\"avg_discount\"),\n",
        "            F.col(\"promo_start_date\"),\n",
        "            F.col(\"promo_end_date\")\n",
        "        ),\n",
        "        F.col(\"promo_promo_code\") == F.col(\"promo_code\"),\n",
        "        how=\"inner\"\n",
        "    )\n",
        "    .select(\n",
        "        F.col(\"promo_code\"),\n",
        "        F.col(\"promo_product_id\").alias(\"product_id\"),\n",
        "        \"product_name\",\n",
        "        \"product_category\",\n",
        "        \"discount_type\",\n",
        "        \"avg_discount\",\n",
        "        \"avg_discount_pct\",\n",
        "        \"promo_start_date\",\n",
        "        \"promo_end_date\",\n",
        "        F.coalesce(F.col(\"baseline_daily_quantity\"), F.lit(0.0)).alias(\"baseline_daily_quantity\"),\n",
        "        F.col(\"promo_daily_quantity\"),\n",
        "        F.coalesce(F.col(\"post_daily_quantity\"), F.lit(0.0)).alias(\"post_daily_quantity\"),\n",
        "        F.col(\"promo_quantity\").alias(\"total_promoted_quantity\"),\n",
        "        F.col(\"promo_revenue\").alias(\"total_promoted_revenue\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"incremental_lift_pct\",\n",
        "        F.when(\n",
        "            F.col(\"baseline_daily_quantity\") > 0,\n",
        "            ((F.col(\"promo_daily_quantity\") - F.col(\"baseline_daily_quantity\")) / \n",
        "             F.col(\"baseline_daily_quantity\")) * 100\n",
        "        ).otherwise(F.lit(None))\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"cannibalization_pct\",\n",
        "        F.when(\n",
        "            F.col(\"baseline_daily_quantity\") > 0,\n",
        "            ((F.col(\"baseline_daily_quantity\") - F.col(\"post_daily_quantity\")) / \n",
        "             F.col(\"baseline_daily_quantity\")) * 100\n",
        "        ).otherwise(F.lit(None))\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"net_lift_pct\",\n",
        "        F.col(\"incremental_lift_pct\") - F.col(\"cannibalization_pct\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"roi_category\",\n",
        "        F.when(F.col(\"net_lift_pct\") > 50, \"High ROI\")\n",
        "         .when(F.col(\"net_lift_pct\") > 20, \"Medium ROI\")\n",
        "         .when(F.col(\"net_lift_pct\") > 0, \"Low ROI\")\n",
        "         .otherwise(\"Negative ROI\")\n",
        "    )\n",
        "    .withColumn(\"computed_at\", F.current_timestamp())\n",
        ")\n",
        "\n",
        "print(f\"\\nPromotion lift analysis complete: {df_promotion_lift.count():,} promo-product combinations\")\n",
        "print(\"\\nSample results:\")\n",
        "df_promotion_lift.orderBy(F.desc(\"incremental_lift_pct\")).show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save promotion lift to Gold layer\n",
        "print(\"\\nSaving gold_promotion_lift...\")\n",
        "save_gold(df_promotion_lift, \"gold_promotion_lift\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary & Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PROMOTION EFFECTIVENESS ANALYSIS COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Elasticity summary\n",
        "print(\"\\n--- Price Elasticity Summary ---\")\n",
        "df_price_elasticity.groupBy(\"elasticity_category\").count().orderBy(F.desc(\"count\")).show()\n",
        "\n",
        "print(\"\\nMost price-sensitive products (highest elasticity):\")\n",
        "df_price_elasticity.orderBy(F.desc(\"elasticity_coefficient\")).select(\n",
        "    \"product_name\", \"product_category\", \"elasticity_coefficient\", \"elasticity_category\"\n",
        ").show(5, truncate=False)\n",
        "\n",
        "print(\"\\nLeast price-sensitive products (lowest elasticity):\")\n",
        "df_price_elasticity.orderBy(F.asc(\"elasticity_coefficient\")).select(\n",
        "    \"product_name\", \"product_category\", \"elasticity_coefficient\", \"elasticity_category\"\n",
        ").show(5, truncate=False)\n",
        "\n",
        "# Promotion lift summary\n",
        "print(\"\\n--- Promotion Lift Summary ---\")\n",
        "df_promotion_lift.groupBy(\"roi_category\").count().orderBy(F.desc(\"count\")).show()\n",
        "\n",
        "print(\"\\nTop performing promotions (highest net lift):\")\n",
        "df_promotion_lift.orderBy(F.desc(\"net_lift_pct\")).select(\n",
        "    \"promo_code\", \"product_name\", \"discount_type\", \n",
        "    \"incremental_lift_pct\", \"cannibalization_pct\", \"net_lift_pct\", \"roi_category\"\n",
        ").show(5, truncate=False)\n",
        "\n",
        "print(\"\\nPromotions with high cannibalization:\")\n",
        "df_promotion_lift.filter(F.col(\"cannibalization_pct\") > 20).orderBy(F.desc(\"cannibalization_pct\")).select(\n",
        "    \"promo_code\", \"product_name\", \"discount_type\",\n",
        "    \"incremental_lift_pct\", \"cannibalization_pct\", \"net_lift_pct\"\n",
        ").show(5, truncate=False)\n",
        "\n",
        "print(\"\\nGold tables created:\")\n",
        "print(f\"  - {GOLD_DB}.gold_price_elasticity\")\n",
        "print(f\"  - {GOLD_DB}.gold_promotion_lift\")\n",
        "print(\"\\nUse these tables for Power BI dashboards and further analysis.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
