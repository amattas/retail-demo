{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ML: Customer Churn Prediction\n",
    "\n",
    "Predicts customers at risk of churning using LightGBM classification.\n",
    "\n",
    "## Model Overview\n",
    "- **Target**: Binary churn (no purchase in configurable days)\n",
    "- **Algorithm**: LightGBM Classifier\n",
    "- **Features**: Behavioral (purchase patterns) + Demographic\n",
    "- **Validation**: 5-fold cross-validation\n",
    "- **Target Performance**: AUC-ROC > 0.75, Precision > 0.6\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "Silver (fact_receipts, dim_customers) --> Feature Engineering --> Model Training --> Gold (churn_predictions)\n",
    "```\n",
    "\n",
    "## Usage\n",
    "Schedule this notebook to run **weekly** via Fabric pipeline to refresh predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML libraries\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "def get_env(var_name, default=None):\n",
    "    return os.environ.get(var_name, default)\n",
    "\n",
    "SILVER_DB = get_env(\"SILVER_DB\", default=\"ag\")\n",
    "GOLD_DB = get_env(\"GOLD_DB\", default=\"au\")\n",
    "\n",
    "# Churn definition (days without purchase)\n",
    "CHURN_WINDOW_DAYS = int(get_env(\"CHURN_WINDOW_DAYS\", default=\"90\"))\n",
    "\n",
    "# Feature engineering window (days of historical data to analyze)\n",
    "FEATURE_WINDOW_DAYS = int(get_env(\"FEATURE_WINDOW_DAYS\", default=\"180\"))\n",
    "\n",
    "# Training window (exclude recent data for label stability)\n",
    "LABEL_OFFSET_DAYS = int(get_env(\"LABEL_OFFSET_DAYS\", default=\"7\"))\n",
    "\n",
    "# Model parameters\n",
    "RANDOM_STATE = 42\n",
    "CV_FOLDS = 5\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  SILVER_DB={SILVER_DB}\")\n",
    "print(f\"  GOLD_DB={GOLD_DB}\")\n",
    "print(f\"  CHURN_WINDOW_DAYS={CHURN_WINDOW_DAYS}\")\n",
    "print(f\"  FEATURE_WINDOW_DAYS={FEATURE_WINDOW_DAYS}\")\n",
    "print(f\"  LABEL_OFFSET_DAYS={LABEL_OFFSET_DAYS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def ensure_database(name):\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {name}\")\n",
    "\n",
    "def read_silver(table_name):\n",
    "    return spark.table(f\"{SILVER_DB}.{table_name}\")\n",
    "\n",
    "def save_gold(df, table_name):\n",
    "    full_name = f\"{GOLD_DB}.{table_name}\"\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_name)\n",
    "    print(f\"  {full_name}: {df.count()} rows\")\n",
    "\n",
    "def silver_exists(table_name):\n",
    "    try:\n",
    "        spark.table(f\"{SILVER_DB}.{table_name}\")\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "ensure_database(GOLD_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDATING DATA SOURCES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "required_tables = [\"fact_receipts\", \"dim_customers\"]\n",
    "for table in required_tables:\n",
    "    if not silver_exists(table):\n",
    "        raise ValueError(f\"Required table {SILVER_DB}.{table} not found!\")\n",
    "    print(f\"  {table}: OK\")\n",
    "\n",
    "print(\"\\nAll required tables exist.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DETERMINE ANALYSIS DATES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DETERMINING ANALYSIS DATES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get latest transaction date from data\n",
    "max_event_ts = read_silver(\"fact_receipts\").agg(F.max(\"event_ts\")).collect()[0][0]\n",
    "\n",
    "if max_event_ts is None:\n",
    "    raise ValueError(\"No transaction data found in fact_receipts!\")\n",
    "\n",
    "# Convert to date\n",
    "latest_date = max_event_ts.date()\n",
    "\n",
    "# Calculate key dates\n",
    "snapshot_date = latest_date - timedelta(days=LABEL_OFFSET_DAYS)\n",
    "feature_start_date = snapshot_date - timedelta(days=FEATURE_WINDOW_DAYS)\n",
    "churn_cutoff_date = snapshot_date - timedelta(days=CHURN_WINDOW_DAYS)\n",
    "\n",
    "print(f\"  Latest transaction date: {latest_date}\")\n",
    "print(f\"  Snapshot date (for predictions): {snapshot_date}\")\n",
    "print(f\"  Feature window: {feature_start_date} to {snapshot_date}\")\n",
    "print(f\"  Churn cutoff (last purchase before): {churn_cutoff_date}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load base data\n",
    "print(\"Loading transaction data...\")\n",
    "receipts_df = (\n",
    "    read_silver(\"fact_receipts\")\n",
    "    .filter(F.col(\"event_ts\") <= F.lit(snapshot_date))\n",
    "    .select(\"customer_id\", \"store_id\", \"event_ts\", \"total_amount\", \"payment_method\")\n",
    ")\n",
    "\n",
    "print(\"Loading customer dimension...\")\n",
    "customers_df = (\n",
    "    read_silver(\"dim_customers\")\n",
    "    .select(\"customer_id\", \"segment\", \"loyalty_status\", \"signup_date\", \"geography_id\")\n",
    ")\n",
    "\n",
    "# Behavioral Features: Purchase patterns in feature window\n",
    "print(\"\\nEngineering behavioral features...\")\n",
    "behavioral_features = (\n",
    "    receipts_df\n",
    "    .filter(\n",
    "        (F.col(\"event_ts\") >= F.lit(feature_start_date)) &\n",
    "        (F.col(\"event_ts\") <= F.lit(snapshot_date))\n",
    "    )\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        # Frequency metrics\n",
    "        F.count(\"*\").alias(\"purchase_count\"),\n",
    "        F.countDistinct(\"store_id\").alias(\"unique_stores\"),\n",
    "        \n",
    "        # Monetary metrics\n",
    "        F.sum(\"total_amount\").alias(\"total_spend\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_basket_value\"),\n",
    "        F.stddev(\"total_amount\").alias(\"basket_std\"),\n",
    "        F.max(\"total_amount\").alias(\"max_basket\"),\n",
    "        F.min(\"total_amount\").alias(\"min_basket\"),\n",
    "        \n",
    "        # Recency metrics\n",
    "        F.max(\"event_ts\").alias(\"last_purchase_date\"),\n",
    "        F.min(\"event_ts\").alias(\"first_purchase_in_window\"),\n",
    "        \n",
    "        # Payment diversity\n",
    "        F.countDistinct(\"payment_method\").alias(\"payment_methods_used\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"days_since_last_purchase\",\n",
    "        F.datediff(F.lit(snapshot_date), F.to_date(\"last_purchase_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"purchase_frequency\",\n",
    "        F.col(\"purchase_count\") / F.lit(FEATURE_WINDOW_DAYS)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"basket_consistency\",\n",
    "        F.when(F.col(\"basket_std\").isNull(), 0.0)\n",
    "         .otherwise(F.col(\"basket_std\") / F.col(\"avg_basket_value\"))\n",
    "    )\n",
    "    .drop(\"first_purchase_in_window\", \"last_purchase_date\")\n",
    ")\n",
    "\n",
    "print(f\"  Behavioral features created for {behavioral_features.count()} customers\")\n",
    "\n",
    "# Demographic Features\n",
    "print(\"\\nEngineering demographic features...\")\n",
    "demographic_features = (\n",
    "    customers_df\n",
    "    .withColumn(\n",
    "        \"customer_tenure_days\",\n",
    "        F.datediff(F.lit(snapshot_date), F.col(\"signup_date\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"is_premium_segment\",\n",
    "        F.when(F.col(\"segment\").isin([\"Premium\", \"VIP\"]), 1).otherwise(0)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"is_loyal\",\n",
    "        F.when(F.col(\"loyalty_status\") == \"Gold\", 2)\n",
    "         .when(F.col(\"loyalty_status\") == \"Silver\", 1)\n",
    "         .otherwise(0)\n",
    "    )\n",
    "    .select(\n",
    "        \"customer_id\",\n",
    "        \"customer_tenure_days\",\n",
    "        \"is_premium_segment\",\n",
    "        \"is_loyal\",\n",
    "        \"geography_id\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"  Demographic features created for {demographic_features.count()} customers\")\n",
    "\n",
    "# Combine all features\n",
    "print(\"\\nCombining features...\")\n",
    "features_df = (\n",
    "    demographic_features\n",
    "    .join(behavioral_features, on=\"customer_id\", how=\"left\")\n",
    "    # Fill nulls for customers with no purchases in window\n",
    "    .fillna({\n",
    "        \"purchase_count\": 0,\n",
    "        \"unique_stores\": 0,\n",
    "        \"total_spend\": 0.0,\n",
    "        \"avg_basket_value\": 0.0,\n",
    "        \"basket_std\": 0.0,\n",
    "        \"max_basket\": 0.0,\n",
    "        \"min_basket\": 0.0,\n",
    "        \"days_since_last_purchase\": FEATURE_WINDOW_DAYS + 1,\n",
    "        \"payment_methods_used\": 0,\n",
    "        \"purchase_frequency\": 0.0,\n",
    "        \"basket_consistency\": 0.0\n",
    "    })\n",
    ")\n",
    "\n",
    "print(f\"  Combined features for {features_df.count()} customers\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE TARGET VARIABLE (CHURN LABEL)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING CHURN LABELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define churn: customers who haven't purchased since churn_cutoff_date\n",
    "churned_customers = (\n",
    "    receipts_df\n",
    "    .filter(F.col(\"event_ts\") > F.lit(churn_cutoff_date))\n",
    "    .select(\"customer_id\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# Join to create binary label\n",
    "dataset_df = (\n",
    "    features_df\n",
    "    .join(\n",
    "        churned_customers.withColumn(\"is_active\", F.lit(1)),\n",
    "        on=\"customer_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .withColumn(\"is_churned\", F.when(F.col(\"is_active\").isNull(), 1).otherwise(0))\n",
    "    .drop(\"is_active\")\n",
    ")\n",
    "\n",
    "# Check class balance\n",
    "churn_stats = dataset_df.groupBy(\"is_churned\").count().collect()\n",
    "total_customers = sum([row[\"count\"] for row in churn_stats])\n",
    "churned_count = [row[\"count\"] for row in churn_stats if row[\"is_churned\"] == 1][0]\n",
    "churn_rate = churned_count / total_customers\n",
    "\n",
    "print(f\"  Total customers: {total_customers}\")\n",
    "print(f\"  Churned: {churned_count} ({churn_rate:.1%})\")\n",
    "print(f\"  Active: {total_customers - churned_count} ({1 - churn_rate:.1%})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREPARE TRAINING DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PREPARING TRAINING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert to Pandas for sklearn/lightgbm\n",
    "print(\"Converting to Pandas DataFrame...\")\n",
    "dataset_pd = dataset_df.toPandas()\n",
    "\n",
    "# Define feature columns (exclude ID and target)\n",
    "feature_cols = [\n",
    "    \"customer_tenure_days\",\n",
    "    \"is_premium_segment\",\n",
    "    \"is_loyal\",\n",
    "    \"geography_id\",\n",
    "    \"purchase_count\",\n",
    "    \"unique_stores\",\n",
    "    \"total_spend\",\n",
    "    \"avg_basket_value\",\n",
    "    \"basket_std\",\n",
    "    \"max_basket\",\n",
    "    \"min_basket\",\n",
    "    \"days_since_last_purchase\",\n",
    "    \"payment_methods_used\",\n",
    "    \"purchase_frequency\",\n",
    "    \"basket_consistency\"\n",
    "]\n",
    "\n",
    "X = dataset_pd[feature_cols]\n",
    "y = dataset_pd[\"is_churned\"]\n",
    "\n",
    "print(f\"  Training set shape: {X.shape}\")\n",
    "print(f\"  Features: {len(feature_cols)}\")\n",
    "print(f\"  Target distribution: {y.value_counts().to_dict()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN LIGHTGBM MODEL WITH CROSS-VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING LIGHTGBM MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# LightGBM classifier with tuned hyperparameters\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    metric='auc',\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Cross-validation with stratified folds\n",
    "print(f\"Running {CV_FOLDS}-fold cross-validation...\")\n",
    "cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_scores = cross_val_score(lgb_model, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "print(f\"\\nCross-Validation AUC-ROC Scores:\")\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"  Fold {i}: {score:.4f}\")\n",
    "print(f\"  Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Train final model on all data\n",
    "print(\"\\nTraining final model on full dataset...\")\n",
    "lgb_model.fit(X, y)\n",
    "print(\"  Model training complete.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_proba = lgb_model.predict_proba(X)[:, 1]\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "auc_roc = roc_auc_score(y, y_pred_proba)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "\n",
    "print(f\"Performance Metrics:\")\n",
    "print(f\"  AUC-ROC: {auc_roc:.4f}\")\n",
    "print(f\"  Precision (at 0.5 threshold): {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "print(f\"  F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Check acceptance criteria\n",
    "print(f\"\\nAcceptance Criteria:\")\n",
    "print(f\"  AUC-ROC > 0.75: {'PASS' if auc_roc > 0.75 else 'FAIL'} ({auc_roc:.4f})\")\n",
    "print(f\"  Precision > 0.6: {'PASS' if precision > 0.6 else 'FAIL'} ({precision:.4f})\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives: {cm[0, 0]}\")\n",
    "print(f\"  False Positives: {cm[0, 1]}\")\n",
    "print(f\"  False Negatives: {cm[1, 0]}\")\n",
    "print(f\"  True Positives: {cm[1, 1]}\")\n",
    "\n",
    "# Top 20% risk analysis\n",
    "top_20_pct_threshold = np.percentile(y_pred_proba, 80)\n",
    "top_20_pct_mask = y_pred_proba >= top_20_pct_threshold\n",
    "actual_churners_captured = y[top_20_pct_mask].sum()\n",
    "total_churners = y.sum()\n",
    "capture_rate = actual_churners_captured / total_churners\n",
    "\n",
    "print(f\"\\nTop 20% High-Risk Customers:\")\n",
    "print(f\"  Threshold: {top_20_pct_threshold:.4f}\")\n",
    "print(f\"  Captured {actual_churners_captured} of {total_churners} churners ({capture_rate:.1%})\")\n",
    "print(f\"  Target: 60%+ - {'PASS' if capture_rate > 0.6 else 'FAIL'}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': lgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "for idx, row in feature_importance.head(10).iterrows():\n",
    "    print(f\"  {row['feature']:<30} {row['importance']:>10.1f}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['feature'].head(10), feature_importance['importance'].head(10))\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 10 Features for Churn Prediction')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHURN RISK DISTRIBUTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHURN RISK DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze risk score distribution\n",
    "risk_bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "risk_labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "dataset_pd['risk_category'] = pd.cut(y_pred_proba, bins=risk_bins, labels=risk_labels, include_lowest=True)\n",
    "\n",
    "risk_distribution = dataset_pd.groupby('risk_category', observed=True).size()\n",
    "print(\"\\nRisk Category Distribution:\")\n",
    "for category, count in risk_distribution.items():\n",
    "    pct = count / len(dataset_pd) * 100\n",
    "    print(f\"  {category:<12} {count:>6} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Visualize distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y_pred_proba, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Threshold (0.5)')\n",
    "plt.axvline(x=top_20_pct_threshold, color='orange', linestyle='--', label=f'Top 20% ({top_20_pct_threshold:.2f})')\n",
    "plt.xlabel('Churn Probability')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.title('Distribution of Churn Risk Scores')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE PREDICTIONS TO GOLD LAYER\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING PREDICTIONS TO GOLD LAYER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create predictions dataframe\n",
    "predictions_pd = pd.DataFrame({\n",
    "    'customer_id': dataset_pd['customer_id'],\n",
    "    'churn_probability': y_pred_proba,\n",
    "    'churn_prediction': y_pred,\n",
    "    'risk_category': dataset_pd['risk_category'],\n",
    "    'is_churned_actual': y,\n",
    "    'prediction_date': pd.Timestamp(snapshot_date),\n",
    "    'model_version': '1.0',\n",
    "    'churn_window_days': CHURN_WINDOW_DAYS\n",
    "})\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "predictions_spark = spark.createDataFrame(predictions_pd)\n",
    "\n",
    "# Save to Gold layer\n",
    "save_gold(predictions_spark, \"gold_churn_predictions\")\n",
    "\n",
    "print(\"\\nPredictions saved successfully.\")\n",
    "print(f\"  Table: {GOLD_DB}.gold_churn_predictions\")\n",
    "print(f\"  Rows: {len(predictions_pd)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY STATISTICS FOR BUSINESS USERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BUSINESS INSIGHTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# High-risk customers requiring immediate attention\n",
    "high_risk_customers = predictions_pd[predictions_pd['churn_probability'] >= 0.7]\n",
    "very_high_risk_customers = predictions_pd[predictions_pd['churn_probability'] >= 0.8]\n",
    "\n",
    "print(f\"\\nActionable Insights:\")\n",
    "print(f\"  Total customers analyzed: {len(predictions_pd):,}\")\n",
    "print(f\"  High risk (>70%): {len(high_risk_customers):,}\")\n",
    "print(f\"  Very high risk (>80%): {len(very_high_risk_customers):,}\")\n",
    "print(f\"  Recommended for retention campaigns: {len(high_risk_customers):,}\")\n",
    "\n",
    "# Calculate potential revenue at risk\n",
    "avg_customer_value = dataset_pd['total_spend'].mean()\n",
    "revenue_at_risk = len(high_risk_customers) * avg_customer_value\n",
    "\n",
    "print(f\"\\nRevenue Impact:\")\n",
    "print(f\"  Avg customer value (last {FEATURE_WINDOW_DAYS} days): ${avg_customer_value:,.2f}\")\n",
    "print(f\"  Estimated revenue at risk: ${revenue_at_risk:,.2f}\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  1. Target high-risk customers with retention offers\")\n",
    "print(f\"  2. Analyze top churn drivers from feature importance\")\n",
    "print(f\"  3. Monitor effectiveness weekly\")\n",
    "print(f\"  4. Refresh predictions regularly (recommended: weekly)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHURN PREDICTION MODEL COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
