{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bronze Layer Shortcuts (cusn schema)\n",
    "\n",
    "This notebook creates the Bronze layer (cusn schema) with shortcuts to both:\n",
    "- **Batch Historical Data**: 24 parquet tables in ADLSv2 (6 dimensions + 18 facts)\n",
    "- **Streaming Real-Time Data**: 18 event tables in Eventhouse\n",
    "\n",
    "The Bronze layer serves as the unified source for the Silver layer transformation.\n",
    "\n",
    "## Prerequisites\n",
    "- ADLSv2 storage account with parquet files (exported from datagen)\n",
    "- Eventhouse with streaming event tables\n",
    "- Lakehouse workspace permissions\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "ADLSv2 Parquet (24 tables) \u2500\u2500shortcut\u2500\u2500> cusn.dim_*, cusn.fact_*\n",
    "Eventhouse Events (18 tables) \u2500\u2500shortcut\u2500\u2500> cusn.receipt_created, etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "# Use environment variables or override with defaults for local testing\n",
    "\n",
    "import os\n",
    "\n",
    "# ADLSv2 Configuration\n",
    "ADLS_ACCOUNT = os.environ.get(\"ADLS_ACCOUNT\", \"stdretail\")\n",
    "ADLS_CONTAINER = os.environ.get(\"ADLS_CONTAINER\", \"supermarket\")\n",
    "ADLS_BASE_PATH = os.environ.get(\"ADLS_BASE_PATH\", \"\")  # Base path within container (empty for root)\n",
    "\n",
    "# Eventhouse Configuration  \n",
    "EVENTHOUSE_URI = os.environ.get(\"EVENTHOUSE_URI\", \"<replace-with-eventhouse-uri>\")\n",
    "EVENTHOUSE_DATABASE = os.environ.get(\"EVENTHOUSE_DATABASE\", \"kql_retail_db\")\n",
    "\n",
    "# Schema Names\n",
    "BRONZE_SCHEMA = os.environ.get(\"BRONZE_SCHEMA\", \"cusn\")\n",
    "\n",
    "# Deployment Mode: Set REQUIRE_EVENTHOUSE=true to fail if Eventhouse not configured\n",
    "# This prevents incomplete Bronze layer in production\n",
    "REQUIRE_EVENTHOUSE = os.environ.get(\"REQUIRE_EVENTHOUSE\", \"false\").lower() == \"true\"\n",
    "\n",
    "# Validate required configuration\n",
    "if EVENTHOUSE_URI == \"<replace-with-eventhouse-uri>\":\n",
    "    if REQUIRE_EVENTHOUSE:\n",
    "        raise EnvironmentError(\n",
    "            \"CRITICAL: EVENTHOUSE_URI not configured but REQUIRE_EVENTHOUSE=true.\\n\"\n",
    "            \"Bronze layer will be incomplete without Eventhouse shortcuts.\\n\"\n",
    "            \"Either:\\n\"\n",
    "            \"  1. Set EVENTHOUSE_URI environment variable, OR\\n\"\n",
    "            \"  2. Set REQUIRE_EVENTHOUSE=false to skip Eventhouse shortcuts (dev/test only)\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  WARNING: EVENTHOUSE_URI not configured. Eventhouse shortcuts will NOT be created.\")\n",
    "        print(\"   Bronze layer will only contain batch parquet shortcuts (24 tables).\")\n",
    "        print(\"   Set EVENTHOUSE_URI environment variable to enable full Bronze layer (42 tables).\")\n",
    "        print(\"   For production deployment, set REQUIRE_EVENTHOUSE=true to fail on missing config.\")\n",
    "        CREATE_EVENTHOUSE_SHORTCUTS = False\n",
    "else:\n",
    "    CREATE_EVENTHOUSE_SHORTCUTS = True\n",
    "\n",
    "print(f\"\\nConfiguration loaded:\")\n",
    "print(f\"  ADLSv2: abfss://{ADLS_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/{ADLS_BASE_PATH}\")\n",
    "print(f\"  Eventhouse: {EVENTHOUSE_URI}/{EVENTHOUSE_DATABASE}\")\n",
    "print(f\"  Bronze Schema: {BRONZE_SCHEMA}\")\n",
    "print(f\"  Require Eventhouse: {REQUIRE_EVENTHOUSE}\")\n",
    "print(f\"  Create Eventhouse Shortcuts: {CREATE_EVENTHOUSE_SHORTCUTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Bronze Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cusn schema\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {BRONZE_SCHEMA}\")\n",
    "print(f\"\u2713 Schema '{BRONZE_SCHEMA}' created or already exists\")\n",
    "\n",
    "# Verify schema creation\n",
    "schemas = spark.sql(\"SHOW SCHEMAS\").collect()\n",
    "if any(row.namespace == BRONZE_SCHEMA for row in schemas):\n",
    "    print(f\"\u2713 Verified: Schema '{BRONZE_SCHEMA}' exists\")\n",
    "else:\n",
    "    raise Exception(f\"Failed to create schema '{BRONZE_SCHEMA}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Shortcuts to ADLSv2 Parquet Tables (Batch Historical Data)\n",
    "\n",
    "Creates 24 shortcuts to parquet files in ADLSv2:\n",
    "- 6 Dimension Tables\n",
    "- 18 Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimension tables\n",
    "dimension_tables = [\n",
    "    \"dim_geographies\",\n",
    "    \"dim_stores\",\n",
    "    \"dim_distribution_centers\",\n",
    "    \"dim_trucks\",\n",
    "    \"dim_customers\",\n",
    "    \"dim_products\"\n",
    "]\n",
    "\n",
    "# Define fact tables (all 18)\n",
    "fact_tables = [\n",
    "    \"fact_receipts\",\n",
    "    \"fact_receipt_lines\",\n",
    "    \"fact_store_inventory_txn\",\n",
    "    \"fact_dc_inventory_txn\",\n",
    "    \"fact_truck_moves\",\n",
    "    \"fact_truck_inventory\",\n",
    "    \"fact_foot_traffic\",\n",
    "    \"fact_ble_pings\",\n",
    "    \"fact_customer_zone_changes\",\n",
    "    \"fact_marketing\",\n",
    "    \"fact_online_order_headers\",\n",
    "    \"fact_online_order_lines\",\n",
    "    \"fact_payments\",\n",
    "    \"fact_store_ops\",\n",
    "    \"fact_stockouts\",\n",
    "    \"fact_promotions\",\n",
    "    \"fact_promo_lines\",\n",
    "    \"fact_reorders\"\n",
    "]\n",
    "\n",
    "all_parquet_tables = dimension_tables + fact_tables\n",
    "print(f\"Total parquet tables to create shortcuts for: {len(all_parquet_tables)}\")\n",
    "print(f\"  Dimensions: {len(dimension_tables)}\")\n",
    "print(f\"  Facts: {len(fact_tables)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "\n",
    "# Helper function to create ADLSv2 shortcut\n",
    "def create_adls_shortcut(table_name: str, schema: str = BRONZE_SCHEMA) -> bool:\n",
    "    \"\"\"\n",
    "    Create a shortcut to an ADLSv2 parquet table.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table (e.g., 'dim_stores')\n",
    "        schema: Target schema name (default: cusn)\n",
    "        \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Construct ADLS path\n",
    "        adls_path = f\"abfss://{ADLS_CONTAINER}@{ADLS_ACCOUNT}.dfs.core.windows.net/{ADLS_BASE_PATH}{table_name}/\"\n",
    "        \n",
    "        # Create shortcut using mssparkutils\n",
    "        # Note: This API may vary based on Fabric version\n",
    "        # Alternative: Use Fabric REST API or Portal for manual creation\n",
    "        \n",
    "        # For now, we'll create external tables pointing to ADLS\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {schema}.{table_name}\n",
    "            USING PARQUET\n",
    "            LOCATION '{adls_path}'\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"  \u2713 {schema}.{table_name} -> {adls_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  \u2717 Failed to create {schema}.{table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create shortcuts for all parquet tables\n",
    "print(f\"\\nCreating shortcuts to ADLSv2 parquet tables...\\n\")\n",
    "success_count = 0\n",
    "failed_tables = []\n",
    "\n",
    "for table in all_parquet_tables:\n",
    "    if create_adls_shortcut(table):\n",
    "        success_count += 1\n",
    "    else:\n",
    "        failed_tables.append(table)\n",
    "\n",
    "print(f\"\\nADLSv2 Shortcut Creation Summary:\")\n",
    "print(f\"  Success: {success_count}/{len(all_parquet_tables)}\")\n",
    "if failed_tables:\n",
    "    print(f\"  Failed tables: {', '.join(failed_tables)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Shortcuts to Eventhouse Event Tables (Streaming Real-Time Data)\n",
    "\n",
    "Creates 18 shortcuts to streaming event tables in Eventhouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define streaming event tables (matches EventType enum)\n",
    "event_tables = [\n",
    "    # Transaction Events\n",
    "    \"receipt_created\",\n",
    "    \"receipt_line_added\",\n",
    "    \"payment_processed\",\n",
    "    \n",
    "    # Inventory Events\n",
    "    \"inventory_updated\",\n",
    "    \"stockout_detected\",\n",
    "    \"reorder_triggered\",\n",
    "    \n",
    "    # Customer Events\n",
    "    \"customer_entered\",\n",
    "    \"customer_zone_changed\",\n",
    "    \"ble_ping_detected\",\n",
    "    \n",
    "    # Operational Events\n",
    "    \"truck_arrived\",\n",
    "    \"truck_departed\",\n",
    "    \"store_opened\",\n",
    "    \"store_closed\",\n",
    "    \n",
    "    # Marketing Events\n",
    "    \"ad_impression\",\n",
    "    \"promotion_applied\",\n",
    "    \n",
    "    # Omnichannel Events\n",
    "    \"online_order_created\",\n",
    "    \"online_order_picked\",\n",
    "    \"online_order_shipped\"\n",
    "]\n",
    "\n",
    "print(f\"Total event tables to create shortcuts for: {len(event_tables)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eventhouse_shortcut(table_name: str, schema: str = BRONZE_SCHEMA) -> bool:\n",
    "    \"\"\"\n",
    "    Create a shortcut to an Eventhouse KQL table.\n",
    "    \n",
    "    NOTE: Eventhouse shortcuts cannot be created programmatically via Spark SQL.\n",
    "    This function provides instructions for manual creation via Fabric Portal UI.\n",
    "    \n",
    "    For programmatic creation, use:\n",
    "    - Fabric REST API: POST /v1/workspaces/{workspaceId}/lakehouses/{lakehouseId}/shortcuts\n",
    "    - PowerShell: New-FabricLakehouseShortcut cmdlet\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the event table (e.g., 'receipt_created')\n",
    "        schema: Target schema name (default: cusn)\n",
    "        \n",
    "    Returns:\n",
    "        False (manual creation required)\n",
    "    \"\"\"\n",
    "    print(f\"  \u26a0 {schema}.{table_name}: Manual creation required\")\n",
    "    print(f\"    1. Navigate to Lakehouse in Fabric workspace\")\n",
    "    print(f\"    2. Right-click Tables \u2192 New shortcut \u2192 Eventhouse\")\n",
    "    print(f\"    3. Connection: {EVENTHOUSE_URI}\")\n",
    "    print(f\"    4. Database: {EVENTHOUSE_DATABASE}\")\n",
    "    print(f\"    5. Table: {table_name}\")\n",
    "    print(f\"    6. Target location: Tables/{schema}/{table_name}\")\n",
    "    print()\n",
    "    return False\n",
    "\n",
    "# Create shortcuts for all event tables\n",
    "if CREATE_EVENTHOUSE_SHORTCUTS:\n",
    "    print(f\"\\nEventhouse shortcuts must be created manually via Fabric Portal UI.\\n\")\n",
    "    print(f\"INSTRUCTIONS FOR MANUAL CREATION:\")\n",
    "    print(f\"=\" * 70)\n",
    "    print(f\"1. Open Fabric workspace and navigate to your Lakehouse\")\n",
    "    print(f\"2. In Lakehouse Explorer, right-click on 'Tables' \u2192 'New shortcut'\")\n",
    "    print(f\"3. Select source: 'Eventhouse'\")\n",
    "    print(f\"4. For each of the {len(event_tables)} event tables below:\")\n",
    "    print(f\"   - Connection: {EVENTHOUSE_URI}\")\n",
    "    print(f\"   - Database: {EVENTHOUSE_DATABASE}\")\n",
    "    print(f\"   - Select table name from list\")\n",
    "    print(f\"   - Target: Tables/{BRONZE_SCHEMA}/[table_name]\")\n",
    "    print(f\"=\" * 70)\n",
    "    print(f\"\\nEvent tables to create shortcuts for:\")\n",
    "    \n",
    "    for i, table in enumerate(event_tables, 1):\n",
    "        print(f\"  {i:2d}. {table}\")\n",
    "    \n",
    "    print(f\"\\nAfter creating all shortcuts, run the verification cell below to confirm.\")\n",
    "    \n",
    "    # Mark all as needing manual creation\n",
    "    event_success_count = 0\n",
    "    event_failed_tables = event_tables.copy()\n",
    "else:\n",
    "    print(f\"\\nSkipping Eventhouse shortcut creation (EVENTHOUSE_URI not configured)\")\n",
    "    print(f\"Set EVENTHOUSE_URI environment variable to enable Eventhouse shortcuts.\")\n",
    "    event_success_count = 0\n",
    "    event_failed_tables = []\n",
    "\n",
    "print(f\"\\nEventhouse Shortcut Summary:\")\n",
    "print(f\"  Programmatic creation: Not supported\")\n",
    "print(f\"  Manual creation required: {len(event_tables) if CREATE_EVENTHOUSE_SHORTCUTS else 0} tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify Bronze Layer Shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in Bronze schema\n",
    "bronze_tables = spark.sql(f\"SHOW TABLES IN {BRONZE_SCHEMA}\").collect()\n",
    "\n",
    "print(f\"\\nBronze Schema ({BRONZE_SCHEMA}) Tables:\")\n",
    "print(f\"  Total: {len(bronze_tables)} tables\\n\")\n",
    "\n",
    "# Categorize tables\n",
    "dim_count = sum(1 for t in bronze_tables if t.tableName.startswith('dim_'))\n",
    "fact_count = sum(1 for t in bronze_tables if t.tableName.startswith('fact_'))\n",
    "event_count = len(bronze_tables) - dim_count - fact_count\n",
    "\n",
    "print(f\"  Dimensions: {dim_count} (expected: 6)\")\n",
    "print(f\"  Facts: {fact_count} (expected: 18)\")\n",
    "print(f\"  Events: {event_count} (expected: 18)\")\n",
    "print(f\"\\n  Target: 42 tables (6 dims + 18 facts + 18 events)\")\n",
    "\n",
    "if len(bronze_tables) == 42:\n",
    "    print(f\"\\n\u2713 Bronze layer complete with all 42 shortcuts!\")\n",
    "else:\n",
    "    print(f\"\\n\u26a0 Bronze layer has {len(bronze_tables)}/42 tables\")\n",
    "    print(f\"  Check for missing tables and create shortcuts manually if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Bronze Layer Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reading from Bronze shortcuts\n",
    "print(\"\\nTesting Bronze layer shortcuts...\\n\")\n",
    "\n",
    "# Test dimension shortcut\n",
    "try:\n",
    "    df_dim = spark.table(f\"{BRONZE_SCHEMA}.dim_stores\")\n",
    "    print(f\"\u2713 dim_stores: {df_dim.count()} rows\")\n",
    "    df_dim.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 dim_stores failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Test fact shortcut\n",
    "try:\n",
    "    df_fact = spark.table(f\"{BRONZE_SCHEMA}.fact_receipts\")\n",
    "    print(f\"\u2713 fact_receipts: {df_fact.count()} rows\")\n",
    "    df_fact.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 fact_receipts failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Test event shortcut (if available)\n",
    "try:\n",
    "    df_event = spark.table(f\"{BRONZE_SCHEMA}.receipt_created\")\n",
    "    print(f\"\u2713 receipt_created: {df_event.count()} rows\")\n",
    "    df_event.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0 receipt_created: {e}\")\n",
    "    print(f\"  (Event shortcuts may need manual creation)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nBronze layer testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has created the Bronze layer (cusn schema) with shortcuts to:\n",
    "\n",
    "### ADLSv2 Parquet Tables (24 tables)\n",
    "- 6 Dimension tables: dim_*\n",
    "- 18 Fact tables: fact_*\n",
    "\n",
    "### Eventhouse Event Tables (18 tables)\n",
    "- Transaction events: receipt_created, receipt_line_added, payment_processed\n",
    "- Inventory events: inventory_updated, stockout_detected, reorder_triggered\n",
    "- Customer events: customer_entered, customer_zone_changed, ble_ping_detected\n",
    "- Operational events: truck_arrived, truck_departed, store_opened, store_closed\n",
    "- Marketing events: ad_impression, promotion_applied\n",
    "- Omnichannel events: online_order_created, online_order_picked, online_order_shipped\n",
    "\n",
    "### Next Steps\n",
    "1. If any shortcuts failed, create them manually via Fabric UI\n",
    "2. Verify all 42 shortcuts are accessible\n",
    "3. Run the Silver transformation notebook (02-onelake-to-silver.ipynb)\n",
    "4. Silver layer will combine batch + streaming data into unified ag.* tables\n",
    "\n",
    "### Manual Shortcut Creation (if needed)\n",
    "For Eventhouse shortcuts that couldn't be created programmatically:\n",
    "1. Navigate to Lakehouse in Fabric workspace\n",
    "2. Right-click on Tables\n",
    "3. Select \"New shortcut\" \u2192 \"Eventhouse\"\n",
    "4. Enter Eventhouse URI and table name\n",
    "5. Target schema: cusn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}