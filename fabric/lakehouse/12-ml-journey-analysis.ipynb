{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Enhancement: In-Store Journey Analysis\n",
        "\n",
        "Analyzes customer shopping paths using BLE beacon zone transitions to understand in-store behavior patterns.\n",
        "\n",
        "## Data Flow\n",
        "```\n",
        "Silver (fact_zone_changes, fact_receipts) --> Gold (gold_journey_patterns)\n",
        "```\n",
        "\n",
        "## Business Value\n",
        "- Optimize store layout based on traffic patterns\n",
        "- Understand customer engagement by zone\n",
        "- Identify high-value path sequences\n",
        "- Correlate paths with purchase outcomes\n",
        "\n",
        "## Outputs\n",
        "- **gold_journey_patterns**: Top 20 common paths with conversion/basket metrics\n",
        "- **gold_zone_transitions**: Zone transition probability matrix\n",
        "- **gold_zone_dwell_stats**: Average dwell time statistics by zone\n",
        "\n",
        "## Usage\n",
        "Run this notebook **on-demand** or schedule periodically (e.g., daily) to update journey insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StringType, ArrayType\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "def get_env(var_name, default=None):\n",
        "    return os.environ.get(var_name, default)\n",
        "\n",
        "SILVER_DB = get_env(\"SILVER_DB\", default=\"ag\")\n",
        "GOLD_DB = get_env(\"GOLD_DB\", default=\"au\")\n",
        "\n",
        "# Analysis window: last 30 days by default\n",
        "ANALYSIS_DAYS = int(get_env(\"ANALYSIS_DAYS\", default=\"30\"))\n",
        "MIN_PATH_LENGTH = int(get_env(\"MIN_PATH_LENGTH\", default=\"2\"))\n",
        "TOP_N_PATHS = int(get_env(\"TOP_N_PATHS\", default=\"20\"))\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  SILVER_DB={SILVER_DB}\")\n",
        "print(f\"  GOLD_DB={GOLD_DB}\")\n",
        "print(f\"  ANALYSIS_DAYS={ANALYSIS_DAYS}\")\n",
        "print(f\"  MIN_PATH_LENGTH={MIN_PATH_LENGTH}\")\n",
        "print(f\"  TOP_N_PATHS={TOP_N_PATHS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HELPER FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def ensure_database(name):\n",
        "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {name}\")\n",
        "\n",
        "def read_silver(table_name):\n",
        "    return spark.table(f\"{SILVER_DB}.{table_name}\")\n",
        "\n",
        "def save_gold(df, table_name):\n",
        "    full_name = f\"{GOLD_DB}.{table_name}\"\n",
        "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_name)\n",
        "    print(f\"  {full_name}: {df.count()} rows\")\n",
        "\n",
        "def silver_exists(table_name):\n",
        "    try:\n",
        "        spark.table(f\"{SILVER_DB}.{table_name}\")\n",
        "        return True\n",
        "    except AnalysisException:\n",
        "        return False\n",
        "\n",
        "ensure_database(GOLD_DB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"IN-STORE JOURNEY ANALYSIS\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Reconstruct Customer Paths\n",
        "\n",
        "Build sequential paths from `customer_zone_changed` events, grouping by customer session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not silver_exists(\"fact_zone_changes\"):\n",
        "    raise ValueError(\"fact_zone_changes table not found in Silver layer\")\n",
        "\n",
        "# Get zone change events for the analysis window\n",
        "analysis_start = datetime.now(timezone.utc) - timedelta(days=ANALYSIS_DAYS)\n",
        "\n",
        "df_zone_changes = (\n",
        "    read_silver(\"fact_zone_changes\")\n",
        "    .filter(F.col(\"event_ts\") >= F.lit(analysis_start))\n",
        "    .select(\n",
        "        \"store_id\",\n",
        "        \"customer_ble_id\",\n",
        "        \"from_zone\",\n",
        "        \"to_zone\",\n",
        "        \"event_ts\",\n",
        "        \"trace_id\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"Loaded {df_zone_changes.count()} zone change events from last {ANALYSIS_DAYS} days\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define sessions: group events by customer and store, with 30-minute session timeout\n",
        "# Session ends if there's a gap > 30 minutes between events\n",
        "\n",
        "window_by_customer = Window.partitionBy(\"store_id\", \"customer_ble_id\").orderBy(\"event_ts\")\n",
        "\n",
        "df_with_sessions = (\n",
        "    df_zone_changes\n",
        "    .withColumn(\n",
        "        \"prev_event_ts\",\n",
        "        F.lag(\"event_ts\").over(window_by_customer)\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"time_gap_minutes\",\n",
        "        (F.unix_timestamp(\"event_ts\") - F.unix_timestamp(\"prev_event_ts\")) / 60\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"is_new_session\",\n",
        "        F.when(\n",
        "            (F.col(\"prev_event_ts\").isNull()) | (F.col(\"time_gap_minutes\") > 30),\n",
        "            1\n",
        "        ).otherwise(0)\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"session_id\",\n",
        "        F.sum(\"is_new_session\").over(\n",
        "            window_by_customer.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "        )\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"session_key\",\n",
        "        F.concat(\n",
        "            F.col(\"store_id\").cast(\"string\"),\n",
        "            F.lit(\"_\"),\n",
        "            F.col(\"customer_ble_id\"),\n",
        "            F.lit(\"_\"),\n",
        "            F.col(\"session_id\").cast(\"string\")\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"Identified {df_with_sessions.select('session_key').distinct().count()} unique customer sessions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build paths: collect zones in order for each session\n",
        "df_paths = (\n",
        "    df_with_sessions\n",
        "    .groupBy(\"session_key\", \"store_id\", \"customer_ble_id\")\n",
        "    .agg(\n",
        "        F.collect_list(\n",
        "            F.struct(\n",
        "                F.col(\"event_ts\"),\n",
        "                F.col(\"to_zone\")\n",
        "            )\n",
        "        ).alias(\"transitions\"),\n",
        "        F.min(\"event_ts\").alias(\"session_start\"),\n",
        "        F.max(\"event_ts\").alias(\"session_end\")\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"session_duration_minutes\",\n",
        "        (F.unix_timestamp(\"session_end\") - F.unix_timestamp(\"session_start\")) / 60\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"path_length\",\n",
        "        F.size(\"transitions\")\n",
        "    )\n",
        "    .filter(F.col(\"path_length\") >= MIN_PATH_LENGTH)\n",
        ")\n",
        "\n",
        "# Extract zone sequence as array\n",
        "df_paths = df_paths.withColumn(\n",
        "    \"zone_path\",\n",
        "    F.expr(\"transform(transitions, x -> x.to_zone)\")\n",
        ").withColumn(\n",
        "    \"path_string\",\n",
        "    F.array_join(\"zone_path\", \" -> \")\n",
        ")\n",
        "\n",
        "print(f\"Built {df_paths.count()} customer paths (min length: {MIN_PATH_LENGTH} zones)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Calculate Zone Dwell Times\n",
        "\n",
        "Compute average time spent in each zone during customer sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate dwell time per zone by looking at time between zone transitions\n",
        "df_dwell_calc = (\n",
        "    df_with_sessions\n",
        "    .withColumn(\n",
        "        \"next_event_ts\",\n",
        "        F.lead(\"event_ts\").over(\n",
        "            Window.partitionBy(\"session_key\").orderBy(\"event_ts\")\n",
        "        )\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"dwell_seconds\",\n",
        "        F.when(\n",
        "            F.col(\"next_event_ts\").isNotNull(),\n",
        "            F.unix_timestamp(\"next_event_ts\") - F.unix_timestamp(\"event_ts\")\n",
        "        ).otherwise(None)\n",
        "    )\n",
        "    .filter(F.col(\"dwell_seconds\").isNotNull())\n",
        "    .filter(F.col(\"dwell_seconds\") > 0)\n",
        "    .filter(F.col(\"dwell_seconds\") < 3600)  # Filter outliers > 1 hour\n",
        ")\n",
        "\n",
        "df_zone_dwell_stats = (\n",
        "    df_dwell_calc\n",
        "    .groupBy(\"store_id\", \"to_zone\")\n",
        "    .agg(\n",
        "        F.avg(\"dwell_seconds\").alias(\"avg_dwell_seconds\"),\n",
        "        F.expr(\"percentile_approx(dwell_seconds, 0.5)\").alias(\"median_dwell_seconds\"),\n",
        "        F.min(\"dwell_seconds\").alias(\"min_dwell_seconds\"),\n",
        "        F.max(\"dwell_seconds\").alias(\"max_dwell_seconds\"),\n",
        "        F.count(\"*\").alias(\"visit_count\")\n",
        "    )\n",
        "    .withColumnRenamed(\"to_zone\", \"zone\")\n",
        "    .withColumn(\"computed_at\", F.lit(datetime.now(timezone.utc)))\n",
        ")\n",
        "\n",
        "print(\"Creating gold_zone_dwell_stats...\")\n",
        "save_gold(df_zone_dwell_stats, \"gold_zone_dwell_stats\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Calculate Zone Transition Probabilities\n",
        "\n",
        "Build a transition matrix showing the probability of moving from one zone to another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count transitions between zones\n",
        "df_transitions = (\n",
        "    df_zone_changes\n",
        "    .groupBy(\"store_id\", \"from_zone\", \"to_zone\")\n",
        "    .agg(F.count(\"*\").alias(\"transition_count\"))\n",
        ")\n",
        "\n",
        "# Calculate total transitions from each zone\n",
        "df_from_totals = (\n",
        "    df_transitions\n",
        "    .groupBy(\"store_id\", \"from_zone\")\n",
        "    .agg(F.sum(\"transition_count\").alias(\"total_from_zone\"))\n",
        ")\n",
        "\n",
        "# Join and calculate probabilities\n",
        "df_zone_transitions = (\n",
        "    df_transitions\n",
        "    .join(\n",
        "        df_from_totals,\n",
        "        on=[\"store_id\", \"from_zone\"],\n",
        "        how=\"inner\"\n",
        "    )\n",
        "    .withColumn(\n",
        "        \"transition_probability\",\n",
        "        F.col(\"transition_count\") / F.col(\"total_from_zone\")\n",
        "    )\n",
        "    .withColumn(\"computed_at\", F.lit(datetime.now(timezone.utc)))\n",
        "    .select(\n",
        "        \"store_id\",\n",
        "        \"from_zone\",\n",
        "        \"to_zone\",\n",
        "        \"transition_count\",\n",
        "        \"transition_probability\",\n",
        "        \"computed_at\"\n",
        "    )\n",
        "    .orderBy(\"store_id\", \"from_zone\", F.desc(\"transition_probability\"))\n",
        ")\n",
        "\n",
        "print(\"Creating gold_zone_transitions...\")\n",
        "save_gold(df_zone_transitions, \"gold_zone_transitions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Identify Common Path Patterns\n",
        "\n",
        "Find the top N most frequent customer paths through the store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count path frequency\n",
        "df_path_frequency = (\n",
        "    df_paths\n",
        "    .groupBy(\"store_id\", \"path_string\", \"path_length\")\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"occurrence_count\"),\n",
        "        F.avg(\"session_duration_minutes\").alias(\"avg_session_duration_minutes\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Rank paths by frequency per store\n",
        "window_by_store = Window.partitionBy(\"store_id\").orderBy(F.desc(\"occurrence_count\"))\n",
        "\n",
        "df_top_paths = (\n",
        "    df_path_frequency\n",
        "    .withColumn(\"rank\", F.row_number().over(window_by_store))\n",
        "    .filter(F.col(\"rank\") <= TOP_N_PATHS)\n",
        ")\n",
        "\n",
        "print(f\"Identified top {TOP_N_PATHS} paths per store\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Correlate Paths with Purchase Outcomes\n",
        "\n",
        "Join customer paths with receipt data to understand conversion and basket size by journey."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if receipts are available\n",
        "if silver_exists(\"fact_receipts\"):\n",
        "    # Get receipts within analysis window\n",
        "    df_receipts = (\n",
        "        read_silver(\"fact_receipts\")\n",
        "        .filter(F.col(\"event_ts\") >= F.lit(analysis_start))\n",
        "        .select(\n",
        "            \"store_id\",\n",
        "            \"customer_id\",\n",
        "            \"event_ts\",\n",
        "            \"total_cents\",\n",
        "            \"receipt_id_ext\"\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Note: Receipts use customer_id, zone changes use customer_ble_id\n",
        "    # We'll correlate by store + time window (same store, purchase within session timeframe)\n",
        "    \n",
        "    # Create session-to-receipt mapping by joining on store and time proximity\n",
        "    df_path_receipts = (\n",
        "        df_paths\n",
        "        .join(\n",
        "            df_receipts,\n",
        "            on=[\n",
        "                (df_paths.store_id == df_receipts.store_id) &\n",
        "                (df_receipts.event_ts >= df_paths.session_start) &\n",
        "                (df_receipts.event_ts <= df_paths.session_end + F.expr(\"INTERVAL 5 MINUTES\"))\n",
        "            ],\n",
        "            how=\"left\"\n",
        "        )\n",
        "        .select(\n",
        "            df_paths.store_id,\n",
        "            df_paths.session_key,\n",
        "            df_paths.path_string,\n",
        "            df_paths.path_length,\n",
        "            df_paths.session_duration_minutes,\n",
        "            df_receipts.receipt_id_ext,\n",
        "            df_receipts.total_cents\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Aggregate path performance metrics\n",
        "    df_path_metrics = (\n",
        "        df_path_receipts\n",
        "        .groupBy(\"store_id\", \"path_string\", \"path_length\")\n",
        "        .agg(\n",
        "            F.count(\"session_key\").alias(\"total_sessions\"),\n",
        "            F.sum(\n",
        "                F.when(F.col(\"receipt_id_ext\").isNotNull(), 1).otherwise(0)\n",
        "            ).alias(\"sessions_with_purchase\"),\n",
        "            F.avg(\"session_duration_minutes\").alias(\"avg_session_duration_minutes\"),\n",
        "            F.avg(\n",
        "                F.when(F.col(\"total_cents\").isNotNull(), F.col(\"total_cents\")).otherwise(None)\n",
        "            ).alias(\"avg_basket_cents\"),\n",
        "            F.sum(\n",
        "                F.when(F.col(\"total_cents\").isNotNull(), F.col(\"total_cents\")).otherwise(0)\n",
        "            ).alias(\"total_revenue_cents\")\n",
        "        )\n",
        "        .withColumn(\n",
        "            \"conversion_rate\",\n",
        "            F.col(\"sessions_with_purchase\") / F.col(\"total_sessions\")\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Join with top paths\n",
        "    df_journey_patterns = (\n",
        "        df_top_paths\n",
        "        .join(\n",
        "            df_path_metrics,\n",
        "            on=[\"store_id\", \"path_string\", \"path_length\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "        .withColumn(\"computed_at\", F.lit(datetime.now(timezone.utc)))\n",
        "        .withColumn(\"analysis_period_days\", F.lit(ANALYSIS_DAYS))\n",
        "        .select(\n",
        "            \"store_id\",\n",
        "            \"rank\",\n",
        "            \"path_string\",\n",
        "            \"path_length\",\n",
        "            \"occurrence_count\",\n",
        "            \"total_sessions\",\n",
        "            \"sessions_with_purchase\",\n",
        "            \"conversion_rate\",\n",
        "            \"avg_session_duration_minutes\",\n",
        "            \"avg_basket_cents\",\n",
        "            \"total_revenue_cents\",\n",
        "            \"analysis_period_days\",\n",
        "            \"computed_at\"\n",
        "        )\n",
        "        .orderBy(\"store_id\", \"rank\")\n",
        "    )\n",
        "    \n",
        "else:\n",
        "    # No receipts available, just output path frequency\n",
        "    print(\"Warning: fact_receipts not found, skipping conversion analysis\")\n",
        "    df_journey_patterns = (\n",
        "        df_top_paths\n",
        "        .withColumn(\"total_sessions\", F.col(\"occurrence_count\"))\n",
        "        .withColumn(\"sessions_with_purchase\", F.lit(None).cast(\"long\"))\n",
        "        .withColumn(\"conversion_rate\", F.lit(None).cast(\"double\"))\n",
        "        .withColumn(\"avg_basket_cents\", F.lit(None).cast(\"double\"))\n",
        "        .withColumn(\"total_revenue_cents\", F.lit(None).cast(\"long\"))\n",
        "        .withColumn(\"computed_at\", F.lit(datetime.now(timezone.utc)))\n",
        "        .withColumn(\"analysis_period_days\", F.lit(ANALYSIS_DAYS))\n",
        "        .select(\n",
        "            \"store_id\",\n",
        "            \"rank\",\n",
        "            \"path_string\",\n",
        "            \"path_length\",\n",
        "            \"occurrence_count\",\n",
        "            \"total_sessions\",\n",
        "            \"sessions_with_purchase\",\n",
        "            \"conversion_rate\",\n",
        "            \"avg_session_duration_minutes\",\n",
        "            \"avg_basket_cents\",\n",
        "            \"total_revenue_cents\",\n",
        "            \"analysis_period_days\",\n",
        "            \"computed_at\"\n",
        "        )\n",
        "        .orderBy(\"store_id\", \"rank\")\n",
        "    )\n",
        "\n",
        "print(\"Creating gold_journey_patterns...\")\n",
        "save_gold(df_journey_patterns, \"gold_journey_patterns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Display key insights from the journey analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"JOURNEY ANALYSIS COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nOutput Tables:\")\n",
        "print(f\"  {GOLD_DB}.gold_journey_patterns - Top {TOP_N_PATHS} paths per store with conversion metrics\")\n",
        "print(f\"  {GOLD_DB}.gold_zone_transitions - Zone transition probability matrix\")\n",
        "print(f\"  {GOLD_DB}.gold_zone_dwell_stats - Zone dwell time statistics\")\n",
        "\n",
        "print(f\"\\nSample Results (Top 5 Paths):\")\n",
        "df_journey_patterns.limit(5).show(truncate=False)\n",
        "\n",
        "print(f\"\\nAnalysis Period: Last {ANALYSIS_DAYS} days\")\n",
        "print(f\"Computed At: {datetime.now(timezone.utc)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
