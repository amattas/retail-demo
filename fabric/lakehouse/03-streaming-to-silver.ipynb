{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Streaming to Silver\n",
    "\n",
    "Incrementally processes real-time events from Eventhouse (cusn schema) to Silver Delta tables.\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "Tables/cusn.* (Eventhouse) --> Silver (Delta)\n",
    "```\n",
    "\n",
    "## Usage\n",
    "Schedule this notebook to run **every 5 minutes** via Fabric pipeline.\n",
    "\n",
    "Uses watermarks stored in `ag._watermarks` to track last processed timestamp per table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from datetime import datetime, timezone\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "def get_env(var_name, default=None):\n",
    "    return os.environ.get(var_name, default)\n",
    "\n",
    "SILVER_DB = get_env(\"SILVER_DB\", default=\"ag\")\n",
    "BRONZE_SCHEMA = get_env(\"BRONZE_SCHEMA\", default=\"cusn\")\n",
    "WATERMARK_TABLE = f\"{SILVER_DB}._watermarks\"\n",
    "\n",
    "print(f\"Configuration: SILVER_DB={SILVER_DB}, BRONZE_SCHEMA={BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WATERMARK MANAGEMENT\n",
    "# =============================================================================\n",
    "\n",
    "def ensure_watermark_table():\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {WATERMARK_TABLE} (\n",
    "            source_table STRING,\n",
    "            last_processed_ts TIMESTAMP,\n",
    "            updated_at TIMESTAMP\n",
    "        )\n",
    "        USING DELTA\n",
    "    \"\"\")\n",
    "    print(f\"Watermark table: {WATERMARK_TABLE}\")\n",
    "\n",
    "def get_watermark(source_table):\n",
    "    try:\n",
    "        result = spark.sql(f\"\"\"\n",
    "            SELECT last_processed_ts \n",
    "            FROM {WATERMARK_TABLE} \n",
    "            WHERE source_table = '{source_table}'\n",
    "        \"\"\").collect()\n",
    "        if result:\n",
    "            return result[0][0]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return datetime(1970, 1, 1, tzinfo=timezone.utc)\n",
    "\n",
    "def update_watermark(source_table, new_ts):\n",
    "    now = datetime.now(timezone.utc)\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {WATERMARK_TABLE} AS target\n",
    "        USING (SELECT '{source_table}' AS source_table) AS source\n",
    "        ON target.source_table = source.source_table\n",
    "        WHEN MATCHED THEN UPDATE SET \n",
    "            last_processed_ts = '{new_ts}',\n",
    "            updated_at = '{now}'\n",
    "        WHEN NOT MATCHED THEN INSERT \n",
    "            (source_table, last_processed_ts, updated_at)\n",
    "            VALUES ('{source_table}', '{new_ts}', '{now}')\n",
    "    \"\"\")\n",
    "\n",
    "ensure_watermark_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def cast_id_columns(df):\n",
    "    \"\"\"Cast ID columns to proper integer types to fix type mismatches.\"\"\"\n",
    "    \n",
    "    # Define ID columns that should be int64/long\n",
    "    id_columns_to_cast = {\n",
    "        # Dimension IDs\n",
    "        \"store_id\": \"long\",\n",
    "        \"dc_id\": \"long\", \n",
    "        \"truck_id\": \"long\",\n",
    "        \"customer_id\": \"long\",\n",
    "        \"product_id\": \"long\",\n",
    "        \"geography_id\": \"long\",\n",
    "        \n",
    "        # Other numeric IDs\n",
    "        \"line_number\": \"int\",\n",
    "        \"line_num\": \"int\",\n",
    "        \"quantity\": \"int\",\n",
    "        \"count\": \"int\",\n",
    "        \"item_count\": \"int\",\n",
    "        \"dwell_seconds\": \"int\",\n",
    "        \"rssi\": \"int\"\n",
    "    }\n",
    "    \n",
    "    # Cast columns if they exist in the dataframe\n",
    "    for col_name, col_type in id_columns_to_cast.items():\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(col_type))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def streaming_table_exists(table_name):\n",
    "    try:\n",
    "        spark.table(f\"{BRONZE_SCHEMA}.{table_name}\")\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "def process_events(source_table, target_table, transform_fn, ts_col=\"ingest_timestamp\"):\n",
    "    \"\"\"\n",
    "    Process new events from Eventhouse and append to Silver.\n",
    "    \n",
    "    Args:\n",
    "        source_table: Eventhouse source (e.g., \"receipt_created\")\n",
    "        target_table: Silver target (e.g., \"fact_receipts\")\n",
    "        transform_fn: Schema transformation function\n",
    "        ts_col: Timestamp column for watermarking\n",
    "    \"\"\"\n",
    "    print(f\"\\n{BRONZE_SCHEMA}.{source_table} -> {SILVER_DB}.{target_table}\")\n",
    "    \n",
    "    if not streaming_table_exists(source_table):\n",
    "        print(f\"  Skipping: source not found\")\n",
    "        return 0\n",
    "    \n",
    "    last_ts = get_watermark(source_table)\n",
    "    print(f\"  Watermark: {last_ts}\")\n",
    "    \n",
    "    df_stream = spark.table(f\"{BRONZE_SCHEMA}.{source_table}\")\n",
    "    df_new = df_stream.filter(F.col(ts_col) > last_ts)\n",
    "    \n",
    "    new_count = df_new.count()\n",
    "    if new_count == 0:\n",
    "        print(f\"  No new events\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"  Processing {new_count} events\")\n",
    "    \n",
    "    # Transform and cast ID columns\n",
    "    df_transformed = transform_fn(df_new)\n",
    "    df_transformed = cast_id_columns(df_transformed)\n",
    "    \n",
    "    df_transformed.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{SILVER_DB}.{target_table}\")\n",
    "    \n",
    "    max_ts = df_new.agg(F.max(ts_col)).collect()[0][0]\n",
    "    update_watermark(source_table, max_ts)\n",
    "    print(f\"  Appended {new_count} rows, watermark -> {max_ts}\")\n",
    "    \n",
    "    return new_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================",
    "# TRANSFORM FUNCTIONS",
    "# =============================================================================",
    "",
    "DEFAULT_RECEIPT_TYPE = \"SALE\"",
    "DEFAULT_DISCOUNT = 0.0",
    "",
    "def transform_receipt_created(df):",
    "    return df.select(",
    "        F.col(\"ingest_timestamp\").alias(\"event_ts\"),",
    "        F.col(\"receipt_id\").alias(\"receipt_id_ext\"),",
    "        F.col(\"tender_type\").alias(\"payment_method\"),",
    "        F.lit(DEFAULT_DISCOUNT).cast(\"string\").alias(\"discount_amount\"),",
    "        F.col(\"tax\"),",
    "        F.round(F.col(\"tax\") * 100).cast(\"bigint\").alias(\"tax_cents\"),",
    "        F.col(\"subtotal\"),",
    "        F.col(\"total\"),",
    "        F.round(F.col(\"total\") * 100).cast(\"bigint\").alias(\"total_cents\"),",
    "        F.lit(DEFAULT_RECEIPT_TYPE).alias(\"receipt_type\"),",
    "        F.round(F.col(\"subtotal\") * 100).cast(\"bigint\").alias(\"subtotal_cents\"),",
    "        F.col(\"customer_id\").cast(\"long\"),",
    "        F.col(\"store_id\").cast(\"long\"),",
    "        F.lit(None).cast(\"string\").alias(\"return_for_receipt_id_ext\")",
    "    )",
    "",
    "def transform_receipt_line_added(df):",
    "    return df.select(",
    "        F.col(\"receipt_id\").alias(\"receipt_id_ext\"),",
    "        F.col(\"ingest_timestamp\").alias(\"event_ts\"),",
    "        F.col(\"product_id\").cast(\"long\"),",
    "        F.col(\"line_number\").cast(\"int\").alias(\"line_num\"),",
    "        F.col(\"quantity\").cast(\"int\"),",
    "        F.col(\"unit_price\"),",
    "        F.col(\"extended_price\").alias(\"ext_price\"),",
    "        F.round(F.col(\"unit_price\") * 100).cast(\"bigint\").alias(\"unit_cents\"),",
    "        F.round(F.col(\"extended_price\") * 100).cast(\"bigint\").alias(\"ext_cents\"),",
    "        F.col(\"promo_code\")",
    "    )",
    "",
    "def transform_payment_processed(df):",
    "    return df.select(",
    "        F.col(\"ingest_timestamp\").alias(\"event_ts\"),",
    "        F.col(\"transaction_id\").alias(\"payment_id\"),",
    "        F.col(\"receipt_id\"),",
    "        F.col(\"payment_method\").alias(\"tender_type\"),",
    "        F.col(\"amount\"),",
    "        F.round(F.col(\"amount\") * 100).cast(\"bigint\").alias(\"amount_cents\")",
    "    )",
    "",
    "def transform_inventory_updated(df):",
    "    return df.select(",
    "        F.col(\"ingest_timestamp\").alias(\"event_ts\"),",
    "        F.col(\"store_id\").cast(\"long\"),",
    "        F.col(\"product_id\").cast(\"long\"),",
    "        F.col(\"quantity_delta\").alias(\"delta\"),",
    "        F.lit(None).cast(\"long\").alias(\"balance\"),  # Balance calculated later",
    "        F.col(\"reason\")",
    "    )",
    "",
    "def transform_customer_entered(df):",
    "    return df.select(",
    "        F.col(\"ingest_timestamp\").alias(\"event_ts\"),",
    "        F.col(\"store_id\").cast(\"long\"),",
    "        F.lit(None).cast(\"long\").alias(\"customer_id\"),  # Not always available",
    "        F.col(\"zone\"),",
    "        F.col(\"dwell_time\").cast(\"int\").alias(\"dwell_seconds\"),",
    "        F.col(\"customer_count\").cast(\"int\").alias(\"count\")",
    "    )",
    "",
    "def transform_truck_arrived(df):",
    "    return df.select(",
    "        F.col(\"ingest_timestamp\").alias(\"event_ts\"),",
    "        F.col(\"truck_id\"),",
    "        F.col(\"dc_id\").cast(\"long\"),",
    "        F.col(\"store_id\").cast(\"long\"),",
    "        F.col(\"shipment_id\"),",
    "        F.col(\"arrival_time\").alias(\"eta\"),",
    "        F.lit(None).cast(\"timestamp\").alias(\"etd\"),",
    "        F.lit(\"ARRIVED\").alias(\"status\")",
    "    )",
    "",
    "def transform_truck_departed(df):",
    "    return df.select(",
    "        F.col(\"ingest_timestamp\").alias(\"event_ts\"),",
    "        F.col(\"truck_id\"),",
    "        F.col(\"dc_id\").cast(\"long\"),",
    "        F.col(\"store_id\").cast(\"long\"),",
    "        F.col(\"shipment_id\"),",
    "        F.lit(None).cast(\"timestamp\").alias(\"eta\"),",
    "        F.col(\"departure_time\").alias(\"etd\"),",
    "        F.lit(\"DEPARTED\").alias(\"status\")",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)",
    "print(\"STREAMING TO SILVER\")",
    "print(\"=\"*60)",
    "",
    "total = 0",
    "",
    "# Transaction events",
    "total += process_events(\"receipt_created\", \"fact_receipts\", transform_receipt_created)",
    "total += process_events(\"receipt_line_added\", \"fact_receipt_lines\", transform_receipt_line_added)",
    "total += process_events(\"payment_processed\", \"fact_payments\", transform_payment_processed)",
    "",
    "# Inventory events",
    "total += process_events(\"inventory_updated\", \"fact_store_inventory_txn\", transform_inventory_updated)",
    "",
    "# Customer events",
    "total += process_events(\"customer_entered\", \"fact_foot_traffic\", transform_customer_entered)",
    "",
    "# Truck events",
    "total += process_events(\"truck_arrived\", \"fact_truck_moves\", transform_truck_arrived)",
    "total += process_events(\"truck_departed\", \"fact_truck_moves\", transform_truck_departed)",
    "",
    "# TODO: Add remaining event types:",
    "# - stockout_detected -> fact_stockouts",
    "# - reorder_triggered -> fact_reorders",
    "# - truck_arrived/departed -> fact_truck_moves",
    "# - store_opened/closed -> fact_store_ops",
    "# - ad_impression -> fact_marketing",
    "# - promotion_applied -> fact_promotions",
    "# - online_order_* -> fact_online_order_*",
    "",
    "print(\"\\n\" + \"=\"*60)",
    "print(f\"COMPLETE: {total} events processed\")",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show watermarks\n",
    "print(\"\\nCurrent Watermarks:\")\n",
    "spark.table(WATERMARK_TABLE).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}