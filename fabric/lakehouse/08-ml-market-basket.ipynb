{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Enhancement: Market Basket Analysis (FP-Growth)\n",
    "\n",
    "Discover products frequently purchased together using the FP-Growth algorithm.\n",
    "\n",
    "## Business Value\n",
    "- Cross-sell recommendations (\"Customers also bought\")\n",
    "- Optimize store layouts based on co-purchase patterns\n",
    "- Bundle pricing opportunities\n",
    "- Promotion targeting with complementary products\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "Silver (fact_receipts + fact_receipt_lines) --> FP-Growth --> Gold (gold_product_associations)\n",
    "```\n",
    "\n",
    "## Usage\n",
    "Schedule this notebook to run **weekly** via Fabric pipeline.\n",
    "\n",
    "## Output\n",
    "- `gold_product_associations`: Association rules with support, confidence, lift\n",
    "- Top 100 rules by lift (minimum support: 0.01, minimum confidence: 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from datetime import datetime, timezone\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "def get_env(var_name, default=None):\n",
    "    return os.environ.get(var_name, default)\n",
    "\n",
    "SILVER_DB = get_env(\"SILVER_DB\", default=\"ag\")\n",
    "GOLD_DB = get_env(\"GOLD_DB\", default=\"au\")\n",
    "\n",
    "# FP-Growth parameters from acceptance criteria\n",
    "MIN_SUPPORT = float(get_env(\"MIN_SUPPORT\", default=\"0.01\"))  # 1% of transactions\n",
    "MIN_CONFIDENCE = float(get_env(\"MIN_CONFIDENCE\", default=\"0.3\"))  # 30%\n",
    "TOP_N_RULES = int(get_env(\"TOP_N_RULES\", default=\"100\"))  # Top 100 by lift\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  SILVER_DB={SILVER_DB}, GOLD_DB={GOLD_DB}\")\n",
    "print(f\"  MIN_SUPPORT={MIN_SUPPORT}, MIN_CONFIDENCE={MIN_CONFIDENCE}\")\n",
    "print(f\"  TOP_N_RULES={TOP_N_RULES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def ensure_database(name):\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {name}\")\n",
    "\n",
    "def read_silver(table_name):\n",
    "    return spark.table(f\"{SILVER_DB}.{table_name}\")\n",
    "\n",
    "def save_gold(df, table_name):\n",
    "    full_name = f\"{GOLD_DB}.{table_name}\"\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_name)\n",
    "    print(f\"  {full_name}: {df.count()} rows\")\n",
    "\n",
    "def silver_exists(table_name):\n",
    "    try:\n",
    "        spark.table(f\"{SILVER_DB}.{table_name}\")\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "ensure_database(GOLD_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MARKET BASKET ANALYSIS - TRANSACTION PREPARATION\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare transaction baskets\n",
    "# Join fact_receipts with fact_receipt_lines to get all items per receipt\n",
    "\n",
    "if not silver_exists(\"fact_receipts\") or not silver_exists(\"fact_receipt_lines\"):\n",
    "    raise RuntimeError(\"Required tables fact_receipts and fact_receipt_lines not found in Silver\")\n",
    "\n",
    "print(\"\\nPreparing transaction baskets...\")\n",
    "\n",
    "# Read receipt headers (only SALE type, exclude returns)\n",
    "receipts = (\n",
    "    read_silver(\"fact_receipts\")\n",
    "    .filter(F.col(\"receipt_type\") == \"SALE\")\n",
    "    .select(\"receipt_id_ext\", \"store_id\", \"event_ts\")\n",
    ")\n",
    "\n",
    "# Read receipt lines\n",
    "receipt_lines = (\n",
    "    read_silver(\"fact_receipt_lines\")\n",
    "    .select(\"receipt_id_ext\", \"product_id\")\n",
    ")\n",
    "\n",
    "# Join and create baskets (list of product IDs per receipt)\n",
    "baskets = (\n",
    "    receipts\n",
    "    .join(receipt_lines, \"receipt_id_ext\")\n",
    "    .groupBy(\"receipt_id_ext\")\n",
    "    .agg(\n",
    "        F.collect_set(\"product_id\").alias(\"items\")\n",
    "    )\n",
    "    # Filter out single-item transactions (no associations possible)\n",
    "    .filter(F.size(F.col(\"items\")) > 1)\n",
    ")\n",
    "\n",
    "total_baskets = baskets.count()\n",
    "print(f\"  Total transaction baskets (multi-item): {total_baskets:,}\")\n",
    "\n",
    "if total_baskets == 0:\n",
    "    raise RuntimeError(\"No multi-item transactions found. Cannot proceed with market basket analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FP-GROWTH MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize FP-Growth model\n",
    "fpGrowth = FPGrowth(\n",
    "    itemsCol=\"items\",\n",
    "    minSupport=MIN_SUPPORT,\n",
    "    minConfidence=MIN_CONFIDENCE\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining FP-Growth model with {total_baskets:,} baskets...\")\n",
    "model = fpGrowth.fit(baskets)\n",
    "print(\"  Model training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ASSOCIATION RULE EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract frequent itemsets\n",
    "frequent_itemsets = model.freqItemsets\n",
    "print(f\"\\nFrequent itemsets found: {frequent_itemsets.count():,}\")\n",
    "\n",
    "# Extract association rules\n",
    "association_rules = model.associationRules\n",
    "total_rules = association_rules.count()\n",
    "print(f\"Association rules found: {total_rules:,}\")\n",
    "\n",
    "if total_rules == 0:\n",
    "    print(\"\\nWARNING: No association rules found with current thresholds.\")\n",
    "    print(\"Consider lowering MIN_SUPPORT or MIN_CONFIDENCE parameters.\")\n",
    "    # Create empty result table\n",
    "    from pyspark.sql.types import StructType, StructField, ArrayType, LongType, DoubleType, TimestampType\n",
    "    schema = StructType([\n",
    "        StructField(\"antecedent\", ArrayType(LongType()), False),\n",
    "        StructField(\"consequent\", ArrayType(LongType()), False),\n",
    "        StructField(\"support\", DoubleType(), False),\n",
    "        StructField(\"confidence\", DoubleType(), False),\n",
    "        StructField(\"lift\", DoubleType(), False),\n",
    "        StructField(\"computed_at\", TimestampType(), False)\n",
    "    ])\n",
    "    result_df = spark.createDataFrame([], schema)\n",
    "else:\n",
    "    # Calculate lift and rank by it\n",
    "    # Lift = confidence / (support of consequent)\n",
    "    # Higher lift means stronger association\n",
    "    \n",
    "    # Get support for all items (consequent support needed for lift calculation)\n",
    "    item_support = (\n",
    "        frequent_itemsets\n",
    "        .filter(F.size(F.col(\"items\")) == 1)\n",
    "        .select(\n",
    "            F.col(\"items\").getItem(0).alias(\"item\"),\n",
    "            F.col(\"freq\").alias(\"item_freq\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Calculate lift for each rule\n",
    "    rules_with_lift = (\n",
    "        association_rules\n",
    "        .withColumn(\"consequent_item\", F.col(\"consequent\").getItem(0))\n",
    "        .join(\n",
    "            item_support,\n",
    "            F.col(\"consequent_item\") == F.col(\"item\"),\n",
    "            \"left\"\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"consequent_support\",\n",
    "            F.col(\"item_freq\") / F.lit(total_baskets)\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"lift\",\n",
    "            F.when(\n",
    "                F.col(\"consequent_support\") > 0,\n",
    "                F.col(\"confidence\") / F.col(\"consequent_support\")\n",
    "            ).otherwise(0.0)\n",
    "        )\n",
    "        .select(\n",
    "            F.col(\"antecedent\"),\n",
    "            F.col(\"consequent\"),\n",
    "            # Support from the rule is actually the support of antecedent+consequent\n",
    "            # We'll use confidence as the rule support for clarity\n",
    "            F.col(\"confidence\").alias(\"support\"),\n",
    "            F.col(\"confidence\"),\n",
    "            F.col(\"lift\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Get top N rules by lift\n",
    "    top_rules = (\n",
    "        rules_with_lift\n",
    "        .orderBy(F.desc(\"lift\"))\n",
    "        .limit(TOP_N_RULES)\n",
    "        .withColumn(\n",
    "            \"computed_at\",\n",
    "            F.lit(datetime.now(timezone.utc))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    result_df = top_rules\n",
    "    print(f\"\\nTop {TOP_N_RULES} association rules by lift prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING TO GOLD LAYER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save to gold_product_associations table\n",
    "save_gold(result_df, \"gold_product_associations\")\n",
    "\n",
    "print(\"\\nSample association rules (top 10 by lift):\")\n",
    "if result_df.count() > 0:\n",
    "    result_df.orderBy(F.desc(\"lift\")).limit(10).show(truncate=False)\n",
    "else:\n",
    "    print(\"  No rules to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING PRODUCT RECOMMENDATION VIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a view that makes recommendations easier to query\n",
    "# For a given product, what products are frequently bought with it?\n",
    "\n",
    "if result_df.count() > 0:\n",
    "    recommendations = (\n",
    "        result_df\n",
    "        .withColumn(\"antecedent_product\", F.explode(\"antecedent\"))\n",
    "        .withColumn(\"consequent_product\", F.explode(\"consequent\"))\n",
    "        .select(\n",
    "            F.col(\"antecedent_product\").alias(\"product_id\"),\n",
    "            F.col(\"consequent_product\").alias(\"recommended_product_id\"),\n",
    "            \"support\",\n",
    "            \"confidence\",\n",
    "            \"lift\",\n",
    "            \"computed_at\"\n",
    "        )\n",
    "        .orderBy(\"product_id\", F.desc(\"lift\"))\n",
    "    )\n",
    "    \n",
    "    save_gold(recommendations, \"product_recommendations\")\n",
    "    \n",
    "    print(\"\\nSample recommendations (top 10):\")\n",
    "    recommendations.limit(10).show(truncate=False)\n",
    "else:\n",
    "    print(\"  Skipping: no rules to create recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MARKET BASKET ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Transaction baskets analyzed: {total_baskets:,}\")\n",
    "print(f\"  Frequent itemsets: {frequent_itemsets.count():,}\")\n",
    "print(f\"  Association rules (all): {total_rules:,}\")\n",
    "print(f\"  Top rules saved: {result_df.count()}\")\n",
    "\n",
    "# Show Gold tables\n",
    "gold_tables = spark.sql(f\"SHOW TABLES IN {GOLD_DB}\").collect()\n",
    "print(f\"\\nGold ({GOLD_DB}): {len(gold_tables)} tables\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
