{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Historical Data Load (Bronze -> Silver -> Gold)\n",
    "\n",
    "Loads batch historical data from Files/ parquet shortcuts through the complete medallion pipeline.\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "Files/ (parquet) --> Silver (Delta) --> Gold (Delta)\n",
    "```\n",
    "\n",
    "## Usage\n",
    "Run this notebook **once** to load historical batch data.\n",
    "\n",
    "For streaming data, use:\n",
    "- `03-streaming-to-silver.ipynb` - Process Eventhouse events to Silver\n",
    "- `04-streaming-to-gold.ipynb` - Aggregate streaming data to Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from notebookutils import mssparkutils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "def get_env(var_name, default=None):\n",
    "    return os.environ.get(var_name, default)\n",
    "\n",
    "SILVER_DB = get_env(\"SILVER_DB\", default=\"ag\")\n",
    "GOLD_DB = get_env(\"GOLD_DB\", default=\"au\")\n",
    "\n",
    "print(f\"Configuration: SILVER_DB={SILVER_DB}, GOLD_DB={GOLD_DB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def ensure_database(name):\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {name}\")\n",
    "    print(f\"Database '{name}' ready.\")\n",
    "\n",
    "def parquet_exists(table_name):\n",
    "    try:\n",
    "        mssparkutils.fs.ls(f\"Files/{table_name}\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def drop_table_if_exists(db, table_name):\n",
    "    \"\"\"Drop table if it exists.\"\"\"\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {db}.{table_name}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def load_to_silver(table_name):\n",
    "    \"\"\"Load parquet from Files/ to Silver Delta table (drop and recreate).\"\"\"\n",
    "    if not parquet_exists(table_name):\n",
    "        print(f\"  Skipping: Files/{table_name} does not exist\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Drop existing table first\n",
    "        drop_table_if_exists(SILVER_DB, table_name)\n",
    "        \n",
    "        # Read parquet - disable schema merge to avoid type conflicts\n",
    "        # Uses schema from first file encountered\n",
    "        df = spark.read.option(\"mergeSchema\", \"false\").parquet(f\"Files/{table_name}\")\n",
    "        \n",
    "        # Cast Source column to string if present (known schema inconsistency)\n",
    "        if \"Source\" in df.columns:\n",
    "            df = df.withColumn(\"Source\", F.col(\"Source\").cast(\"string\"))\n",
    "        \n",
    "        full_name = f\"{SILVER_DB}.{table_name}\"\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_name)\n",
    "        print(f\"  {full_name}: {df.count()} rows\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading {table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def read_silver(table_name):\n",
    "    return spark.table(f\"{SILVER_DB}.{table_name}\")\n",
    "\n",
    "def save_gold(df, table_name):\n",
    "    drop_table_if_exists(GOLD_DB, table_name)\n",
    "    full_name = f\"{GOLD_DB}.{table_name}\"\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_name)\n",
    "    print(f\"  {full_name}: {df.count()} rows\")\n",
    "\n",
    "def silver_exists(table_name):\n",
    "    try:\n",
    "        spark.table(f\"{SILVER_DB}.{table_name}\")\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "ensure_database(SILVER_DB)\n",
    "ensure_database(GOLD_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Files/ -> Silver\n",
    "\n",
    "Load dimension and fact tables from parquet to Delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LOADING DIMENSIONS TO SILVER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dimensions = [\n",
    "    \"dim_geographies\", \"dim_stores\", \"dim_distribution_centers\",\n",
    "    \"dim_trucks\", \"dim_customers\", \"dim_products\"\n",
    "]\n",
    "\n",
    "for table in dimensions:\n",
    "    load_to_silver(table)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LOADING FACTS TO SILVER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "facts = [\n",
    "    \"fact_receipts\", \"fact_receipt_lines\", \"fact_payments\",\n",
    "    \"fact_store_inventory_txn\", \"fact_dc_inventory_txn\",\n",
    "    \"fact_truck_moves\", \"fact_truck_inventory\",\n",
    "    \"fact_foot_traffic\", \"fact_ble_pings\", \"fact_customer_zone_changes\",\n",
    "    \"fact_marketing\", \"fact_online_order_headers\", \"fact_online_order_lines\",\n",
    "    \"fact_store_ops\", \"fact_stockouts\", \"fact_promotions\",\n",
    "    \"fact_promo_lines\", \"fact_reorders\"\n",
    "]\n",
    "\n",
    "for table in facts:\n",
    "    load_to_silver(table)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Silver -> Gold\n",
    "\n",
    "Create aggregated Gold tables for dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CREATING GOLD AGGREGATES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sales by minute per store\n",
    "if silver_exists(\"fact_receipts\"):\n",
    "    print(\"Creating sales_minute_store...\")\n",
    "    df = (\n",
    "        read_silver(\"fact_receipts\")\n",
    "        .withColumn(\"ts\", F.date_trunc(\"minute\", F.col(\"event_ts\")))\n",
    "        .groupBy(\"store_id\", \"ts\")\n",
    "        .agg(\n",
    "            F.sum(\"total\").alias(\"total_sales\"),\n",
    "            F.count(\"*\").alias(\"receipts\"),\n",
    "            F.avg(\"total\").alias(\"avg_basket\")\n",
    "        )\n",
    "    )\n",
    "    save_gold(df, \"sales_minute_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top products by revenue (15m windows)\n",
    "if silver_exists(\"fact_receipt_lines\"):\n",
    "    print(\"Creating top_products_15m...\")\n",
    "    df = (\n",
    "        read_silver(\"fact_receipt_lines\")\n",
    "        .withColumn(\"window_15m\", F.window(F.col(\"event_ts\"), \"15 minutes\"))\n",
    "        .groupBy(\"product_id\", \"window_15m\")\n",
    "        .agg(\n",
    "            F.sum(\"ext_price\").alias(\"revenue\"),\n",
    "            F.sum(\"quantity\").alias(\"units\")\n",
    "        )\n",
    "        .withColumn(\"computed_at\", F.col(\"window_15m.end\"))\n",
    "        .drop(\"window_15m\")\n",
    "    )\n",
    "    save_gold(df, \"top_products_15m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current store inventory position\n",
    "if silver_exists(\"fact_store_inventory_txn\"):\n",
    "    print(\"Creating inventory_position_current...\")\n",
    "    window_spec = Window.partitionBy(\"store_id\", \"product_id\").orderBy(F.desc(\"event_ts\"))\n",
    "    df = (\n",
    "        read_silver(\"fact_store_inventory_txn\")\n",
    "        .withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"rn\") == 1)\n",
    "        .select(\n",
    "            \"store_id\", \"product_id\",\n",
    "            F.col(\"balance\").alias(\"on_hand\"),\n",
    "            F.col(\"event_ts\").alias(\"as_of\")\n",
    "        )\n",
    "    )\n",
    "    save_gold(df, \"inventory_position_current\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC inventory position\n",
    "if silver_exists(\"fact_dc_inventory_txn\"):\n",
    "    print(\"Creating dc_inventory_position_current...\")\n",
    "    window_spec = Window.partitionBy(\"dc_id\", \"product_id\").orderBy(F.desc(\"event_ts\"))\n",
    "    df = (\n",
    "        read_silver(\"fact_dc_inventory_txn\")\n",
    "        .withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"rn\") == 1)\n",
    "        .select(\n",
    "            \"dc_id\", \"product_id\",\n",
    "            F.col(\"balance\").alias(\"on_hand\"),\n",
    "            F.col(\"event_ts\").alias(\"as_of\")\n",
    "        )\n",
    "    )\n",
    "    save_gold(df, \"dc_inventory_position_current\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truck dwell time daily\n",
    "if silver_exists(\"fact_truck_moves\"):\n",
    "    print(\"Creating truck_dwell_daily...\")\n",
    "    df = (\n",
    "        read_silver(\"fact_truck_moves\")\n",
    "        .withColumn(\"day\", F.to_date(\"event_ts\"))\n",
    "        .withColumn(\"site\", \n",
    "            F.when(F.col(\"store_id\").isNotNull(), F.concat(F.lit(\"STORE_\"), F.col(\"store_id\")))\n",
    "             .otherwise(F.concat(F.lit(\"DC_\"), F.col(\"dc_id\")))\n",
    "        )\n",
    "        .withColumn(\"dwell_min\", \n",
    "            (F.unix_timestamp(\"etd\") - F.unix_timestamp(\"eta\")) / 60\n",
    "        )\n",
    "        .filter(F.col(\"dwell_min\").isNotNull() & (F.col(\"dwell_min\") > 0))\n",
    "        .groupBy(\"site\", \"day\")\n",
    "        .agg(\n",
    "            F.avg(\"dwell_min\").alias(\"avg_dwell_min\"),\n",
    "            F.countDistinct(\"truck_id\").alias(\"trucks\")\n",
    "        )\n",
    "    )\n",
    "    save_gold(df, \"truck_dwell_daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online sales daily\n",
    "if silver_exists(\"fact_online_order_headers\"):\n",
    "    print(\"Creating online_sales_daily...\")\n",
    "    df = (\n",
    "        read_silver(\"fact_online_order_headers\")\n",
    "        .withColumn(\"day\", F.to_date(\"event_ts\"))\n",
    "        .groupBy(\"day\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"orders\"),\n",
    "            F.sum(\"subtotal\").alias(\"subtotal\"),\n",
    "            F.sum(\"tax\").alias(\"tax\"),\n",
    "            F.sum(\"total\").alias(\"total\"),\n",
    "            F.avg(\"total\").alias(\"avg_order_value\")\n",
    "        )\n",
    "    )\n",
    "    save_gold(df, \"online_sales_daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zone dwell per minute\n",
    "if silver_exists(\"fact_foot_traffic\"):\n",
    "    print(\"Creating zone_dwell_minute...\")\n",
    "    df = (\n",
    "        read_silver(\"fact_foot_traffic\")\n",
    "        .withColumn(\"ts\", F.date_trunc(\"minute\", F.col(\"event_ts\")))\n",
    "        .groupBy(\"store_id\", \"zone\", \"ts\")\n",
    "        .agg(\n",
    "            F.avg(\"dwell_seconds\").alias(\"avg_dwell\"),\n",
    "            F.sum(\"count\").alias(\"customers\")\n",
    "        )\n",
    "    )\n",
    "    save_gold(df, \"zone_dwell_minute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marketing cost daily\n",
    "if silver_exists(\"fact_marketing\"):\n",
    "    print(\"Creating marketing_cost_daily...\")\n",
    "    df = (\n",
    "        read_silver(\"fact_marketing\")\n",
    "        .withColumn(\"day\", F.to_date(\"event_ts\"))\n",
    "        .groupBy(\"campaign_id\", \"day\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"impressions\"),\n",
    "            F.sum(\"cost\").alias(\"cost\")\n",
    "        )\n",
    "    )\n",
    "    save_gold(df, \"marketing_cost_daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tender mix daily\n",
    "if silver_exists(\"fact_receipts\"):\n",
    "    print(\"Creating tender_mix_daily...\")\n",
    "    df = (\n",
    "        read_silver(\"fact_receipts\")\n",
    "        .withColumn(\"day\", F.to_date(\"event_ts\"))\n",
    "        .groupBy(\"day\", \"payment_method\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"transactions\"),\n",
    "            F.sum(\"total\").alias(\"total_amount\")\n",
    "        )\n",
    "    )\n",
    "    save_gold(df, \"tender_mix_daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HISTORICAL DATA LOAD COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "silver_tables = spark.sql(f\"SHOW TABLES IN {SILVER_DB}\").collect()\n",
    "gold_tables = spark.sql(f\"SHOW TABLES IN {GOLD_DB}\").collect()\n",
    "\n",
    "print(f\"\\nSilver ({SILVER_DB}): {len(silver_tables)} tables\")\n",
    "print(f\"Gold ({GOLD_DB}): {len(gold_tables)} tables\")\n",
    "\n",
    "print(\"\\nHistorical data pipeline complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}