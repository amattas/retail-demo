{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Silver to Gold\n",
    "\n",
    "Creates Gold layer aggregated tables from Silver Delta tables.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run `02-onelake-to-silver.ipynb` first to populate Silver tables\n",
    "\n",
    "**Gold Tables Created:**\n",
    "- Sales aggregates (minute, daily)\n",
    "- Inventory position (current snapshot)\n",
    "- Fulfillment metrics\n",
    "- Marketing/campaign metrics\n",
    "- Zone dwell and BLE presence\n",
    "\n",
    "**Note:** Some tables depend on fact tables not yet implemented (stockouts, reorders, promotions, store_ops). These are stubbed but will be empty until datagen is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from datetime import datetime\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# =============================================================================\n",
    "# PARAMETERS - Configure these for your environment\n",
    "# =============================================================================\n",
    "# REQUIRED ENVIRONMENT VARIABLES:\n",
    "#   - SILVER_DB: Database name for Silver layer tables (source)\n",
    "#   - GOLD_DB: Database name for Gold layer tables (target)\n",
    "#\n",
    "# These can be set via:\n",
    "#   1. Fabric pipeline parameters (when run from a pipeline)\n",
    "#   2. Environment variables in the Fabric workspace\n",
    "#   3. Notebook %run magic or widget parameters\n",
    "#\n",
    "# For local testing, you can uncomment the defaults below:\n",
    "#   SILVER_DB = \"ag\"\n",
    "#   GOLD_DB = \"au\"\n",
    "# =============================================================================\n",
    "\n",
    "def get_required_env(var_name, description, default=None):\n",
    "    \"\"\"Get required environment variable with clear error message.\"\"\"\n",
    "    value = os.environ.get(var_name, default)\n",
    "    if value is None:\n",
    "        raise EnvironmentError(\n",
    "            f\"Required environment variable '{var_name}' is not set.\\n\"\n",
    "            f\"Description: {description}\\n\"\n",
    "            f\"Set it via Fabric pipeline parameters or workspace environment variables.\"\n",
    "        )\n",
    "    return value\n",
    "\n",
    "# Database names - REQUIRED\n",
    "# Uncomment the defaults for local testing\n",
    "SILVER_DB = get_required_env(\n",
    "    \"SILVER_DB\",\n",
    "    \"Source database containing Silver layer Delta tables\",\n",
    "    default=\"ag\"  # Default for backward compatibility; remove in production\n",
    ")\n",
    "GOLD_DB = get_required_env(\n",
    "    \"GOLD_DB\",\n",
    "    \"Target database for Gold layer aggregated tables\",\n",
    "    default=\"au\"  # Default for backward compatibility; remove in production\n",
    ")\n",
    "\n",
    "print(f\"Configuration: SILVER_DB={SILVER_DB}, GOLD_DB={GOLD_DB}\")\n",
    "\n",
    "def ensure_database(name):\n",
    "    \"\"\"Create database if it doesn't exist and validate access.\"\"\"\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {name}\")\n",
    "    # Validate we can access the database\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE DATABASE {name}\")\n",
    "        print(f\"Database '{name}' is ready.\")\n",
    "    except AnalysisException as e:\n",
    "        raise RuntimeError(f\"Cannot access database '{name}': {e}\")\n",
    "\n",
    "def save_gold_table(df, table_name, mode=\"overwrite\"):\n",
    "    full_name = f\"{GOLD_DB}.{table_name}\"\n",
    "    df.write.format(\"delta\").mode(mode).saveAsTable(full_name)\n",
    "    print(f\"  Written to {full_name}: {df.count()} rows\")\n",
    "\n",
    "def read_silver(table_name):\n",
    "    return spark.table(f\"{SILVER_DB}.{table_name}\")\n",
    "\n",
    "def table_exists(db, table):\n",
    "    try:\n",
    "        spark.table(f\"{db}.{table}\")\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "def process_gold_table(table_name, transform_fn):\n",
    "    \"\"\"Process a gold table with proper exception handling.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the gold table to create\n",
    "        transform_fn: Function that returns a DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        True if processed successfully, False if skipped\n",
    "        \n",
    "    Raises:\n",
    "        PermissionError: Re-raised for infrastructure issues\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Creating {table_name}...\")\n",
    "        df = transform_fn()\n",
    "        save_gold_table(df, table_name)\n",
    "        return True\n",
    "    except AnalysisException as e:\n",
    "        # Table doesn't exist in Silver or schema mismatch\n",
    "        print(f\"  Skipping {table_name}: Source table not available - {e}\")\n",
    "        return False\n",
    "    except PermissionError as e:\n",
    "        # Re-raise permission errors - infrastructure problem\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        # Log unexpected errors with type for debugging\n",
    "        print(f\"  Skipping {table_name}: {type(e).__name__}: {e}\")\n",
    "        return False\n",
    "\n",
    "ensure_database(GOLD_DB)\n",
    "print(f\"Gold database ready: {GOLD_DB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_sales_minute_store - Sales by minute per store\n",
    "def create_sales_minute_store():\n",
    "    df_receipts = read_silver(\"fact_receipts\")\n",
    "    return (\n",
    "        df_receipts\n",
    "        .withColumn(\"ts\", F.date_trunc(\"minute\", F.col(\"event_ts\")))\n",
    "        .groupBy(\"store_id\", \"ts\")\n",
    "        .agg(\n",
    "            F.sum(\"total\").alias(\"total_sales\"),\n",
    "            F.count(\"*\").alias(\"receipts\"),\n",
    "            F.avg(\"total\").alias(\"avg_basket\")\n",
    "        )\n",
    "        .orderBy(\"store_id\", \"ts\")\n",
    "    )\n",
    "\n",
    "process_gold_table(\"sales_minute_store\", create_sales_minute_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_top_products_15m - Top products by revenue (rolling 15m windows)\n",
    "def create_top_products_15m():\n",
    "    df_lines = read_silver(\"fact_receipt_lines\")\n",
    "    return (\n",
    "        df_lines\n",
    "        .withColumn(\"ts\", F.date_trunc(\"minute\", F.col(\"event_ts\")))\n",
    "        .withColumn(\"window_15m\", F.window(F.col(\"event_ts\"), \"15 minutes\"))\n",
    "        .groupBy(\"product_id\", \"window_15m\")\n",
    "        .agg(\n",
    "            F.sum(\"ext_price\").alias(\"revenue\"),\n",
    "            F.sum(\"quantity\").alias(\"units\")\n",
    "        )\n",
    "        .withColumn(\"computed_at\", F.col(\"window_15m.end\"))\n",
    "        .drop(\"window_15m\")\n",
    "        .orderBy(F.desc(\"revenue\"))\n",
    "    )\n",
    "\n",
    "process_gold_table(\"top_products_15m\", create_top_products_15m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inventory Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_inventory_position_current - Current store inventory\n",
    "def create_inventory_position_current():\n",
    "    df_store_inv = read_silver(\"fact_store_inventory_txn\")\n",
    "    window_spec = Window.partitionBy(\"store_id\", \"product_id\").orderBy(F.desc(\"event_ts\"))\n",
    "    return (\n",
    "        df_store_inv\n",
    "        .withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"rn\") == 1)\n",
    "        .select(\n",
    "            \"store_id\",\n",
    "            \"product_id\",\n",
    "            F.col(\"balance\").alias(\"on_hand\"),\n",
    "            F.col(\"event_ts\").alias(\"as_of\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "process_gold_table(\"inventory_position_current\", create_inventory_position_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_dc_inventory_position_current - Current DC inventory\n",
    "def create_dc_inventory_position_current():\n",
    "    df_dc_inv = read_silver(\"fact_dc_inventory_txn\")\n",
    "    window_spec = Window.partitionBy(\"dc_id\", \"product_id\").orderBy(F.desc(\"event_ts\"))\n",
    "    return (\n",
    "        df_dc_inv\n",
    "        .withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"rn\") == 1)\n",
    "        .select(\n",
    "            \"dc_id\",\n",
    "            \"product_id\",\n",
    "            F.col(\"balance\").alias(\"on_hand\"),\n",
    "            F.col(\"event_ts\").alias(\"as_of\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "process_gold_table(\"dc_inventory_position_current\", create_dc_inventory_position_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics & Fulfillment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_truck_dwell_daily - Truck dwell time by site per day\n",
    "def create_truck_dwell_daily():\n",
    "    df_trucks = read_silver(\"fact_truck_moves\")\n",
    "    return (\n",
    "        df_trucks\n",
    "        .withColumn(\"day\", F.to_date(\"event_ts\"))\n",
    "        .withColumn(\"site\", \n",
    "            F.when(F.col(\"store_id\").isNotNull(), F.concat(F.lit(\"STORE_\"), F.col(\"store_id\")))\n",
    "             .otherwise(F.concat(F.lit(\"DC_\"), F.col(\"dc_id\")))\n",
    "        )\n",
    "        .withColumn(\"dwell_min\", \n",
    "            (F.unix_timestamp(\"etd\") - F.unix_timestamp(\"eta\")) / 60\n",
    "        )\n",
    "        .filter(F.col(\"dwell_min\").isNotNull() & (F.col(\"dwell_min\") > 0))\n",
    "        .groupBy(\"site\", \"day\")\n",
    "        .agg(\n",
    "            F.avg(\"dwell_min\").alias(\"avg_dwell_min\"),\n",
    "            F.countDistinct(\"truck_id\").alias(\"trucks\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "process_gold_table(\"truck_dwell_daily\", create_truck_dwell_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_online_sales_daily - Online sales aggregated daily\n",
    "def create_online_sales_daily():\n",
    "    df_online = read_silver(\"fact_online_order_headers\")\n",
    "    return (\n",
    "        df_online\n",
    "        .withColumn(\"day\", F.to_date(\"event_ts\"))\n",
    "        .groupBy(\"day\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"orders\"),\n",
    "            F.sum(\"subtotal\").alias(\"subtotal\"),\n",
    "            F.sum(\"tax\").alias(\"tax\"),\n",
    "            F.sum(\"total\").alias(\"total\"),\n",
    "            F.avg(\"total\").alias(\"avg_order_value\")\n",
    "        )\n",
    "        .orderBy(\"day\")\n",
    "    )\n",
    "\n",
    "process_gold_table(\"online_sales_daily\", create_online_sales_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_fulfillment_daily - Fulfillment performance daily\n",
    "def create_fulfillment_daily():\n",
    "    df_lines = read_silver(\"fact_online_order_lines\")\n",
    "    return (\n",
    "        df_lines\n",
    "        .withColumn(\"day\", F.to_date(F.coalesce(\"shipped_ts\", \"picked_ts\", \"delivered_ts\")))\n",
    "        .filter(F.col(\"day\").isNotNull())\n",
    "        .groupBy(\"day\", \"fulfillment_mode\", \"fulfillment_status\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"order_id\").alias(\"orders\"),\n",
    "            F.sum(\"quantity\").alias(\"units\")\n",
    "        )\n",
    "        .orderBy(\"day\")\n",
    "    )\n",
    "\n",
    "process_gold_table(\"fulfillment_daily\", create_fulfillment_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer & Zone Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_zone_dwell_minute - Zone dwell per minute\n",
    "def create_zone_dwell_minute():\n",
    "    df_traffic = read_silver(\"fact_foot_traffic\")\n",
    "    return (\n",
    "        df_traffic\n",
    "        .withColumn(\"ts\", F.date_trunc(\"minute\", F.col(\"event_ts\")))\n",
    "        .groupBy(\"store_id\", \"zone\", \"ts\")\n",
    "        .agg(\n",
    "            F.avg(\"dwell_seconds\").alias(\"avg_dwell\"),\n",
    "            F.sum(\"count\").alias(\"customers\")\n",
    "        )\n",
    "        .orderBy(\"store_id\", \"zone\", \"ts\")\n",
    "    )\n",
    "\n",
    "process_gold_table(\"zone_dwell_minute\", create_zone_dwell_minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_ble_presence_minute - BLE device presence per minute\n",
    "def create_ble_presence_minute():\n",
    "    df_ble = read_silver(\"fact_ble_pings\")\n",
    "    return (\n",
    "        df_ble\n",
    "        .withColumn(\"ts\", F.date_trunc(\"minute\", F.col(\"event_ts\")))\n",
    "        .groupBy(\"store_id\", \"ts\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_ble_id\").alias(\"devices\")\n",
    "        )\n",
    "        .orderBy(\"store_id\", \"ts\")\n",
    "    )\n",
    "\n",
    "process_gold_table(\"ble_presence_minute\", create_ble_presence_minute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marketing & Campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_marketing_cost_daily - Marketing impressions and cost daily\n",
    "def create_marketing_cost_daily():\n",
    "    df_marketing = read_silver(\"fact_marketing\")\n",
    "    return (\n",
    "        df_marketing\n",
    "        .withColumn(\"day\", F.to_date(\"event_ts\"))\n",
    "        .groupBy(\"campaign_id\", \"day\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"impressions\"),\n",
    "            F.sum(\"cost\").alias(\"cost\")\n",
    "        )\n",
    "        .orderBy(\"campaign_id\", \"day\")\n",
    "    )\n",
    "\n",
    "process_gold_table(\"marketing_cost_daily\", create_marketing_cost_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_campaign_revenue_daily - Campaign revenue with conversion tracking\n",
    "# Note: This requires joining marketing impressions with receipts via customer_id\n",
    "# which is a complex attribution model. Simplified version here.\n",
    "def create_campaign_revenue_daily():\n",
    "    df_marketing = read_silver(\"fact_marketing\")\n",
    "    df_receipts = read_silver(\"fact_receipts\")\n",
    "    \n",
    "    # Simple attribution: count conversions where customer had impression same day\n",
    "    df_mkt_day = df_marketing.withColumn(\"day\", F.to_date(\"event_ts\")).select(\n",
    "        \"campaign_id\", \"day\", \"customer_id\"\n",
    "    ).distinct()\n",
    "    \n",
    "    df_receipts_day = df_receipts.withColumn(\"day\", F.to_date(\"event_ts\")).select(\n",
    "        \"customer_id\", \"day\", \"total\"\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        df_mkt_day\n",
    "        .join(df_receipts_day, [\"customer_id\", \"day\"], \"left\")\n",
    "        .groupBy(\"campaign_id\", \"day\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"impressions\"),\n",
    "            F.count(\"total\").alias(\"conversions\"),\n",
    "            F.sum(\"total\").alias(\"revenue\")\n",
    "        )\n",
    "        .orderBy(\"campaign_id\", \"day\")\n",
    "    )\n",
    "\n",
    "process_gold_table(\"campaign_revenue_daily\", create_campaign_revenue_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tender Mix (Payments)\n",
    "\n",
    "**Note:** This uses `payment_method` from `fact_receipts`. \n",
    "A dedicated `fact_payments` table is planned (see GitHub issue #7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_tender_mix_daily - Payment methods by day\n",
    "def create_tender_mix_daily():\n",
    "    df_receipts = read_silver(\"fact_receipts\")\n",
    "    return (\n",
    "        df_receipts\n",
    "        .withColumn(\"day\", F.to_date(\"event_ts\"))\n",
    "        .groupBy(\"day\", \"payment_method\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"transactions\"),\n",
    "            F.sum(\"total\").alias(\"total_amount\")\n",
    "        )\n",
    "        .orderBy(\"day\", \"payment_method\")\n",
    "    )\n",
    "\n",
    "process_gold_table(\"tender_mix_daily\", create_tender_mix_daily)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}