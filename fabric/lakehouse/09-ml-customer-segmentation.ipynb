{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ML: Customer Segmentation (RFM + K-means)\n",
    "\n",
    "Implements customer segmentation using RFM (Recency, Frequency, Monetary) analysis with K-means clustering.\n",
    "\n",
    "## Business Value\n",
    "- Target marketing campaigns to high-value customers\n",
    "- Identify at-risk customers for retention efforts\n",
    "- Personalize promotions by segment\n",
    "- Optimize marketing spend allocation\n",
    "\n",
    "## Technical Approach\n",
    "- **Features:** RFM metrics (Recency, Frequency, Monetary value)\n",
    "- **Algorithm:** K-means clustering with elbow method\n",
    "- **Platform:** Synapse Data Science with PySpark MLlib\n",
    "- **Segments:** Champion, Loyal, At-Risk, Lost, New, etc.\n",
    "\n",
    "## Data Flow\n",
    "```\n",
    "Silver (fact_receipts, dim_customers) --> Gold (gold_customer_segments)\n",
    "```\n",
    "\n",
    "## Usage\n",
    "Schedule this notebook to run **weekly** via Fabric pipeline to refresh customer segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from datetime import datetime, timezone\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "def get_env(var_name, default=None):\n",
    "    return os.environ.get(var_name, default)\n",
    "\n",
    "SILVER_DB = get_env(\"SILVER_DB\", default=\"ag\")\n",
    "GOLD_DB = get_env(\"GOLD_DB\", default=\"au\")\n",
    "\n",
    "# K-means configuration\n",
    "MIN_K = 4\n",
    "MAX_K = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(f\"Configuration: SILVER_DB={SILVER_DB}, GOLD_DB={GOLD_DB}\")\n",
    "print(f\"K-means range: {MIN_K}-{MAX_K} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def ensure_database(name):\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {name}\")\n",
    "\n",
    "def read_silver(table_name):\n",
    "    return spark.table(f\"{SILVER_DB}.{table_name}\")\n",
    "\n",
    "def save_gold(df, table_name):\n",
    "    full_name = f\"{GOLD_DB}.{table_name}\"\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_name)\n",
    "    print(f\"  {full_name}: {df.count()} rows\")\n",
    "\n",
    "ensure_database(GOLD_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Calculate RFM Metrics\n",
    "\n",
    "Calculate RFM (Recency, Frequency, Monetary) metrics for each customer:\n",
    "- **Recency**: Days since last purchase\n",
    "- **Frequency**: Total number of transactions\n",
    "- **Monetary**: Total spending amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CALCULATING RFM METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get current timestamp for recency calculation\n",
    "analysis_date = datetime.now(timezone.utc)\n",
    "print(f\"\\nAnalysis date: {analysis_date}\")\n",
    "\n",
    "# Read receipt data\n",
    "df_receipts = read_silver(\"fact_receipts\")\n",
    "\n",
    "print(f\"\\nTotal receipts: {df_receipts.count():,}\")\n",
    "print(f\"Date range: {df_receipts.agg(F.min('event_ts')).collect()[0][0]} to {df_receipts.agg(F.max('event_ts')).collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM metrics per customer\n",
    "df_rfm = (\n",
    "    df_receipts\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        # Recency: days since last purchase\n",
    "        F.datediff(\n",
    "            F.lit(analysis_date.date()),\n",
    "            F.max(F.to_date(\"event_ts\"))\n",
    "        ).alias(\"recency_days\"),\n",
    "        \n",
    "        # Frequency: total number of transactions\n",
    "        F.count(\"receipt_id_ext\").alias(\"frequency\"),\n",
    "        \n",
    "        # Monetary: total spending (use total_amount or total_cents/100)\n",
    "        F.sum(\n",
    "            F.when(F.col(\"total_amount\").isNotNull(), F.col(\"total_amount\"))\n",
    "            .otherwise(F.col(\"total_cents\") / 100.0)\n",
    "        ).alias(\"monetary_value\"),\n",
    "        \n",
    "        # Additional metrics for profiling\n",
    "        F.min(F.to_date(\"event_ts\")).alias(\"first_purchase_date\"),\n",
    "        F.max(F.to_date(\"event_ts\")).alias(\"last_purchase_date\")\n",
    "    )\n",
    "    # Filter out customers with invalid data\n",
    "    .filter(\n",
    "        (F.col(\"recency_days\").isNotNull()) &\n",
    "        (F.col(\"frequency\") > 0) &\n",
    "        (F.col(\"monetary_value\") > 0)\n",
    "    )\n",
    "    # Calculate average order value\n",
    "    .withColumn(\"avg_order_value\", F.col(\"monetary_value\") / F.col(\"frequency\"))\n",
    ")\n",
    "\n",
    "print(f\"\\nCustomers with RFM metrics: {df_rfm.count():,}\")\n",
    "\n",
    "# Show sample and statistics\n",
    "print(\"\\nSample RFM data:\")\n",
    "df_rfm.orderBy(F.rand()).limit(5).show()\n",
    "\n",
    "print(\"\\nRFM Statistics:\")\n",
    "df_rfm.select(\n",
    "    F.mean(\"recency_days\").alias(\"avg_recency\"),\n",
    "    F.mean(\"frequency\").alias(\"avg_frequency\"),\n",
    "    F.mean(\"monetary_value\").alias(\"avg_monetary\"),\n",
    "    F.mean(\"avg_order_value\").alias(\"avg_order_value\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Feature Engineering & Scaling\n",
    "\n",
    "Standardize RFM features for K-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING & SCALING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cast to double for ML pipeline\n",
    "df_features = df_rfm.select(\n",
    "    \"customer_id\",\n",
    "    F.col(\"recency_days\").cast(\"double\").alias(\"recency_days\"),\n",
    "    F.col(\"frequency\").cast(\"double\").alias(\"frequency\"),\n",
    "    F.col(\"monetary_value\").cast(\"double\").alias(\"monetary_value\"),\n",
    "    F.col(\"avg_order_value\").cast(\"double\").alias(\"avg_order_value\"),\n",
    "    \"first_purchase_date\",\n",
    "    \"last_purchase_date\"\n",
    ")\n",
    "\n",
    "# Assemble feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"recency_days\", \"frequency\", \"monetary_value\"],\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "df_assembled = assembler.transform(df_features)\n",
    "\n",
    "# Standardize features (mean=0, std=1)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "print(\"\\nFeature scaling complete.\")\n",
    "print(f\"Scaled features ready: {df_scaled.count():,} customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Elbow Method for Optimal K\n",
    "\n",
    "Determine optimal number of clusters using the elbow method (WCSS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ELBOW METHOD FOR OPTIMAL K\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate Within-Cluster Sum of Squares (WCSS) for different K values\n",
    "wcss_scores = []\n",
    "silhouette_scores = []\n",
    "\n",
    "evaluator = ClusteringEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    featuresCol=\"features\",\n",
    "    metricName=\"silhouette\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTesting K from {MIN_K} to {MAX_K}...\\n\")\n",
    "\n",
    "for k in range(MIN_K, MAX_K + 1):\n",
    "    kmeans = KMeans(\n",
    "        featuresCol=\"features\",\n",
    "        predictionCol=\"prediction\",\n",
    "        k=k,\n",
    "        seed=RANDOM_SEED,\n",
    "        maxIter=20\n",
    "    )\n",
    "    \n",
    "    model = kmeans.fit(df_scaled)\n",
    "    predictions = model.transform(df_scaled)\n",
    "    \n",
    "    # WCSS (Within-Cluster Sum of Squares)\n",
    "    wcss = model.summary.trainingCost\n",
    "    \n",
    "    # Silhouette score\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    \n",
    "    wcss_scores.append((k, wcss))\n",
    "    silhouette_scores.append((k, silhouette))\n",
    "    \n",
    "    print(f\"K={k}: WCSS={wcss:,.2f}, Silhouette={silhouette:.4f}\")\n",
    "\n",
    "print(\"\\nElbow analysis complete.\")\n",
    "print(\"\\nRecommendation: Review WCSS and Silhouette scores to select optimal K.\")\n",
    "print(\"Typically K=5-7 works well for customer segmentation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Train K-means with Optimal K\n",
    "\n",
    "Based on elbow method, train K-means with optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING K-MEANS MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select optimal K (default to 6 for 6 common segments)\n",
    "# User can adjust based on elbow analysis above\n",
    "OPTIMAL_K = 6\n",
    "\n",
    "print(f\"\\nTraining K-means with K={OPTIMAL_K}...\")\n",
    "\n",
    "kmeans = KMeans(\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"cluster_id\",\n",
    "    k=OPTIMAL_K,\n",
    "    seed=RANDOM_SEED,\n",
    "    maxIter=50\n",
    ")\n",
    "\n",
    "model = kmeans.fit(df_scaled)\n",
    "df_clustered = model.transform(df_scaled)\n",
    "\n",
    "print(\"\\nModel training complete.\")\n",
    "print(f\"Final WCSS: {model.summary.trainingCost:,.2f}\")\n",
    "\n",
    "# Show cluster distribution\n",
    "print(\"\\nCluster distribution:\")\n",
    "df_clustered.groupBy(\"cluster_id\").count().orderBy(\"cluster_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Segment Profiling & Labeling\n",
    "\n",
    "Analyze cluster characteristics and assign meaningful business labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SEGMENT PROFILING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate cluster statistics\n",
    "df_cluster_profiles = (\n",
    "    df_clustered\n",
    "    .groupBy(\"cluster_id\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"customer_count\"),\n",
    "        F.mean(\"recency_days\").alias(\"avg_recency\"),\n",
    "        F.mean(\"frequency\").alias(\"avg_frequency\"),\n",
    "        F.mean(\"monetary_value\").alias(\"avg_monetary\"),\n",
    "        F.mean(\"avg_order_value\").alias(\"avg_order_value\"),\n",
    "        F.percentile_approx(\"recency_days\", 0.5).alias(\"median_recency\"),\n",
    "        F.percentile_approx(\"frequency\", 0.5).alias(\"median_frequency\"),\n",
    "        F.percentile_approx(\"monetary_value\", 0.5).alias(\"median_monetary\")\n",
    "    )\n",
    "    .orderBy(\"cluster_id\")\n",
    ")\n",
    "\n",
    "print(\"\\nCluster profiles:\")\n",
    "df_cluster_profiles.show(truncate=False)\n",
    "\n",
    "# Calculate percentiles for segment assignment\n",
    "recency_percentiles = df_clustered.approxQuantile(\"recency_days\", [0.33, 0.67], 0.05)\n",
    "frequency_percentiles = df_clustered.approxQuantile(\"frequency\", [0.33, 0.67], 0.05)\n",
    "monetary_percentiles = df_clustered.approxQuantile(\"monetary_value\", [0.33, 0.67], 0.05)\n",
    "\n",
    "print(f\"\\nRecency percentiles (33%, 67%): {recency_percentiles}\")\n",
    "print(f\"Frequency percentiles (33%, 67%): {frequency_percentiles}\")\n",
    "print(f\"Monetary percentiles (33%, 67%): {monetary_percentiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign segment labels based on RFM characteristics\n",
    "# Logic: Low Recency (recent) = Good, High Frequency = Good, High Monetary = Good\n",
    "\n",
    "def assign_segment_label(avg_recency, avg_frequency, avg_monetary,\n",
    "                        recency_p33, recency_p67,\n",
    "                        frequency_p33, frequency_p67,\n",
    "                        monetary_p33, monetary_p67):\n",
    "    \"\"\"\n",
    "    Assign segment label based on cluster characteristics.\n",
    "    \n",
    "    Segments:\n",
    "    - Champions: Low recency, high frequency, high monetary\n",
    "    - Loyal Customers: Low recency, high frequency, medium monetary\n",
    "    - Potential Loyalists: Low recency, medium frequency, medium monetary\n",
    "    - New Customers: Low recency, low frequency, low monetary\n",
    "    - At Risk: Medium recency, medium frequency, medium monetary\n",
    "    - Hibernating: High recency, low frequency, low monetary\n",
    "    - Lost: High recency, low frequency, low monetary (very high recency)\n",
    "    \"\"\"\n",
    "    # Classify each metric as Low/Medium/High\n",
    "    r_score = 3 if avg_recency < recency_p33 else (2 if avg_recency < recency_p67 else 1)\n",
    "    f_score = 3 if avg_frequency > frequency_p67 else (2 if avg_frequency > frequency_p33 else 1)\n",
    "    m_score = 3 if avg_monetary > monetary_p67 else (2 if avg_monetary > monetary_p33 else 1)\n",
    "    \n",
    "    # Assign segment based on RFM scores\n",
    "    if r_score == 3 and f_score == 3 and m_score == 3:\n",
    "        return \"Champions\"\n",
    "    elif r_score == 3 and f_score >= 2 and m_score >= 2:\n",
    "        return \"Loyal Customers\"\n",
    "    elif r_score == 3 and f_score <= 2 and m_score <= 2:\n",
    "        return \"New Customers\"\n",
    "    elif r_score == 2 and f_score >= 2 and m_score >= 2:\n",
    "        return \"Potential Loyalists\"\n",
    "    elif r_score == 2:\n",
    "        return \"At Risk\"\n",
    "    elif r_score == 1 and avg_recency > 180:\n",
    "        return \"Lost\"\n",
    "    else:\n",
    "        return \"Hibernating\"\n",
    "\n",
    "# Collect cluster profiles for labeling\n",
    "cluster_profiles_list = df_cluster_profiles.collect()\n",
    "\n",
    "# Create segment mapping\n",
    "segment_mapping = []\n",
    "for row in cluster_profiles_list:\n",
    "    cluster_id = row[\"cluster_id\"]\n",
    "    label = assign_segment_label(\n",
    "        row[\"avg_recency\"], row[\"avg_frequency\"], row[\"avg_monetary\"],\n",
    "        recency_percentiles[0], recency_percentiles[1],\n",
    "        frequency_percentiles[0], frequency_percentiles[1],\n",
    "        monetary_percentiles[0], monetary_percentiles[1]\n",
    "    )\n",
    "    segment_mapping.append((cluster_id, label))\n",
    "    print(f\"Cluster {cluster_id}: {label}\")\n",
    "\n",
    "# Create mapping DataFrame\n",
    "df_segment_mapping = spark.createDataFrame(segment_mapping, [\"cluster_id\", \"segment_label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Create Output Table\n",
    "\n",
    "Join segment labels and save to gold_customer_segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CREATING OUTPUT TABLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Join segment labels\n",
    "df_output = (\n",
    "    df_clustered\n",
    "    .join(df_segment_mapping, \"cluster_id\", \"left\")\n",
    "    .select(\n",
    "        \"customer_id\",\n",
    "        \"cluster_id\",\n",
    "        \"segment_label\",\n",
    "        \"recency_days\",\n",
    "        \"frequency\",\n",
    "        \"monetary_value\",\n",
    "        \"avg_order_value\",\n",
    "        \"first_purchase_date\",\n",
    "        \"last_purchase_date\",\n",
    "        F.lit(analysis_date).cast(\"timestamp\").alias(\"segmented_at\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save to Gold layer\n",
    "save_gold(df_output, \"gold_customer_segments\")\n",
    "\n",
    "print(\"\\nSegment summary:\")\n",
    "df_output.groupBy(\"segment_label\").agg(\n",
    "    F.count(\"*\").alias(\"customers\"),\n",
    "    F.round(F.avg(\"monetary_value\"), 2).alias(\"avg_ltv\"),\n",
    "    F.round(F.avg(\"frequency\"), 1).alias(\"avg_frequency\"),\n",
    "    F.round(F.avg(\"recency_days\"), 1).alias(\"avg_recency_days\")\n",
    ").orderBy(F.desc(\"avg_ltv\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Segment Insights & Recommendations\n",
    "\n",
    "Provide actionable insights for each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SEGMENT INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "recommendations = {\n",
    "    \"Champions\": \"Reward with VIP benefits, early access, exclusive offers. Encourage referrals.\",\n",
    "    \"Loyal Customers\": \"Upsell higher-value products, loyalty programs, personalized recommendations.\",\n",
    "    \"Potential Loyalists\": \"Nurture with engagement campaigns, increase purchase frequency incentives.\",\n",
    "    \"New Customers\": \"Onboarding programs, welcome offers, build engagement early.\",\n",
    "    \"At Risk\": \"Re-engagement campaigns, special discounts, win-back offers.\",\n",
    "    \"Hibernating\": \"Aggressive win-back campaigns, limited-time offers, surveys for feedback.\",\n",
    "    \"Lost\": \"Final retention attempt with deep discounts or surveys to understand churn.\"\n",
    "}\n",
    "\n",
    "segment_summary = df_output.groupBy(\"segment_label\").count().collect()\n",
    "\n",
    "print(\"\\n\")\n",
    "for row in segment_summary:\n",
    "    segment = row[\"segment_label\"]\n",
    "    count = row[\"count\"]\n",
    "    recommendation = recommendations.get(segment, \"No recommendation available\")\n",
    "    \n",
    "    print(f\"**{segment}** ({count:,} customers)\")\n",
    "    print(f\"  - {recommendation}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CUSTOMER SEGMENTATION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
