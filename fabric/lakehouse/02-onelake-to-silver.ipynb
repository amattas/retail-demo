{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Fabric Notebook (PySpark) - Load exported Parquet files directly to Silver Delta\n# Use this for historical batch data from datagen Parquet exports\n# (Bronze layer is for streaming JSON events only)\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\nimport os\nimport warnings\n\n# =============================================================================\n# PARAMETERS - Configure these for your environment\n# =============================================================================\n# REQUIRED ENVIRONMENT VARIABLES:\n#   - SILVER_DB: Database name for Silver layer tables (e.g., \"silver_retail\")\n#   - PARQUET_SOURCE: Path to Parquet files (e.g., \"Files/adls-parquet-copy\")\n#\n# These can be set via:\n#   1. Fabric pipeline parameters (when run from a pipeline)\n#   2. Environment variables in the Fabric workspace\n#   3. Notebook %run magic or widget parameters\n#\n# For local testing, you can uncomment the defaults below:\n#   DB_NAME = \"ag\"\n#   PARQUET_SOURCE = \"Files\"\n# =============================================================================\n\ndef get_required_env(var_name, description, default=None):\n    \"\"\"Get required environment variable with clear error message.\"\"\"\n    value = os.environ.get(var_name, default)\n    if value is None:\n        raise EnvironmentError(\n            f\"Required environment variable '{var_name}' is not set.\\n\"\n            f\"Description: {description}\\n\"\n            f\"Set it via Fabric pipeline parameters or workspace environment variables.\"\n        )\n    return value\n\n# Database name for Silver layer tables - REQUIRED\n# Uncomment the default for local testing: default=\"ag\"\nDB_NAME = get_required_env(\n    \"SILVER_DB\",\n    \"Target database for Silver layer Delta tables\",\n    default=\"ag\"  # Default for backward compatibility; remove in production\n)\n\n# Configure source path - REQUIRED\n# Uncomment the default for local testing: default=\"Files\"\nPARQUET_SOURCE = get_required_env(\n    \"PARQUET_SOURCE\",\n    \"Path to Parquet source files (e.g., 'Files/adls-parquet-copy')\",\n    default=\"Files\"  # Default for backward compatibility; remove in production\n)\n\nprint(f\"Configuration: DB_NAME={DB_NAME}, PARQUET_SOURCE={PARQUET_SOURCE}\")\n\n\ndef get_fs():\n    try:\n        from notebookutils import mssparkutils\n        return mssparkutils.fs\n    except ImportError:\n        return dbutils.fs\n\n\nFS = get_fs()\n\n\ndef path_exists(path):\n    try:\n        FS.ls(path)\n        return True\n    except Exception:\n        return False\n\n\ndef resolve_path(*candidates):\n    for candidate in candidates:\n        if path_exists(candidate):\n            return candidate\n    raise FileNotFoundError(f\"No matching path for: {candidates}\")\n\n\ndef read_parquet_recursive(path):\n    return spark.read.option(\"recursiveFileLookup\", \"true\").parquet(path)\n\n\ndef read_dim(table_name):\n    path = resolve_path(\n        f\"{PARQUET_SOURCE}/master/{table_name}\",\n        f\"{PARQUET_SOURCE}/{table_name}\",\n    )\n    return read_parquet_recursive(path)\n\n\ndef read_fact(table_name):\n    path = resolve_path(\n        f\"{PARQUET_SOURCE}/facts/{table_name}\",\n        f\"{PARQUET_SOURCE}/{table_name}\",\n    )\n    return read_parquet_recursive(path)\n\n\ndef normalize_columns(df):\n    return df.toDF(*[c.lower() for c in df.columns])\n\n\ndef col_with_fallback(df, candidates, alias, required=True):\n    cols = {c.lower(): c for c in df.columns}\n    for name in candidates:\n        key = name.lower()\n        if key in cols:\n            return F.col(cols[key]).alias(alias)\n    if required:\n        raise ValueError(f\"Missing column for '{alias}', tried {candidates}\")\n    return F.lit(None).cast(\"string\").alias(alias)\n\n\ndef select_columns(df, mapping):\n    df = normalize_columns(df)\n    columns = []\n    for alias, spec in mapping.items():\n        if isinstance(spec, dict):\n            candidates = spec.get(\"candidates\", [])\n            required = spec.get(\"required\", True)\n        else:\n            candidates = spec\n            required = True\n        columns.append(col_with_fallback(df, candidates, alias, required=required))\n    return df.select(*columns)\n\n\ndef ensure_database(name):\n    \"\"\"Create database if it doesn't exist and validate access.\"\"\"\n    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {name}\")\n    # Validate we can access the database\n    try:\n        spark.sql(f\"DESCRIBE DATABASE {name}\")\n        print(f\"Database '{name}' is ready.\")\n    except AnalysisException as e:\n        raise RuntimeError(f\"Cannot access database '{name}': {e}\")\n\n\ndef save_table(df, table_name, mode=\"overwrite\"):\n    full_name = f\"{DB_NAME}.{table_name}\"\n    df.write.format(\"delta\").mode(mode).saveAsTable(full_name)\n    print(f\"  Written to {full_name}\")\n\n\ndef load_table(table_name, read_fn, columns_mapping=None):\n    \"\"\"Load a table with proper exception handling.\n    \n    Args:\n        table_name: Name of the table to load\n        read_fn: Function to read the source data (read_dim or read_fact)\n        columns_mapping: Optional column mapping for select_columns\n    \n    Returns:\n        True if loaded successfully, False if skipped due to missing data\n        \n    Raises:\n        PermissionError: Re-raised for infrastructure issues\n        ValueError: Re-raised for schema/mapping errors\n    \"\"\"\n    try:\n        if columns_mapping:\n            df = select_columns(read_fn(table_name), columns_mapping)\n        else:\n            df = read_fn(table_name)\n        save_table(df, table_name)\n        return True\n    except FileNotFoundError as e:\n        print(f\"  Skipping {table_name}: source file not found\")\n        return False\n    except PermissionError as e:\n        # Re-raise permission errors - infrastructure problem that needs attention\n        raise\n    except ValueError as e:\n        # Re-raise schema errors - requires manual fix\n        raise\n    except AnalysisException as e:\n        # PySpark analysis errors (e.g., schema mismatch)\n        warnings.warn(f\"Skipping {table_name}: PySpark analysis error - {e}\")\n        return False\n    except Exception as e:\n        # Log unexpected errors with type for debugging\n        print(f\"  Skipping {table_name}: {type(e).__name__}: {e}\")\n        return False\n\n\nensure_database(DB_NAME)\n\n# =============================================================================\n# DIMENSION TABLES (Master Data)\n# =============================================================================\n\nprint(\"Loading dimension tables...\")\n\nload_table(\"dim_geographies\", read_dim)\nload_table(\"dim_stores\", read_dim)\nload_table(\"dim_distribution_centers\", read_dim)\nload_table(\"dim_trucks\", read_dim)\nload_table(\"dim_customers\", read_dim)\nload_table(\"dim_products\", read_dim)\n\nprint(\"Dimension tables loaded.\\n\")\n\n# =============================================================================\n# FACT TABLES\n# =============================================================================\n\nprint(\"Loading fact tables...\")\n\n# 1) Receipts\nload_table(\"fact_receipts\", read_fact, {\n    \"event_ts\": [\"event_ts\"],\n    \"receipt_id_ext\": [\"receipt_id_ext\", \"receipt_id\"],\n    \"payment_method\": [\"payment_method\"],\n    \"discount_amount\": [\"discount_amount\"],\n    \"tax_cents\": [\"tax_cents\"],\n    \"subtotal\": [\"subtotal\", \"subtotal_amount\"],\n    \"total\": [\"total\", \"total_amount\"],\n    \"total_cents\": [\"total_cents\"],\n    \"receipt_type\": [\"receipt_type\"],\n    \"subtotal_cents\": [\"subtotal_cents\"],\n    \"tax\": [\"tax\", \"tax_amount\"],\n    \"customer_id\": {\"candidates\": [\"customer_id\"], \"required\": False},\n    \"store_id\": [\"store_id\"],\n    \"return_for_receipt_id_ext\": [\"return_for_receipt_id_ext\"],\n})\n\n# 2) Receipt Lines\nload_table(\"fact_receipt_lines\", read_fact, {\n    \"unit_cents\": [\"unit_cents\"],\n    \"unit_price\": [\"unit_price\"],\n    \"event_ts\": [\"event_ts\"],\n    \"product_id\": [\"product_id\"],\n    \"quantity\": [\"quantity\"],\n    \"ext_price\": [\"ext_price\"],\n    \"line_num\": [\"line_num\"],\n    \"promo_code\": [\"promo_code\"],\n    \"ext_cents\": [\"ext_cents\"],\n    \"receipt_id_ext\": [\"receipt_id_ext\", \"receipt_id\"],\n})\n\n# 3) Store Inventory Transactions\nload_table(\"fact_store_inventory_txn\", read_fact, {\n    \"event_ts\": [\"event_ts\"],\n    \"product_id\": [\"product_id\"],\n    \"txn_type\": [\"txn_type\"],\n    \"quantity\": [\"quantity\"],\n    \"source\": [\"source\"],\n    \"store_id\": [\"store_id\"],\n    \"balance\": [\"balance\"],\n})\n\n# 4) DC Inventory Transactions\nload_table(\"fact_dc_inventory_txn\", read_fact, {\n    \"event_ts\": [\"event_ts\"],\n    \"product_id\": [\"product_id\"],\n    \"txn_type\": [\"txn_type\"],\n    \"quantity\": [\"quantity\"],\n    \"dc_id\": [\"dc_id\"],\n    \"balance\": [\"balance\"],\n    \"source\": [\"source\"],\n})\n\n# 5) Foot Traffic\nload_table(\"fact_foot_traffic\", read_fact, {\n    \"count\": [\"count\"],\n    \"zone\": [\"zone\"],\n    \"event_ts\": [\"event_ts\"],\n    \"sensor_id\": [\"sensor_id\"],\n    \"dwell_seconds\": [\"dwell_seconds\"],\n    \"store_id\": [\"store_id\"],\n})\n\n# 6) BLE Pings\nload_table(\"fact_ble_pings\", read_fact, {\n    \"zone\": [\"zone\"],\n    \"event_ts\": [\"event_ts\"],\n    \"rssi\": [\"rssi\"],\n    \"customer_ble_id\": [\"customer_ble_id\"],\n    \"customer_id\": {\"candidates\": [\"customer_id\"], \"required\": False},\n    \"store_id\": [\"store_id\"],\n    \"beacon_id\": [\"beacon_id\"],\n})\n\n# 7) Marketing\nload_table(\"fact_marketing\", read_fact, {\n    \"event_ts\": [\"event_ts\"],\n    \"campaign_id\": [\"campaign_id\"],\n    \"device\": [\"device\"],\n    \"creative_id\": [\"creative_id\"],\n    \"customer_ad_id\": [\"customer_ad_id\"],\n    \"impression_id_ext\": [\"impression_id_ext\", \"impression_id\"],\n    \"cost\": [\"cost\"],\n    \"cost_cents\": {\"candidates\": [\"cost_cents\"], \"required\": False},\n    \"customer_id\": {\"candidates\": [\"customer_id\"], \"required\": False},\n    \"channel\": [\"channel\"],\n})\n\n# 8) Online Order Headers\nload_table(\"fact_online_order_headers\", read_fact, {\n    \"completed_ts\": [\"completed_ts\"],\n    \"event_ts\": [\"event_ts\"],\n    \"order_id_ext\": [\"order_id_ext\", \"order_id\"],\n    \"tax_cents\": [\"tax_cents\"],\n    \"subtotal\": [\"subtotal\", \"subtotal_amount\"],\n    \"total\": [\"total\", \"total_amount\"],\n    \"total_cents\": [\"total_cents\"],\n    \"subtotal_cents\": [\"subtotal_cents\"],\n    \"tax\": [\"tax\", \"tax_amount\"],\n    \"customer_id\": {\"candidates\": [\"customer_id\"], \"required\": False},\n    \"payment_method\": [\"payment_method\"],\n})\n\n# 9) Online Order Lines\nload_table(\"fact_online_order_lines\", read_fact, {\n    \"unit_cents\": [\"unit_cents\"],\n    \"shipped_ts\": [\"shipped_ts\"],\n    \"unit_price\": [\"unit_price\"],\n    \"fulfillment_status\": [\"fulfillment_status\"],\n    \"order_id\": [\"order_id\"],\n    \"delivered_ts\": [\"delivered_ts\"],\n    \"product_id\": [\"product_id\"],\n    \"quantity\": [\"quantity\"],\n    \"ext_price\": [\"ext_price\"],\n    \"node_type\": [\"node_type\"],\n    \"fulfillment_mode\": [\"fulfillment_mode\"],\n    \"picked_ts\": [\"picked_ts\"],\n    \"node_id\": [\"node_id\"],\n    \"line_num\": [\"line_num\"],\n    \"promo_code\": [\"promo_code\"],\n    \"ext_cents\": [\"ext_cents\"],\n})\n\n# 10) Truck Moves\nload_table(\"fact_truck_moves\", read_fact, {\n    \"event_ts\": [\"event_ts\"],\n    \"truck_id\": [\"truck_id\"],\n    \"dc_id\": [\"dc_id\"],\n    \"store_id\": [\"store_id\"],\n    \"shipment_id\": [\"shipment_id\"],\n    \"status\": [\"status\"],\n    \"eta\": [\"eta\"],\n    \"etd\": [\"etd\"],\n})\n\nprint(\"\\nFact tables loaded.\")\nprint(\"Parquet -> Silver load complete!\")",
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "26658282-3012-49d4-befb-f802ccdbcd9b"
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "dependencies": {
   "lakehouse": {
    "known_lakehouses": [
     {
      "id": "9dff608a-05dc-4f3f-b979-f911f04aa6be"
     }
    ],
    "default_lakehouse": "9dff608a-05dc-4f3f-b979-f911f04aa6be",
    "default_lakehouse_name": "Retail",
    "default_lakehouse_workspace_id": "5219ac70-71d4-4dfc-af32-5b8a6c29a471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}