{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fabric Notebook (PySpark) - Bronze to Silver Transformation\n",
    "\n",
    "Reads from Bronze layer shortcuts (cusn schema) and transforms to Silver Delta tables (ag schema).\n",
    "\n",
    "## Data Sources\n",
    "- **Bronze Batch Data** (`cusn.dim_*`, `cusn.fact_*`): ADLSv2 parquet shortcuts (historical)\n",
    "- **Bronze Streaming Data** (`cusn.receipt_created`, etc.): Eventhouse shortcuts (real-time)\n",
    "\n",
    "## Outputs\n",
    "- **Silver Delta Tables** (`ag.dim_*`, `ag.fact_*`): Combined batch + streaming, validated, transformed\n",
    "\n",
    "## Processing Logic\n",
    "1. Dimensions: Load from batch parquet only\n",
    "2. Facts: UNION batch parquet + streaming events (no overlap in demo environment)\n",
    "3. Schema alignment: Map streaming event fields to fact table schema\n",
    "4. Validation: Ensure data quality (no nulls in required FKs)\n",
    "\n",
    "**Note**: User confirmed batch and streaming data never overlap, so simple UNION ALL without deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARAMETERS - Configure these for your environment\n",
    "# =============================================================================\n",
    "# REQUIRED ENVIRONMENT VARIABLES:\n",
    "#   - SILVER_DB: Database name for Silver layer Delta tables (default: \"ag\")\n",
    "#   - BRONZE_SCHEMA: Bronze schema with shortcuts to ADLSv2 + Eventhouse (default: \"cusn\")\n",
    "#\n",
    "# These can be set via:\n",
    "#   1. Fabric pipeline parameters (when run from a pipeline)\n",
    "#   2. Environment variables in the Fabric workspace\n",
    "#   3. Notebook %run magic or widget parameters\n",
    "#\n",
    "# For local testing, you can use the defaults below:\n",
    "#   SILVER_DB = \"ag\"\n",
    "#   BRONZE_SCHEMA = \"cusn\"\n",
    "# =============================================================================\n",
    "\n",
    "def get_required_env(var_name, description, default=None):\n",
    "    \"\"\"Get required environment variable with clear error message.\"\"\"\n",
    "    value = os.environ.get(var_name, default)\n",
    "    if value is None:\n",
    "        raise EnvironmentError(\n",
    "            f\"Required environment variable '{var_name}' is not set.\\n\"\n",
    "            f\"Description: {description}\\n\"\n",
    "            f\"Set it via Fabric pipeline parameters or workspace environment variables.\"\n",
    "        )\n",
    "    return value\n",
    "\n",
    "# Database name for Silver layer tables\n",
    "DB_NAME = get_required_env(\n",
    "    \"SILVER_DB\",\n",
    "    \"Target database for Silver layer Delta tables\",\n",
    "    default=\"ag\"\n",
    ")\n",
    "\n",
    "# Bronze schema name (contains shortcuts to ADLSv2 parquet + Eventhouse events)\n",
    "BRONZE_SCHEMA = get_required_env(\n",
    "    \"BRONZE_SCHEMA\",\n",
    "    \"Bronze schema with shortcuts to batch and streaming data sources\",\n",
    "    default=\"cusn\"\n",
    ")\n",
    "\n",
    "print(f\"Configuration: SILVER_DB={DB_NAME}, BRONZE_SCHEMA={BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def read_bronze_table(table_name):\n",
    "    \"\"\"Read a table from Bronze schema.\"\"\"\n",
    "    return spark.table(f\"{BRONZE_SCHEMA}.{table_name}\")\n",
    "\n",
    "def ensure_database(name):\n",
    "    \"\"\"Create database if it doesn't exist and validate access.\"\"\"\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {name}\")\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE DATABASE {name}\")\n",
    "        print(f\"Database '{name}' is ready.\")\n",
    "    except AnalysisException as e:\n",
    "        raise RuntimeError(f\"Cannot access database '{name}': {e}\")\n",
    "\n",
    "def save_table(df, table_name, mode=\"overwrite\"):\n",
    "    full_name = f\"{DB_NAME}.{table_name}\"\n",
    "    df.write.format(\"delta\").mode(mode).saveAsTable(full_name)\n",
    "    print(f\"  Written to {full_name}: {df.count()} rows\")\n",
    "\n",
    "def bronze_table_exists(table_name):\n",
    "    \"\"\"Check if a table exists in Bronze schema.\"\"\"\n",
    "    try:\n",
    "        spark.table(f\"{BRONZE_SCHEMA}.{table_name}\")\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "def load_table(table_name, transform_fn=None, skip_if_missing=True):\n",
    "    \"\"\"\n",
    "    Load a table from Bronze to Silver with optional transformation.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table to load\n",
    "        transform_fn: Optional function to transform the DataFrame\n",
    "        skip_if_missing: If True, skip if Bronze table doesn't exist\n",
    "        \n",
    "    Returns:\n",
    "        True if loaded successfully, False if skipped\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading {table_name}...\")\n",
    "        \n",
    "        # Check if Bronze table exists\n",
    "        if not bronze_table_exists(table_name):\n",
    "            if skip_if_missing:\n",
    "                print(f\"  Skipping {table_name}: Bronze table does not exist\")\n",
    "                return False\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Required Bronze table {BRONZE_SCHEMA}.{table_name} not found\")\n",
    "        \n",
    "        # Read from Bronze\n",
    "        df = read_bronze_table(table_name)\n",
    "        \n",
    "        # Apply transformation if provided\n",
    "        if transform_fn:\n",
    "            df = transform_fn(df)\n",
    "        \n",
    "        # Save to Silver\n",
    "        save_table(df, table_name)\n",
    "        return True\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  Skipping {table_name}: {e}\")\n",
    "        return False\n",
    "    except PermissionError as e:\n",
    "        # Re-raise permission errors - infrastructure problem\n",
    "        raise\n",
    "    except AnalysisException as e:\n",
    "        # PySpark analysis errors (e.g., schema mismatch)\n",
    "        warnings.warn(f\"Skipping {table_name}: PySpark analysis error - {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        # Log unexpected errors with type for debugging\n",
    "        print(f\"  Skipping {table_name}: {type(e).__name__}: {e}\")\n",
    "        return False\n",
    "\n",
    "ensure_database(DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dimension Tables (Batch Only)\n",
    "\n",
    "Dimensions are loaded from ADLSv2 parquet only (no streaming component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dimension tables...\\n\")\n",
    "\n",
    "load_table(\"dim_geographies\")\n",
    "load_table(\"dim_stores\")\n",
    "load_table(\"dim_distribution_centers\")\n",
    "load_table(\"dim_trucks\")\n",
    "load_table(\"dim_customers\")\n",
    "load_table(\"dim_products\")\n",
    "\n",
    "print(\"\\nDimension tables loaded.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fact Tables (Batch + Streaming)\n",
    "\n",
    "Fact tables combine:\n",
    "- **Batch historical data** from ADLSv2 parquet (cusn.fact_*)\n",
    "- **Streaming real-time data** from Eventhouse (cusn.receipt_created, etc.)\n",
    "\n",
    "### Mapping: Batch Tables → Streaming Events\n",
    "\n",
    "| Batch Parquet Table | Streaming Event Table | Combined in Silver |\n",
    "|---------------------|----------------------|-------------------|\n",
    "| `fact_receipts` | `receipt_created` | `ag.fact_receipts` |\n",
    "| `fact_receipt_lines` | `receipt_line_added` | `ag.fact_receipt_lines` |\n",
    "| `fact_payments` | `payment_processed` | `ag.fact_payments` |\n",
    "| `fact_store_inventory_txn`, `fact_dc_inventory_txn` | `inventory_updated` | `ag.fact_store_inventory_txn`, `ag.fact_dc_inventory_txn` |\n",
    "| `fact_stockouts` | `stockout_detected` | `ag.fact_stockouts` |\n",
    "| `fact_reorders` | `reorder_triggered` | `ag.fact_reorders` |\n",
    "| `fact_foot_traffic` | `customer_entered` | `ag.fact_foot_traffic` |\n",
    "| `fact_customer_zone_changes` | `customer_zone_changed` | `ag.fact_customer_zone_changes` |\n",
    "| `fact_ble_pings` | `ble_ping_detected` | `ag.fact_ble_pings` |\n",
    "| `fact_truck_moves` | `truck_arrived`, `truck_departed` | `ag.fact_truck_moves` |\n",
    "| `fact_store_ops` | `store_opened`, `store_closed` | `ag.fact_store_ops` |\n",
    "| `fact_marketing` | `ad_impression` | `ag.fact_marketing` |\n",
    "| `fact_promotions` | `promotion_applied` | `ag.fact_promotions` |\n",
    "| `fact_online_order_headers` | `online_order_created` | `ag.fact_online_order_headers` |\n",
    "| `fact_online_order_lines` | `online_order_picked`, `online_order_shipped` | `ag.fact_online_order_lines` |\n",
    "\n",
    "**Note:** No deduplication needed - user confirmed batch and streaming never overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading fact tables (all 18)...\\n\")\n",
    "\n",
    "# Fact tables without streaming equivalent (batch only)\n",
    "load_table(\"fact_truck_inventory\")  # No streaming event for truck inventory\n",
    "load_table(\"fact_promo_lines\")  # Promotion line details (no separate streaming event)\n",
    "\n",
    "# Fact tables with streaming equivalent - simple UNION ALL\n",
    "# These will be handled with custom transform functions to combine batch + streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. fact_receipts (Batch + Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def transform_receipts(df_batch):\n    \"\"\"Combine batch fact_receipts with streaming receipt_created.\"\"\"\n    try:\n        # Try to read streaming data\n        df_stream = read_bronze_table(\"receipt_created\")\n        \n        # Map streaming fields to batch schema\n        df_stream_mapped = df_stream.select(\n            F.col(\"ingest_timestamp\").alias(\"event_ts\"),\n            F.col(\"receipt_id\").alias(\"receipt_id_ext\"),\n            F.col(\"tender_type\").alias(\"payment_method\"),\n            F.lit(0.0).alias(\"discount_amount\"),\n            F.col(\"tax\"),\n            (F.col(\"tax\") * 100).cast(\"bigint\").alias(\"tax_cents\"),\n            F.col(\"subtotal\"),\n            F.col(\"total\"),\n            (F.col(\"total\") * 100).cast(\"bigint\").alias(\"total_cents\"),\n            F.lit(\"SALE\").alias(\"receipt_type\"),\n            (F.col(\"subtotal\") * 100).cast(\"bigint\").alias(\"subtotal_cents\"),\n            F.col(\"customer_id\"),\n            F.col(\"store_id\"),\n            F.lit(None).cast(\"string\").alias(\"return_for_receipt_id_ext\")\n        )\n        \n        # Validate schemas match before UNION\n        batch_cols = set(df_batch.columns)\n        stream_cols = set(df_stream_mapped.columns)\n        if batch_cols != stream_cols:\n            missing_in_stream = batch_cols - stream_cols\n            extra_in_stream = stream_cols - batch_cols\n            error_msg = f\"Schema mismatch for fact_receipts: \"\n            if missing_in_stream:\n                error_msg += f\"Missing in stream: {missing_in_stream}. \"\n            if extra_in_stream:\n                error_msg += f\"Extra in stream: {extra_in_stream}.\"\n            raise ValueError(error_msg)\n        \n        # UNION batch + streaming (no dedup needed)\n        return df_batch.unionAll(df_stream_mapped)\n    except Exception as e:\n        # If streaming table doesn't exist or schema mismatch, return batch only\n        print(f\"  Warning: Could not combine streaming data for fact_receipts: {e}\")\n        return df_batch\n\nload_table(\"fact_receipts\", transform_receipts)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. fact_receipt_lines (Batch + Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def transform_receipt_lines(df_batch):\n    \"\"\"Combine batch fact_receipt_lines with streaming receipt_line_added.\"\"\"\n    try:\n        df_stream = read_bronze_table(\"receipt_line_added\")\n        \n        # Map streaming fields to batch schema\n        # CRITICAL: Column order must match batch schema exactly:\n        # receipt_id_ext, event_ts, product_id, line_num, quantity, \n        # unit_price, ext_price, unit_cents, ext_cents, promo_code\n        df_stream_mapped = df_stream.select(\n            F.col(\"receipt_id\").alias(\"receipt_id_ext\"),\n            F.col(\"ingest_timestamp\").alias(\"event_ts\"),\n            F.col(\"product_id\"),\n            F.col(\"line_number\").alias(\"line_num\"),\n            F.col(\"quantity\"),\n            F.col(\"unit_price\"),\n            F.col(\"extended_price\").alias(\"ext_price\"),\n            (F.col(\"unit_price\") * 100).cast(\"bigint\").alias(\"unit_cents\"),\n            (F.col(\"extended_price\") * 100).cast(\"bigint\").alias(\"ext_cents\"),\n            F.col(\"promo_code\")\n        )\n        \n        # Validate schemas match before UNION\n        batch_cols = set(df_batch.columns)\n        stream_cols = set(df_stream_mapped.columns)\n        if batch_cols != stream_cols:\n            missing_in_stream = batch_cols - stream_cols\n            extra_in_stream = stream_cols - batch_cols\n            error_msg = f\"Schema mismatch for fact_receipt_lines: \"\n            if missing_in_stream:\n                error_msg += f\"Missing in stream: {missing_in_stream}. \"\n            if extra_in_stream:\n                error_msg += f\"Extra in stream: {extra_in_stream}.\"\n            raise ValueError(error_msg)\n        \n        return df_batch.unionAll(df_stream_mapped)\n    except Exception as e:\n        print(f\"  Warning: Could not combine streaming data for fact_receipt_lines: {e}\")\n        return df_batch\n\nload_table(\"fact_receipt_lines\", transform_receipt_lines)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-11. Remaining Fact Tables with Batch Only\n",
    "\n",
    "These tables currently only have batch data (streaming events planned but not yet implemented)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store and DC inventory transactions\n",
    "load_table(\"fact_store_inventory_txn\")\n",
    "load_table(\"fact_dc_inventory_txn\")\n",
    "\n",
    "# Logistics\n",
    "load_table(\"fact_truck_moves\")\n",
    "\n",
    "# Customer tracking\n",
    "load_table(\"fact_foot_traffic\")\n",
    "load_table(\"fact_ble_pings\")\n",
    "load_table(\"fact_customer_zone_changes\")\n",
    "\n",
    "# Marketing\n",
    "load_table(\"fact_marketing\")\n",
    "\n",
    "# Omnichannel\n",
    "load_table(\"fact_online_order_headers\")\n",
    "load_table(\"fact_online_order_lines\")\n",
    "\n",
    "# Payments\n",
    "load_table(\"fact_payments\")\n",
    "\n",
    "# Store operations\n",
    "load_table(\"fact_store_ops\")\n",
    "\n",
    "# Inventory management\n",
    "load_table(\"fact_stockouts\")\n",
    "load_table(\"fact_promotions\")\n",
    "load_table(\"fact_reorders\")\n",
    "\n",
    "print(\"\\nFact tables loaded.\")\n",
    "print(\"Bronze → Silver transformation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Verify Silver tables were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in Silver database\n",
    "silver_tables = spark.sql(f\"SHOW TABLES IN {DB_NAME}\").collect()\n",
    "\n",
    "print(f\"\\nSilver Database ({DB_NAME}) Tables:\")\n",
    "print(f\"  Total: {len(silver_tables)} tables\\n\")\n",
    "\n",
    "dim_count = sum(1 for t in silver_tables if t.tableName.startswith('dim_'))\n",
    "fact_count = sum(1 for t in silver_tables if t.tableName.startswith('fact_'))\n",
    "\n",
    "print(f\"  Dimensions: {dim_count} (expected: 6)\")\n",
    "print(f\"  Facts: {fact_count} (expected: 18)\")\n",
    "print(f\"\\n  Target: 24 tables (6 dims + 18 facts)\")\n",
    "\n",
    "if len(silver_tables) == 24:\n",
    "    print(f\"\\n✓ Silver layer complete!\")\n",
    "elif len(silver_tables) < 24:\n",
    "    print(f\"\\n⚠ Silver layer has {len(silver_tables)}/24 tables\")\n",
    "    print(f\"  Some Bronze tables may be missing - check Bronze layer creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test read from Silver\n",
    "print(\"\\nTesting Silver layer access...\\n\")\n",
    "\n",
    "try:\n",
    "    df_receipts = spark.table(f\"{DB_NAME}.fact_receipts\")\n",
    "    print(f\"✓ fact_receipts: {df_receipts.count()} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ fact_receipts: {e}\")\n",
    "\n",
    "try:\n",
    "    df_receipt_lines = spark.table(f\"{DB_NAME}.fact_receipt_lines\")\n",
    "    print(f\"✓ fact_receipt_lines: {df_receipt_lines.count()} rows\")\n",
    "    print(f\"\\nfact_receipt_lines schema:\")\n",
    "    df_receipt_lines.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"✗ fact_receipt_lines: {e}\")\n",
    "\n",
    "print(\"\\nSilver layer ready for Gold aggregations!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}