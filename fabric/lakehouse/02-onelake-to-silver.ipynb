{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fabric Notebook (PySpark) - Bronze to Silver Transformation\n",
    "\n",
    "Reads from Bronze layer shortcuts (cusn schema) and transforms to Silver Delta tables (ag schema).\n",
    "\n",
    "## Data Sources\n",
    "- **Bronze Batch Data** (`cusn.dim_*`, `cusn.fact_*`): ADLSv2 parquet shortcuts (historical)\n",
    "- **Bronze Streaming Data** (`cusn.receipt_created`, etc.): Eventhouse shortcuts (real-time)\n",
    "\n",
    "## Outputs\n",
    "- **Silver Delta Tables** (`ag.dim_*`, `ag.fact_*`): Combined batch + streaming, validated, transformed\n",
    "\n",
    "## Processing Logic\n",
    "1. Dimensions: Load from batch parquet only\n",
    "2. Facts: UNION batch parquet + streaming events (no overlap in demo environment)\n",
    "3. Schema alignment: Map streaming event fields to fact table schema\n",
    "4. Validation: Ensure data quality (no nulls in required FKs)\n",
    "\n",
    "**Note**: User confirmed batch and streaming data never overlap, so simple UNION ALL without deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.utils import AnalysisException\nimport os\nimport warnings\n\n# =============================================================================\n# CONSTANTS - Default values for streaming data mapping\n# =============================================================================\nDEFAULT_RECEIPT_TYPE = \"SALE\"\nDEFAULT_DISCOUNT_AMOUNT = 0.0\nDEFAULT_RETURN_FOR_RECEIPT_ID = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# PARAMETERS - Configure these for your environment\n# =============================================================================\n# REQUIRED ENVIRONMENT VARIABLES:\n#   - SILVER_DB: Database name for Silver layer Delta tables (default: \"ag\")\n#   - BRONZE_SCHEMA: Bronze schema with shortcuts to ADLSv2 + Eventhouse (default: \"cusn\")\n#   - FAIL_ON_SCHEMA_MISMATCH: Fail instead of falling back to batch-only (default: \"false\")\n#\n# These can be set via:\n#   1. Fabric pipeline parameters (when run from a pipeline)\n#   2. Environment variables in the Fabric workspace\n#   3. Notebook %run magic or widget parameters\n#\n# For local testing, you can use the defaults below:\n#   SILVER_DB = \"ag\"\n#   BRONZE_SCHEMA = \"cusn\"\n#   FAIL_ON_SCHEMA_MISMATCH = \"false\"\n# =============================================================================\n\ndef get_required_env(var_name, description, default=None):\n    \"\"\"Get required environment variable with clear error message.\"\"\"\n    value = os.environ.get(var_name, default)\n    if value is None:\n        raise EnvironmentError(\n            f\"Required environment variable '{var_name}' is not set.\\n\"\n            f\"Description: {description}\\n\"\n            f\"Set it via Fabric pipeline parameters or workspace environment variables.\"\n        )\n    return value\n\n# Database name for Silver layer tables\nDB_NAME = get_required_env(\n    \"SILVER_DB\",\n    \"Target database for Silver layer Delta tables\",\n    default=\"ag\"\n)\n\n# Bronze schema name (contains shortcuts to ADLSv2 parquet + Eventhouse events)\nBRONZE_SCHEMA = get_required_env(\n    \"BRONZE_SCHEMA\",\n    \"Bronze schema with shortcuts to batch and streaming data sources\",\n    default=\"cusn\"\n)\n\n# Deployment Mode: Set FAIL_ON_SCHEMA_MISMATCH=true for production\n# In production, schema mismatches should fail loudly rather than silently falling back\nFAIL_ON_SCHEMA_MISMATCH = os.environ.get(\"FAIL_ON_SCHEMA_MISMATCH\", \"false\").lower() == \"true\"\n\n# Schema mismatch counter for metrics\nschema_mismatch_count = 0\n\nprint(f\"Configuration: SILVER_DB={DB_NAME}, BRONZE_SCHEMA={BRONZE_SCHEMA}\")\nprint(f\"Deployment Mode: FAIL_ON_SCHEMA_MISMATCH={FAIL_ON_SCHEMA_MISMATCH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def read_bronze_table(table_name):\n",
    "    \"\"\"Read a table from Bronze schema.\"\"\"\n",
    "    return spark.table(f\"{BRONZE_SCHEMA}.{table_name}\")\n",
    "\n",
    "def ensure_database(name):\n",
    "    \"\"\"Create database if it doesn't exist and validate access.\"\"\"\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {name}\")\n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE DATABASE {name}\")\n",
    "        print(f\"Database '{name}' is ready.\")\n",
    "    except AnalysisException as e:\n",
    "        raise RuntimeError(f\"Cannot access database '{name}': {e}\")\n",
    "\n",
    "def save_table(df, table_name, mode=\"overwrite\"):\n",
    "    full_name = f\"{DB_NAME}.{table_name}\"\n",
    "    df.write.format(\"delta\").mode(mode).saveAsTable(full_name)\n",
    "    print(f\"  Written to {full_name}: {df.count()} rows\")\n",
    "\n",
    "def bronze_table_exists(table_name):\n",
    "    \"\"\"Check if a table exists in Bronze schema.\"\"\"\n",
    "    try:\n",
    "        spark.table(f\"{BRONZE_SCHEMA}.{table_name}\")\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "def load_table(table_name, transform_fn=None, skip_if_missing=True):\n",
    "    \"\"\"\n",
    "    Load a table from Bronze to Silver with optional transformation.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table to load\n",
    "        transform_fn: Optional function to transform the DataFrame\n",
    "        skip_if_missing: If True, skip if Bronze table doesn't exist\n",
    "        \n",
    "    Returns:\n",
    "        True if loaded successfully, False if skipped\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading {table_name}...\")\n",
    "        \n",
    "        # Check if Bronze table exists\n",
    "        if not bronze_table_exists(table_name):\n",
    "            if skip_if_missing:\n",
    "                print(f\"  Skipping {table_name}: Bronze table does not exist\")\n",
    "                return False\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Required Bronze table {BRONZE_SCHEMA}.{table_name} not found\")\n",
    "        \n",
    "        # Read from Bronze\n",
    "        df = read_bronze_table(table_name)\n",
    "        \n",
    "        # Apply transformation if provided\n",
    "        if transform_fn:\n",
    "            df = transform_fn(df)\n",
    "        \n",
    "        # Save to Silver\n",
    "        save_table(df, table_name)\n",
    "        return True\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  Skipping {table_name}: {e}\")\n",
    "        return False\n",
    "    except PermissionError as e:\n",
    "        # Re-raise permission errors - infrastructure problem\n",
    "        raise\n",
    "    except AnalysisException as e:\n",
    "        # PySpark analysis errors (e.g., schema mismatch)\n",
    "        warnings.warn(f\"Skipping {table_name}: PySpark analysis error - {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        # Log unexpected errors with type for debugging\n",
    "        print(f\"  Skipping {table_name}: {type(e).__name__}: {e}\")\n",
    "        return False\n",
    "\n",
    "ensure_database(DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dimension Tables (Batch Only)\n",
    "\n",
    "Dimensions are loaded from ADLSv2 parquet only (no streaming component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dimension tables...\\n\")\n",
    "\n",
    "load_table(\"dim_geographies\")\n",
    "load_table(\"dim_stores\")\n",
    "load_table(\"dim_distribution_centers\")\n",
    "load_table(\"dim_trucks\")\n",
    "load_table(\"dim_customers\")\n",
    "load_table(\"dim_products\")\n",
    "\n",
    "print(\"\\nDimension tables loaded.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fact Tables (Batch + Streaming)\n",
    "\n",
    "Fact tables combine:\n",
    "- **Batch historical data** from ADLSv2 parquet (cusn.fact_*)\n",
    "- **Streaming real-time data** from Eventhouse (cusn.receipt_created, etc.)\n",
    "\n",
    "### Mapping: Batch Tables → Streaming Events\n",
    "\n",
    "| Batch Parquet Table | Streaming Event Table | Combined in Silver |\n",
    "|---------------------|----------------------|-------------------|\n",
    "| `fact_receipts` | `receipt_created` | `ag.fact_receipts` |\n",
    "| `fact_receipt_lines` | `receipt_line_added` | `ag.fact_receipt_lines` |\n",
    "| `fact_payments` | `payment_processed` | `ag.fact_payments` |\n",
    "| `fact_store_inventory_txn`, `fact_dc_inventory_txn` | `inventory_updated` | `ag.fact_store_inventory_txn`, `ag.fact_dc_inventory_txn` |\n",
    "| `fact_stockouts` | `stockout_detected` | `ag.fact_stockouts` |\n",
    "| `fact_reorders` | `reorder_triggered` | `ag.fact_reorders` |\n",
    "| `fact_foot_traffic` | `customer_entered` | `ag.fact_foot_traffic` |\n",
    "| `fact_customer_zone_changes` | `customer_zone_changed` | `ag.fact_customer_zone_changes` |\n",
    "| `fact_ble_pings` | `ble_ping_detected` | `ag.fact_ble_pings` |\n",
    "| `fact_truck_moves` | `truck_arrived`, `truck_departed` | `ag.fact_truck_moves` |\n",
    "| `fact_store_ops` | `store_opened`, `store_closed` | `ag.fact_store_ops` |\n",
    "| `fact_marketing` | `ad_impression` | `ag.fact_marketing` |\n",
    "| `fact_promotions` | `promotion_applied` | `ag.fact_promotions` |\n",
    "| `fact_online_order_headers` | `online_order_created` | `ag.fact_online_order_headers` |\n",
    "| `fact_online_order_lines` | `online_order_picked`, `online_order_shipped` | `ag.fact_online_order_lines` |\n",
    "\n",
    "**Note:** No deduplication needed - user confirmed batch and streaming never overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading fact tables (all 18)...\\n\")\n",
    "\n",
    "# Fact tables without streaming equivalent (batch only)\n",
    "load_table(\"fact_truck_inventory\")  # No streaming event for truck inventory\n",
    "load_table(\"fact_promo_lines\")  # Promotion line details (no separate streaming event)\n",
    "\n",
    "# Fact tables with streaming equivalent - simple UNION ALL\n",
    "# These will be handled with custom transform functions to combine batch + streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. fact_receipts (Batch + Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def transform_receipts(df_batch):\n    \"\"\"Combine batch fact_receipts with streaming receipt_created.\"\"\"\n    global schema_mismatch_count\n    try:\n        # Try to read streaming data\n        df_stream = read_bronze_table(\"receipt_created\")\n        \n        # Map streaming fields to batch schema\n        # IMPORTANT: Use ROUND for cents to avoid truncation errors\n        df_stream_mapped = df_stream.select(\n            F.col(\"ingest_timestamp\").alias(\"event_ts\"),\n            F.col(\"receipt_id\").alias(\"receipt_id_ext\"),\n            F.col(\"tender_type\").alias(\"payment_method\"),\n            F.lit(DEFAULT_DISCOUNT_AMOUNT).alias(\"discount_amount\"),\n            F.col(\"tax\"),\n            F.round(F.col(\"tax\") * 100).cast(\"bigint\").alias(\"tax_cents\"),\n            F.col(\"subtotal\"),\n            F.col(\"total\"),\n            F.round(F.col(\"total\") * 100).cast(\"bigint\").alias(\"total_cents\"),\n            F.lit(DEFAULT_RECEIPT_TYPE).alias(\"receipt_type\"),\n            F.round(F.col(\"subtotal\") * 100).cast(\"bigint\").alias(\"subtotal_cents\"),\n            F.col(\"customer_id\"),\n            F.col(\"store_id\"),\n            F.lit(DEFAULT_RETURN_FOR_RECEIPT_ID).cast(\"string\").alias(\"return_for_receipt_id_ext\")\n        )\n        \n        # Validate schemas match before UNION\n        batch_cols = set(df_batch.columns)\n        stream_cols = set(df_stream_mapped.columns)\n        if batch_cols != stream_cols:\n            missing_in_stream = batch_cols - stream_cols\n            extra_in_stream = stream_cols - batch_cols\n            error_msg = f\"Schema mismatch for fact_receipts: \"\n            if missing_in_stream:\n                error_msg += f\"Missing in stream: {missing_in_stream}. \"\n            if extra_in_stream:\n                error_msg += f\"Extra in stream: {extra_in_stream}.\"\n            \n            # Log detailed mismatch information\n            print(f\"⚠️  SCHEMA MISMATCH DETECTED: {error_msg}\")\n            print(f\"   Batch columns: {sorted(batch_cols)}\")\n            print(f\"   Stream columns: {sorted(stream_cols)}\")\n            schema_mismatch_count += 1\n            \n            if FAIL_ON_SCHEMA_MISMATCH:\n                raise ValueError(f\"Production failure - {error_msg}\")\n            else:\n                print(f\"   Falling back to batch-only (FAIL_ON_SCHEMA_MISMATCH=false)\")\n                return df_batch\n        \n        # UNION batch + streaming (no dedup needed)\n        # Using union() for name-based column matching (safer than position-based unionAll())\n        print(f\"  ✓ Successfully combined batch + streaming for fact_receipts\")\n        return df_batch.union(df_stream_mapped)\n    except Exception as e:\n        # If streaming table doesn't exist or other error, return batch only\n        if \"does not exist\" in str(e).lower():\n            print(f\"  ℹ️  Streaming table not found for fact_receipts, using batch only\")\n        else:\n            print(f\"  ⚠️  Could not combine streaming data for fact_receipts: {e}\")\n            if FAIL_ON_SCHEMA_MISMATCH and \"schema mismatch\" in str(e).lower():\n                raise  # Re-raise schema mismatch in production mode\n        return df_batch\n\nload_table(\"fact_receipts\", transform_receipts)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. fact_receipt_lines (Batch + Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def transform_receipt_lines(df_batch):\n    \"\"\"Combine batch fact_receipt_lines with streaming receipt_line_added.\"\"\"\n    global schema_mismatch_count\n    try:\n        df_stream = read_bronze_table(\"receipt_line_added\")\n        \n        # Map streaming fields to batch schema\n        # IMPORTANT: Use ROUND for cents to avoid truncation errors\n        df_stream_mapped = df_stream.select(\n            F.col(\"receipt_id\").alias(\"receipt_id_ext\"),\n            F.col(\"ingest_timestamp\").alias(\"event_ts\"),\n            F.col(\"product_id\"),\n            F.col(\"line_number\").alias(\"line_num\"),\n            F.col(\"quantity\"),\n            F.col(\"unit_price\"),\n            F.col(\"extended_price\").alias(\"ext_price\"),\n            F.round(F.col(\"unit_price\") * 100).cast(\"bigint\").alias(\"unit_cents\"),\n            F.round(F.col(\"extended_price\") * 100).cast(\"bigint\").alias(\"ext_cents\"),\n            F.col(\"promo_code\")\n        )\n        \n        # Validate schemas match before UNION\n        batch_cols = set(df_batch.columns)\n        stream_cols = set(df_stream_mapped.columns)\n        if batch_cols != stream_cols:\n            missing_in_stream = batch_cols - stream_cols\n            extra_in_stream = stream_cols - batch_cols\n            error_msg = f\"Schema mismatch for fact_receipt_lines: \"\n            if missing_in_stream:\n                error_msg += f\"Missing in stream: {missing_in_stream}. \"\n            if extra_in_stream:\n                error_msg += f\"Extra in stream: {extra_in_stream}.\"\n            \n            # Log detailed mismatch information\n            print(f\"⚠️  SCHEMA MISMATCH DETECTED: {error_msg}\")\n            print(f\"   Batch columns: {sorted(batch_cols)}\")\n            print(f\"   Stream columns: {sorted(stream_cols)}\")\n            schema_mismatch_count += 1\n            \n            if FAIL_ON_SCHEMA_MISMATCH:\n                raise ValueError(f\"Production failure - {error_msg}\")\n            else:\n                print(f\"   Falling back to batch-only (FAIL_ON_SCHEMA_MISMATCH=false)\")\n                return df_batch\n        \n        # UNION batch + streaming (no dedup needed)\n        # Using union() for name-based column matching (safer than position-based unionAll())\n        print(f\"  ✓ Successfully combined batch + streaming for fact_receipt_lines\")\n        return df_batch.union(df_stream_mapped)\n    except Exception as e:\n        if \"does not exist\" in str(e).lower():\n            print(f\"  ℹ️  Streaming table not found for fact_receipt_lines, using batch only\")\n        else:\n            print(f\"  ⚠️  Could not combine streaming data for fact_receipt_lines: {e}\")\n            if FAIL_ON_SCHEMA_MISMATCH and \"schema mismatch\" in str(e).lower():\n                raise  # Re-raise schema mismatch in production mode\n        return df_batch\n\nload_table(\"fact_receipt_lines\", transform_receipt_lines)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-11. Remaining Fact Tables with Batch Only\n",
    "\n",
    "These tables currently only have batch data (streaming events planned but not yet implemented)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store and DC inventory transactions\n",
    "load_table(\"fact_store_inventory_txn\")\n",
    "load_table(\"fact_dc_inventory_txn\")\n",
    "\n",
    "# Logistics\n",
    "load_table(\"fact_truck_moves\")\n",
    "\n",
    "# Customer tracking\n",
    "load_table(\"fact_foot_traffic\")\n",
    "load_table(\"fact_ble_pings\")\n",
    "load_table(\"fact_customer_zone_changes\")\n",
    "\n",
    "# Marketing\n",
    "load_table(\"fact_marketing\")\n",
    "\n",
    "# Omnichannel\n",
    "load_table(\"fact_online_order_headers\")\n",
    "load_table(\"fact_online_order_lines\")\n",
    "\n",
    "# Payments\n",
    "load_table(\"fact_payments\")\n",
    "\n",
    "# Store operations\n",
    "load_table(\"fact_store_ops\")\n",
    "\n",
    "# Inventory management\n",
    "load_table(\"fact_stockouts\")\n",
    "load_table(\"fact_promotions\")\n",
    "load_table(\"fact_reorders\")\n",
    "\n",
    "print(\"\\nFact tables loaded.\")\n",
    "print(\"Bronze → Silver transformation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# BRONZE LAYER VALIDATION (Optional)\n# =============================================================================\n# Uncomment the line below to verify Bronze shortcuts before Silver transformation\n# This will run the validation script to ensure all 42 Bronze shortcuts exist\n# %run ./05-validate-bronze-shortcuts.ipynb",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DATA QUALITY VALIDATION\n# =============================================================================\n\ndef validate_fact_table(table_name, expected_min_rows=0):\n    \"\"\"\n    Validate Silver fact table data quality.\n    \n    Args:\n        table_name: Name of the table to validate\n        expected_min_rows: Minimum expected row count (0 = no minimum check)\n        \n    Returns:\n        True if validation passed, False otherwise\n    \"\"\"\n    try:\n        df = spark.table(f\"{DB_NAME}.{table_name}\")\n        count = df.count()\n        \n        if count < expected_min_rows:\n            print(f\"  ⚠️  {table_name}: Only {count:,} rows (expected >= {expected_min_rows:,})\")\n            return False\n        \n        if count == 0:\n            print(f\"  ⚠️  {table_name}: Empty table\")\n            return False\n        \n        print(f\"  ✓ {table_name}: {count:,} rows\")\n        return True\n    except Exception as e:\n        print(f\"  ✗ {table_name}: {e}\")\n        return False\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"DATA QUALITY VALIDATION\")\nprint(\"=\"*70)\n\n# Validate key fact tables\nvalidate_fact_table(\"fact_receipts\", expected_min_rows=1)\nvalidate_fact_table(\"fact_receipt_lines\", expected_min_rows=1)\n\nprint(\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Verify Silver tables were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in Silver database\n",
    "silver_tables = spark.sql(f\"SHOW TABLES IN {DB_NAME}\").collect()\n",
    "\n",
    "print(f\"\\nSilver Database ({DB_NAME}) Tables:\")\n",
    "print(f\"  Total: {len(silver_tables)} tables\\n\")\n",
    "\n",
    "dim_count = sum(1 for t in silver_tables if t.tableName.startswith('dim_'))\n",
    "fact_count = sum(1 for t in silver_tables if t.tableName.startswith('fact_'))\n",
    "\n",
    "print(f\"  Dimensions: {dim_count} (expected: 6)\")\n",
    "print(f\"  Facts: {fact_count} (expected: 18)\")\n",
    "print(f\"\\n  Target: 24 tables (6 dims + 18 facts)\")\n",
    "\n",
    "if len(silver_tables) == 24:\n",
    "    print(f\"\\n✓ Silver layer complete!\")\n",
    "elif len(silver_tables) < 24:\n",
    "    print(f\"\\n⚠ Silver layer has {len(silver_tables)}/24 tables\")\n",
    "    print(f\"  Some Bronze tables may be missing - check Bronze layer creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test read from Silver\nprint(\"\\nTesting Silver layer access...\\n\")\n\ntry:\n    df_receipts = spark.table(f\"{DB_NAME}.fact_receipts\")\n    print(f\"✓ fact_receipts: {df_receipts.count()} rows\")\nexcept Exception as e:\n    print(f\"✗ fact_receipts: {e}\")\n\ntry:\n    df_receipt_lines = spark.table(f\"{DB_NAME}.fact_receipt_lines\")\n    print(f\"✓ fact_receipt_lines: {df_receipt_lines.count()} rows\")\n    print(f\"\\nfact_receipt_lines schema:\")\n    df_receipt_lines.printSchema()\nexcept Exception as e:\n    print(f\"✗ fact_receipt_lines: {e}\")\n\n# Report schema mismatch metrics\nprint(\"\\n\" + \"=\"*70)\nprint(\"TRANSFORMATION SUMMARY\")\nprint(\"=\"*70)\nprint(f\"Schema mismatches detected: {schema_mismatch_count}\")\nif schema_mismatch_count > 0:\n    if FAIL_ON_SCHEMA_MISMATCH:\n        print(\"⚠️  Production mode: Schema mismatches caused failures (as expected)\")\n    else:\n        print(\"⚠️  Development mode: Fell back to batch-only for mismatched schemas\")\n        print(\"   For production, set FAIL_ON_SCHEMA_MISMATCH=true to fail fast\")\nelse:\n    print(\"✓ All schemas validated successfully\")\n\nprint(\"\\nSilver layer ready for Gold aggregations!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}