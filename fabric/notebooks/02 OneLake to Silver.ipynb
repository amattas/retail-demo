{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Fabric Notebook (PySpark) - Load exported Parquet files directly to Silver Delta\n",
    "# Use this for historical batch data from datagen Parquet exports\n",
    "# (Bronze layer is for streaming JSON events only)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Configure source path - update this to your Parquet export location\n",
    "# If using Azure Storage: \"abfss://container@account.dfs.core.windows.net/path\"\n",
    "# If using Lakehouse Files: \"Files/adls-parquet-copy\"\n",
    "PARQUET_SOURCE = \"Files\"\n",
    "DB_NAME = \"ag\"\n",
    "\n",
    "\n",
    "def get_fs():\n",
    "    try:\n",
    "        from notebookutils import mssparkutils\n",
    "        return mssparkutils.fs\n",
    "    except Exception:\n",
    "        return dbutils.fs\n",
    "\n",
    "\n",
    "FS = get_fs()\n",
    "\n",
    "\n",
    "def path_exists(path):\n",
    "    try:\n",
    "        FS.ls(path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resolve_path(*candidates):\n",
    "    for candidate in candidates:\n",
    "        if path_exists(candidate):\n",
    "            return candidate\n",
    "    raise FileNotFoundError(f\"No matching path for: {candidates}\")\n",
    "\n",
    "\n",
    "def read_parquet_recursive(path):\n",
    "    return spark.read.option(\"recursiveFileLookup\", \"true\").parquet(path)\n",
    "\n",
    "\n",
    "def read_dim(table_name):\n",
    "    path = resolve_path(\n",
    "        f\"{PARQUET_SOURCE}/master/{table_name}\",\n",
    "        f\"{PARQUET_SOURCE}/{table_name}\",\n",
    "    )\n",
    "    return read_parquet_recursive(path)\n",
    "\n",
    "\n",
    "def read_fact(table_name):\n",
    "    path = resolve_path(\n",
    "        f\"{PARQUET_SOURCE}/facts/{table_name}\",\n",
    "        f\"{PARQUET_SOURCE}/{table_name}\",\n",
    "    )\n",
    "    return read_parquet_recursive(path)\n",
    "\n",
    "\n",
    "def normalize_columns(df):\n",
    "    return df.toDF(*[c.lower() for c in df.columns])\n",
    "\n",
    "\n",
    "def col_with_fallback(df, candidates, alias, required=True):\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for name in candidates:\n",
    "        key = name.lower()\n",
    "        if key in cols:\n",
    "            return F.col(cols[key]).alias(alias)\n",
    "    if required:\n",
    "        raise ValueError(f\"Missing column for '{alias}', tried {candidates}\")\n",
    "    return F.lit(None).cast(\"string\").alias(alias)\n",
    "\n",
    "\n",
    "def select_columns(df, mapping):\n",
    "    df = normalize_columns(df)\n",
    "    columns = []\n",
    "    for alias, spec in mapping.items():\n",
    "        if isinstance(spec, dict):\n",
    "            candidates = spec.get(\"candidates\", [])\n",
    "            required = spec.get(\"required\", True)\n",
    "        else:\n",
    "            candidates = spec\n",
    "            required = True\n",
    "        columns.append(col_with_fallback(df, candidates, alias, required=required))\n",
    "    return df.select(*columns)\n",
    "\n",
    "\n",
    "def ensure_database(name):\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {name}\")\n",
    "\n",
    "\n",
    "def save_table(df, table_name, mode=\"overwrite\"):\n",
    "    full_name = f\"{DB_NAME}.{table_name}\"\n",
    "    df.write.format(\"delta\").mode(mode).saveAsTable(full_name)\n",
    "    print(f\"  Written to {full_name}\")\n",
    "\n",
    "\n",
    "ensure_database(DB_NAME)\n",
    "\n",
    "# =============================================================================\n",
    "# DIMENSION TABLES (Master Data)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Loading dimension tables...\")\n",
    "\n",
    "# Geographies\n",
    "try:\n",
    "    df_geo = read_dim(\"dim_geographies\")\n",
    "    save_table(df_geo, \"dim_geographies\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping dim_geographies: {e}\")\n",
    "\n",
    "# Stores\n",
    "try:\n",
    "    df_stores = read_dim(\"dim_stores\")\n",
    "    save_table(df_stores, \"dim_stores\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping dim_stores: {e}\")\n",
    "\n",
    "# Distribution Centers\n",
    "try:\n",
    "    df_dcs = read_dim(\"dim_distribution_centers\")\n",
    "    save_table(df_dcs, \"dim_distribution_centers\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping dim_distribution_centers: {e}\")\n",
    "\n",
    "# Trucks\n",
    "try:\n",
    "    df_trucks = read_dim(\"dim_trucks\")\n",
    "    save_table(df_trucks, \"dim_trucks\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping dim_trucks: {e}\")\n",
    "\n",
    "# Customers\n",
    "try:\n",
    "    df_customers = read_dim(\"dim_customers\")\n",
    "    save_table(df_customers, \"dim_customers\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping dim_customers: {e}\")\n",
    "\n",
    "# Products\n",
    "try:\n",
    "    df_products = read_dim(\"dim_products\")\n",
    "    save_table(df_products, \"dim_products\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping dim_products: {e}\")\n",
    "\n",
    "print(\"Dimension tables loaded.\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# FACT TABLES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Loading fact tables...\")\n",
    "\n",
    "# 1) Receipts\n",
    "try:\n",
    "    df_receipts = select_columns(\n",
    "        read_fact(\"fact_receipts\"),\n",
    "        {\n",
    "            \"event_ts\": [\"event_ts\"],\n",
    "            \"receipt_id_ext\": [\"receipt_id_ext\", \"receipt_id\"],\n",
    "            \"payment_method\": [\"payment_method\"],\n",
    "            \"discount_amount\": [\"discount_amount\"],\n",
    "            \"tax_cents\": [\"tax_cents\"],\n",
    "            \"subtotal\": [\"subtotal\", \"subtotal_amount\"],\n",
    "            \"total\": [\"total\", \"total_amount\"],\n",
    "            \"total_cents\": [\"total_cents\"],\n",
    "            \"receipt_type\": [\"receipt_type\"],\n",
    "            \"subtotal_cents\": [\"subtotal_cents\"],\n",
    "            \"tax\": [\"tax\", \"tax_amount\"],\n",
    "            \"customer_id\": [\"customer_id\"],\n",
    "            \"store_id\": [\"store_id\"],\n",
    "            \"return_for_receipt_id_ext\": [\"return_for_receipt_id_ext\"],\n",
    "        },\n",
    "    )\n",
    "    save_table(df_receipts, \"fact_receipts\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping receipts: {e}\")\n",
    "\n",
    "# 2) Receipt Lines\n",
    "try:\n",
    "    df_receipt_lines = select_columns(\n",
    "        read_fact(\"fact_receipt_lines\"),\n",
    "        {\n",
    "            \"unit_cents\": [\"unit_cents\"],\n",
    "            \"unit_price\": [\"unit_price\"],\n",
    "            \"event_ts\": [\"event_ts\"],\n",
    "            \"product_id\": [\"product_id\"],\n",
    "            \"quantity\": [\"quantity\"],\n",
    "            \"ext_price\": [\"ext_price\"],\n",
    "            \"line_num\": [\"line_num\"],\n",
    "            \"promo_code\": [\"promo_code\"],\n",
    "            \"ext_cents\": [\"ext_cents\"],\n",
    "            \"receipt_id_ext\": [\"receipt_id_ext\", \"receipt_id\"],\n",
    "        },\n",
    "    )\n",
    "    save_table(df_receipt_lines, \"fact_receipt_lines\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping receipt_lines: {e}\")\n",
    "\n",
    "# 3) Store Inventory Transactions\n",
    "try:\n",
    "    df_store_inv = select_columns(\n",
    "        read_fact(\"fact_store_inventory_txn\"),\n",
    "        {\n",
    "            \"event_ts\": [\"event_ts\"],\n",
    "            \"product_id\": [\"product_id\"],\n",
    "            \"txn_type\": [\"txn_type\"],\n",
    "            \"quantity\": [\"quantity\"],\n",
    "            \"source\": [\"source\"],\n",
    "            \"store_id\": [\"store_id\"],\n",
    "            \"balance\": [\"balance\"],\n",
    "        },\n",
    "    )\n",
    "    save_table(df_store_inv, \"fact_store_inventory_txn\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping store_inventory_txn: {e}\")\n",
    "\n",
    "# 4) DC Inventory Transactions\n",
    "try:\n",
    "    df_dc_inv = select_columns(\n",
    "        read_fact(\"fact_dc_inventory_txn\"),\n",
    "        {\n",
    "            \"event_ts\": [\"event_ts\"],\n",
    "            \"product_id\": [\"product_id\"],\n",
    "            \"txn_type\": [\"txn_type\"],\n",
    "            \"quantity\": [\"quantity\"],\n",
    "            \"dc_id\": [\"dc_id\"],\n",
    "            \"balance\": [\"balance\"],\n",
    "            \"source\": [\"source\"],\n",
    "        },\n",
    "    )\n",
    "    save_table(df_dc_inv, \"fact_dc_inventory_txn\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping dc_inventory_txn: {e}\")\n",
    "\n",
    "# 5) Foot Traffic\n",
    "try:\n",
    "    df_foot = select_columns(\n",
    "        read_fact(\"fact_foot_traffic\"),\n",
    "        {\n",
    "            \"count\": [\"count\"],\n",
    "            \"zone\": [\"zone\"],\n",
    "            \"event_ts\": [\"event_ts\"],\n",
    "            \"sensor_id\": [\"sensor_id\"],\n",
    "            \"dwell_seconds\": [\"dwell_seconds\"],\n",
    "            \"store_id\": [\"store_id\"],\n",
    "        },\n",
    "    )\n",
    "    save_table(df_foot, \"fact_foot_traffic\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping foot_traffic: {e}\")\n",
    "\n",
    "# 6) BLE Pings\n",
    "try:\n",
    "    df_ble = select_columns(\n",
    "        read_fact(\"fact_ble_pings\"),\n",
    "        {\n",
    "            \"zone\": [\"zone\"],\n",
    "            \"event_ts\": [\"event_ts\"],\n",
    "            \"rssi\": [\"rssi\"],\n",
    "            \"customer_ble_id\": [\"customer_ble_id\"],\n",
    "            \"customer_id\": {\"candidates\": [\"customer_id\"], \"required\": False},\n",
    "            \"store_id\": [\"store_id\"],\n",
    "            \"beacon_id\": [\"beacon_id\"],\n",
    "        },\n",
    "    )\n",
    "    save_table(df_ble, \"fact_ble_pings\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping ble_pings: {e}\")\n",
    "\n",
    "# 7) Marketing\n",
    "try:\n",
    "    df_mkt = select_columns(\n",
    "        read_fact(\"fact_marketing\"),\n",
    "        {\n",
    "            \"event_ts\": [\"event_ts\"],\n",
    "            \"campaign_id\": [\"campaign_id\"],\n",
    "            \"device\": [\"device\"],\n",
    "            \"creative_id\": [\"creative_id\"],\n",
    "            \"customer_ad_id\": [\"customer_ad_id\"],\n",
    "            \"impression_id_ext\": [\"impression_id_ext\", \"impression_id\"],\n",
    "            \"cost\": [\"cost\"],\n",
    "            \"cost_cents\": {\"candidates\": [\"cost_cents\"], \"required\": False},\n",
    "            \"customer_id\": [\"customer_id\"],\n",
    "            \"channel\": [\"channel\"],\n",
    "        },\n",
    "    )\n",
    "    save_table(df_mkt, \"fact_marketing\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping marketing: {e}\")\n",
    "\n",
    "# 8) Online Order Headers\n",
    "try:\n",
    "    df_oo_hdr = select_columns(\n",
    "        read_fact(\"fact_online_order_headers\"),\n",
    "        {\n",
    "            \"completed_ts\": [\"completed_ts\"],\n",
    "            \"event_ts\": [\"event_ts\"],\n",
    "            \"order_id_ext\": [\"order_id_ext\", \"order_id\"],\n",
    "            \"tax_cents\": [\"tax_cents\"],\n",
    "            \"subtotal\": [\"subtotal\", \"subtotal_amount\"],\n",
    "            \"total\": [\"total\", \"total_amount\"],\n",
    "            \"total_cents\": [\"total_cents\"],\n",
    "            \"subtotal_cents\": [\"subtotal_cents\"],\n",
    "            \"tax\": [\"tax\", \"tax_amount\"],\n",
    "            \"customer_id\": [\"customer_id\"],\n",
    "            \"payment_method\": [\"payment_method\"],\n",
    "        },\n",
    "    )\n",
    "    save_table(df_oo_hdr, \"fact_online_order_headers\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping online_order_headers: {e}\")\n",
    "\n",
    "# 9) Online Order Lines\n",
    "try:\n",
    "    df_oo_lines = select_columns(\n",
    "        read_fact(\"fact_online_order_lines\"),\n",
    "        {\n",
    "            \"unit_cents\": [\"unit_cents\"],\n",
    "            \"shipped_ts\": [\"shipped_ts\"],\n",
    "            \"unit_price\": [\"unit_price\"],\n",
    "            \"fulfillment_status\": [\"fulfillment_status\"],\n",
    "            \"order_id\": [\"order_id\"],\n",
    "            \"delivered_ts\": [\"delivered_ts\"],\n",
    "            \"product_id\": [\"product_id\"],\n",
    "            \"quantity\": [\"quantity\"],\n",
    "            \"ext_price\": [\"ext_price\"],\n",
    "            \"node_type\": [\"node_type\"],\n",
    "            \"fulfillment_mode\": [\"fulfillment_mode\"],\n",
    "            \"picked_ts\": [\"picked_ts\"],\n",
    "            \"node_id\": [\"node_id\"],\n",
    "            \"line_num\": [\"line_num\"],\n",
    "            \"promo_code\": [\"promo_code\"],\n",
    "            \"ext_cents\": [\"ext_cents\"],\n",
    "        },\n",
    "    )\n",
    "    save_table(df_oo_lines, \"fact_online_order_lines\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping online_order_lines: {e}\")\n",
    "\n",
    "# 10) Truck Moves\n",
    "try:\n",
    "    df_trucks = select_columns(\n",
    "        read_fact(\"fact_truck_moves\"),\n",
    "        {\n",
    "            \"event_ts\": [\"event_ts\"],\n",
    "            \"truck_id\": [\"truck_id\"],\n",
    "            \"dc_id\": [\"dc_id\"],\n",
    "            \"store_id\": [\"store_id\"],\n",
    "            \"shipment_id\": [\"shipment_id\"],\n",
    "            \"status\": [\"status\"],\n",
    "            \"eta\": [\"eta\"],\n",
    "            \"etd\": [\"etd\"],\n",
    "        },\n",
    "    )\n",
    "    save_table(df_trucks, \"fact_truck_moves\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping truck_moves: {e}\")\n",
    "\n",
    "print(\"\\nFact tables loaded.\")\n",
    "print(\"Parquet -> Silver load complete!\")\n"
   ],
   "outputs": [],
   "execution_count": null,
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "id": "26658282-3012-49d4-befb-f802ccdbcd9b"
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "dependencies": {
   "lakehouse": {
    "known_lakehouses": [
     {
      "id": "9dff608a-05dc-4f3f-b979-f911f04aa6be"
     }
    ],
    "default_lakehouse": "9dff608a-05dc-4f3f-b979-f911f04aa6be",
    "default_lakehouse_name": "Retail",
    "default_lakehouse_workspace_id": "5219ac70-71d4-4dfc-af32-5b8a6c29a471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}